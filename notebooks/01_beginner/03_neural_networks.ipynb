{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic 3: Building Neural Networks with nn.Module\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Understand WHY `nn.Module` is the foundation of PyTorch models\n",
    "- Build neural networks using `nn.Module` and pre-built layers\n",
    "- Understand the difference between layers, models, and modules\n",
    "- Use activation functions and understand their purpose\n",
    "- Inspect model architecture and parameters\n",
    "- Build custom layers and models\n",
    "- Understand forward pass vs backward pass\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Big Picture: Why nn.Module?\n",
    "\n",
    "### From Manual to Automatic\n",
    "\n",
    "In Topic 2, we implemented linear regression manually:\n",
    "```python\n",
    "w = torch.randn(1, requires_grad=True)\n",
    "b = torch.randn(1, requires_grad=True)\n",
    "y_pred = w * x + b\n",
    "```\n",
    "\n",
    "**Problems with this approach**:\n",
    "1. Tedious: Manually create every parameter\n",
    "2. Error-prone: Easy to forget `requires_grad=True`\n",
    "3. Not modular: Hard to reuse and compose\n",
    "4. No structure: Can't easily inspect or save models\n",
    "\n",
    "### The nn.Module Solution\n",
    "\n",
    "`nn.Module` is PyTorch's **base class for all neural network components**. It provides:\n",
    "\n",
    "1. **Automatic parameter management**: Tracks all parameters automatically\n",
    "2. **Modular design**: Build complex models from simple building blocks\n",
    "3. **State management**: Easy to save/load models\n",
    "4. **GPU support**: Move entire model to GPU with one line\n",
    "5. **Training/eval modes**: Switch behavior (dropout, batchnorm, etc.)\n",
    "\n",
    "### The Hierarchy\n",
    "\n",
    "```\n",
    "nn.Module (base class)\n",
    "    ├── Layers (nn.Linear, nn.Conv2d, etc.)\n",
    "    ├── Activation Functions (nn.ReLU, nn.Sigmoid, etc.)\n",
    "    ├── Loss Functions (nn.MSELoss, nn.CrossEntropyLoss, etc.)\n",
    "    └── Your Custom Models (inherit from nn.Module)\n",
    "```\n",
    "\n",
    "**Key insight**: Everything is a module! This makes PyTorch incredibly composable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Set random seed\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Your First Module: nn.Linear\n",
    "\n",
    "### What Is nn.Linear?\n",
    "\n",
    "`nn.Linear` implements: $y = xW^T + b$\n",
    "\n",
    "Where:\n",
    "- $x$: input `(batch_size, in_features)`\n",
    "- $W$: weight matrix `(out_features, in_features)` \n",
    "- $b$: bias vector `(out_features,)`\n",
    "- $y$: output `(batch_size, out_features)`\n",
    "\n",
    "**Why \"Linear\"?** It's a linear transformation (no activation function).\n",
    "\n",
    "**Why this is used**: Every fully connected layer in a neural network is an `nn.Linear`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a linear layer\n",
    "in_features = 10\n",
    "out_features = 5\n",
    "\n",
    "linear = nn.Linear(in_features, out_features)\n",
    "\n",
    "print(f\"Linear layer: {linear}\")\n",
    "print()\n",
    "\n",
    "# Inspect parameters\n",
    "print(\"Parameters:\")\n",
    "print(f\"Weight shape: {linear.weight.shape}\")  # (out_features, in_features)\n",
    "print(f\"Bias shape: {linear.bias.shape}\")      # (out_features,)\n",
    "print()\n",
    "\n",
    "# All parameters have requires_grad=True automatically!\n",
    "print(f\"Weight requires_grad: {linear.weight.requires_grad}\")\n",
    "print(f\"Bias requires_grad: {linear.bias.requires_grad}\")\n",
    "print()\n",
    "\n",
    "# Use the layer (forward pass)\n",
    "batch_size = 3\n",
    "x = torch.randn(batch_size, in_features)\n",
    "y = linear(x)  # Calls linear.forward(x) internally\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {y.shape}\")\n",
    "print(f\"Output:\\n{y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Observations\n",
    "\n",
    "1. **Automatic initialization**: Weights and biases are initialized with good default values\n",
    "2. **requires_grad=True**: Set automatically for all parameters\n",
    "3. **Callable**: Use `layer(x)` instead of manually computing `x @ W.T + b`\n",
    "4. **Shape handling**: Automatically handles batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with manual implementation\n",
    "x = torch.randn(3, 10)\n",
    "linear = nn.Linear(10, 5)\n",
    "\n",
    "# Using nn.Linear\n",
    "y_auto = linear(x)\n",
    "\n",
    "# Manual implementation\n",
    "y_manual = x @ linear.weight.T + linear.bias\n",
    "\n",
    "print(\"Using nn.Linear:\")\n",
    "print(y_auto)\n",
    "print()\n",
    "\n",
    "print(\"Manual implementation:\")\n",
    "print(y_manual)\n",
    "print()\n",
    "\n",
    "print(f\"Are they equal? {torch.allclose(y_auto, y_manual)}\")\n",
    "print(\"\\nConclusion: nn.Linear is just a convenient wrapper!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Activation Functions: Adding Non-Linearity\n",
    "\n",
    "### Why Activation Functions?\n",
    "\n",
    "**Problem**: Stacking linear layers without activations is still linear!\n",
    "\n",
    "$$\\text{Linear}(\\text{Linear}(x)) = W_2(W_1x + b_1) + b_2 = (W_2W_1)x + (W_2b_1 + b_2) = Wx + b$$\n",
    "\n",
    "Just another linear transformation! You can't learn complex patterns.\n",
    "\n",
    "**Solution**: Add **non-linear** activation functions between layers.\n",
    "\n",
    "### Common Activation Functions\n",
    "\n",
    "1. **ReLU** (Rectified Linear Unit): $f(x) = \\max(0, x)$\n",
    "   - Most popular in deep learning\n",
    "   - Fast to compute\n",
    "   - Helps with vanishing gradient problem\n",
    "\n",
    "2. **Sigmoid**: $f(x) = \\frac{1}{1 + e^{-x}}$\n",
    "   - Outputs between 0 and 1\n",
    "   - Used for binary classification output\n",
    "   - Can cause vanishing gradients\n",
    "\n",
    "3. **Tanh**: $f(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$\n",
    "   - Outputs between -1 and 1\n",
    "   - Zero-centered (better than sigmoid)\n",
    "   - Still has vanishing gradient issues\n",
    "\n",
    "4. **LeakyReLU**: $f(x) = \\max(0.01x, x)$\n",
    "   - Fixes \"dying ReLU\" problem\n",
    "   - Allows small negative values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize activation functions\n",
    "x = torch.linspace(-5, 5, 200)\n",
    "\n",
    "# Compute activations\n",
    "relu = F.relu(x)\n",
    "sigmoid = torch.sigmoid(x)\n",
    "tanh = torch.tanh(x)\n",
    "leaky_relu = F.leaky_relu(x, negative_slope=0.1)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# ReLU\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(x.numpy(), relu.numpy(), 'b-', linewidth=2)\n",
    "plt.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "plt.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "plt.title('ReLU: max(0, x)', fontsize=14)\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.ylabel('ReLU(x)', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Sigmoid\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(x.numpy(), sigmoid.numpy(), 'r-', linewidth=2)\n",
    "plt.axhline(y=0.5, color='k', linestyle='--', alpha=0.3)\n",
    "plt.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "plt.title('Sigmoid: 1/(1 + e^(-x))', fontsize=14)\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.ylabel('Sigmoid(x)', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Tanh\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(x.numpy(), tanh.numpy(), 'g-', linewidth=2)\n",
    "plt.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "plt.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "plt.title('Tanh: (e^x - e^(-x))/(e^x + e^(-x))', fontsize=14)\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.ylabel('Tanh(x)', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# LeakyReLU\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(x.numpy(), leaky_relu.numpy(), 'm-', linewidth=2)\n",
    "plt.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "plt.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "plt.title('LeakyReLU: max(0.1x, x)', fontsize=14)\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.ylabel('LeakyReLU(x)', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key properties:\")\n",
    "print(\"ReLU: Simple, effective, most popular\")\n",
    "print(\"Sigmoid: Outputs [0, 1], good for probabilities\")\n",
    "print(\"Tanh: Outputs [-1, 1], zero-centered\")\n",
    "print(\"LeakyReLU: Prevents dying ReLU problem\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two Ways to Use Activations\n",
    "\n",
    "1. **Functional API** (`torch.nn.functional`): Stateless functions\n",
    "2. **Module API** (`torch.nn`): Stateful modules\n",
    "\n",
    "**When to use which?**\n",
    "- Functional: When you just need to apply the function\n",
    "- Module: When building models (more consistent style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functional API\n",
    "x = torch.tensor([-1.0, 0.0, 1.0])\n",
    "y_func = F.relu(x)\n",
    "print(f\"Functional ReLU: {y_func}\")\n",
    "\n",
    "# Module API\n",
    "relu = nn.ReLU()\n",
    "y_module = relu(x)\n",
    "print(f\"Module ReLU: {y_module}\")\n",
    "\n",
    "print(f\"\\nAre they equal? {torch.equal(y_func, y_module)}\")\n",
    "print(\"\\nUse modules when building models for consistency.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Building Your First Neural Network\n",
    "\n",
    "### The Recipe\n",
    "\n",
    "1. **Inherit from `nn.Module`**\n",
    "2. **Define layers in `__init__`**: Create all your layers\n",
    "3. **Implement `forward`**: Define how data flows through the network\n",
    "4. **That's it!** PyTorch handles everything else\n",
    "\n",
    "### Example: 3-Layer Fully Connected Network\n",
    "\n",
    "```\n",
    "Input (10) → Linear(10→64) → ReLU → Linear(64→32) → ReLU → Linear(32→5) → Output (5)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "    \"\"\"Simple 3-layer neural network\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "        # MUST call parent constructor\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define layers\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "        \n",
    "        # Define activations\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass: define computation\"\"\"\n",
    "        # Layer 1\n",
    "        x = self.fc1(x)      # Linear transformation\n",
    "        x = self.relu(x)     # Non-linearity\n",
    "        \n",
    "        # Layer 2\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        # Layer 3 (output layer - no activation)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Create model\n",
    "model = SimpleNet(input_size=10, hidden_size1=64, hidden_size2=32, output_size=5)\n",
    "print(model)\n",
    "print()\n",
    "\n",
    "# Test forward pass\n",
    "x = torch.randn(3, 10)  # Batch of 3 samples\n",
    "output = model(x)       # Calls model.forward(x)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Output:\\n{output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened?\n",
    "\n",
    "1. **Defined architecture in `__init__`**: Specified all layers\n",
    "2. **Defined computation in `forward`**: How data flows\n",
    "3. **Automatic parameter tracking**: All parameters are tracked automatically\n",
    "4. **Forward pass**: Calling `model(x)` executes `forward(x)`\n",
    "5. **Backward pass**: Will happen automatically when we call `loss.backward()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect model parameters\n",
    "print(\"Model parameters:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name:15s} | shape: {str(param.shape):20s} | requires_grad: {param.requires_grad}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Count total parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Sequential API: The Quick Way\n",
    "\n",
    "For simple sequential models, `nn.Sequential` is more concise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same network using nn.Sequential\n",
    "model_seq = nn.Sequential(\n",
    "    nn.Linear(10, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 5)\n",
    ")\n",
    "\n",
    "print(model_seq)\n",
    "print()\n",
    "\n",
    "# Test it\n",
    "x = torch.randn(3, 10)\n",
    "output = model_seq(x)\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to Use Sequential vs Custom Module?\n",
    "\n",
    "**Use `nn.Sequential` when**:\n",
    "- Simple linear flow (output of layer N → input of layer N+1)\n",
    "- No branching or skip connections\n",
    "- Quick prototyping\n",
    "\n",
    "**Use custom `nn.Module` when**:\n",
    "- Complex architectures (ResNet, Transformers)\n",
    "- Need control flow (if/else, loops)\n",
    "- Skip connections or multiple inputs/outputs\n",
    "- Want readable, documented code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Named Sequential for better readability\n",
    "from collections import OrderedDict\n",
    "\n",
    "model_named = nn.Sequential(OrderedDict([\n",
    "    ('fc1', nn.Linear(10, 64)),\n",
    "    ('relu1', nn.ReLU()),\n",
    "    ('fc2', nn.Linear(64, 32)),\n",
    "    ('relu2', nn.ReLU()),\n",
    "    ('fc3', nn.Linear(32, 5))\n",
    "]))\n",
    "\n",
    "print(model_named)\n",
    "print()\n",
    "\n",
    "# Access layers by name\n",
    "print(f\"First layer: {model_named.fc1}\")\n",
    "print(f\"First layer weight shape: {model_named.fc1.weight.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Model Inspection and Utilities\n",
    "\n",
    "PyTorch provides many utilities to inspect and manipulate models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model summary\n",
    "model = SimpleNet(10, 64, 32, 5)\n",
    "\n",
    "print(\"Model architecture:\")\n",
    "print(model)\n",
    "print()\n",
    "\n",
    "# Iterate over modules\n",
    "print(\"All modules:\")\n",
    "for i, module in enumerate(model.modules()):\n",
    "    print(f\"{i}: {module.__class__.__name__}\")\n",
    "print()\n",
    "\n",
    "# Iterate over children (direct children only)\n",
    "print(\"Direct children:\")\n",
    "for i, module in enumerate(model.children()):\n",
    "    print(f\"{i}: {module}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get specific parameters\n",
    "print(\"First layer weights:\")\n",
    "print(f\"Shape: {model.fc1.weight.shape}\")\n",
    "print(f\"First 3 rows:\\n{model.fc1.weight[:3]}\")\n",
    "print()\n",
    "\n",
    "# Get all parameters as list\n",
    "params = list(model.parameters())\n",
    "print(f\"Number of parameter tensors: {len(params)}\")\n",
    "for i, p in enumerate(params):\n",
    "    print(f\"Parameter {i}: shape {p.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training vs Evaluation Mode\n",
    "\n",
    "Some layers behave differently during training vs evaluation (Dropout, BatchNorm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check mode\n",
    "print(f\"Model training mode: {model.training}\")\n",
    "\n",
    "# Switch to evaluation mode\n",
    "model.eval()\n",
    "print(f\"After eval(): {model.training}\")\n",
    "\n",
    "# Switch back to training mode\n",
    "model.train()\n",
    "print(f\"After train(): {model.training}\")\n",
    "\n",
    "print(\"\\nAlways remember:\")\n",
    "print(\"- model.train() before training\")\n",
    "print(\"- model.eval() before evaluation/inference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving Models to GPU\n",
    "\n",
    "Move entire model (all parameters) to GPU with one line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Move model to device\n",
    "model = model.to(device)\n",
    "\n",
    "# Verify parameters moved\n",
    "print(f\"\\nFirst parameter device: {next(model.parameters()).device}\")\n",
    "\n",
    "# Now inputs must also be on same device\n",
    "x = torch.randn(3, 10).to(device)\n",
    "output = model(x)\n",
    "print(f\"Output device: {output.device}\")\n",
    "\n",
    "print(\"\\nKey point: Model and inputs must be on same device!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Building Complex Architectures\n",
    "\n",
    "Let's build more sophisticated models!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Model with Skip Connections (Residual)\n",
    "\n",
    "Skip connections allow gradients to flow better through deep networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"Basic residual block: output = F(x) + x\"\"\"\n",
    "    \n",
    "    def __init__(self, size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(size, size)\n",
    "        self.fc2 = nn.Linear(size, size)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Save input for skip connection\n",
    "        identity = x\n",
    "        \n",
    "        # Compute F(x)\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        # Add skip connection: F(x) + x\n",
    "        out = out + identity\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# Test it\n",
    "block = ResidualBlock(64)\n",
    "x = torch.randn(2, 64)\n",
    "output = block(x)\n",
    "\n",
    "print(f\"ResidualBlock:\")\n",
    "print(block)\n",
    "print(f\"\\nInput shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(\"\\nWhy residuals? Help gradients flow in very deep networks!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Model with Multiple Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiOutputNet(nn.Module):\n",
    "    \"\"\"Network with two output heads\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size1, output_size2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Shared layers\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Output head 1 (e.g., classification)\n",
    "        self.head1 = nn.Linear(hidden_size, output_size1)\n",
    "        \n",
    "        # Output head 2 (e.g., regression)\n",
    "        self.head2 = nn.Linear(hidden_size, output_size2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Shared feature extraction\n",
    "        features = self.shared(x)\n",
    "        \n",
    "        # Two separate outputs\n",
    "        output1 = self.head1(features)\n",
    "        output2 = self.head2(features)\n",
    "        \n",
    "        return output1, output2\n",
    "\n",
    "# Test it\n",
    "model = MultiOutputNet(input_size=20, hidden_size=64, output_size1=10, output_size2=1)\n",
    "x = torch.randn(3, 20)\n",
    "out1, out2 = model(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output 1 shape: {out1.shape}\")  # (3, 10) - classification\n",
    "print(f\"Output 2 shape: {out2.shape}\")  # (3, 1) - regression\n",
    "print(\"\\nUse case: Multi-task learning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: ModuleList and ModuleDict\n",
    "\n",
    "For dynamic architectures with variable number of layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicNet(nn.Module):\n",
    "    \"\"\"Network with variable number of layers\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Create layers dynamically\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        # Input layer\n",
    "        self.layers.append(nn.Linear(input_size, hidden_sizes[0]))\n",
    "        \n",
    "        # Hidden layers\n",
    "        for i in range(len(hidden_sizes) - 1):\n",
    "            self.layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i+1]))\n",
    "        \n",
    "        # Output layer\n",
    "        self.layers.append(nn.Linear(hidden_sizes[-1], output_size))\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Pass through all layers except last\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = self.relu(layer(x))\n",
    "        \n",
    "        # Output layer (no activation)\n",
    "        x = self.layers[-1](x)\n",
    "        return x\n",
    "\n",
    "# Create network with 5 hidden layers\n",
    "model = DynamicNet(input_size=10, hidden_sizes=[64, 128, 128, 64, 32], output_size=5)\n",
    "print(model)\n",
    "print(f\"\\nNumber of layers: {len(model.layers)}\")\n",
    "\n",
    "# Test\n",
    "x = torch.randn(2, 10)\n",
    "output = model(x)\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Mini Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Build a Simple Classifier\n",
    "\n",
    "Create a 2-layer neural network:\n",
    "- Input: 784 features (28x28 flattened image)\n",
    "- Hidden: 128 neurons with ReLU\n",
    "- Output: 10 classes\n",
    "\n",
    "Use `nn.Module` (not Sequential)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleClassifier()\n",
    "print(model)\n",
    "print()\n",
    "\n",
    "# Test with batch of 5 images\n",
    "x = torch.randn(5, 784)\n",
    "output = model(x)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "\n",
    "# Count parameters\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Convert to Sequential\n",
    "\n",
    "Rewrite the above network using `nn.Sequential`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "model_seq = nn.Sequential(\n",
    "    nn.Linear(784, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 10)\n",
    ")\n",
    "\n",
    "print(model_seq)\n",
    "print()\n",
    "\n",
    "# Test\n",
    "x = torch.randn(5, 784)\n",
    "output = model_seq(x)\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "\n",
    "# Both versions should produce same shape\n",
    "print(\"\\nMuch more concise!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Add Dropout\n",
    "\n",
    "Modify the network to include dropout (p=0.5) after the first layer.\n",
    "Test that dropout behaves differently in train vs eval mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "class ClassifierWithDropout(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)  # Randomly zeros 50% of elements\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = ClassifierWithDropout()\n",
    "x = torch.randn(3, 784)\n",
    "\n",
    "# Training mode\n",
    "model.train()\n",
    "out1 = model(x)\n",
    "out2 = model(x)  # Different output!\n",
    "print(\"Training mode (dropout active):\")\n",
    "print(f\"Output 1:\\n{out1[0, :5]}\")\n",
    "print(f\"Output 2:\\n{out2[0, :5]}\")\n",
    "print(f\"Are they equal? {torch.equal(out1, out2)}\")\n",
    "print()\n",
    "\n",
    "# Eval mode\n",
    "model.eval()\n",
    "out1 = model(x)\n",
    "out2 = model(x)  # Same output!\n",
    "print(\"Eval mode (dropout disabled):\")\n",
    "print(f\"Output 1:\\n{out1[0, :5]}\")\n",
    "print(f\"Output 2:\\n{out2[0, :5]}\")\n",
    "print(f\"Are they equal? {torch.equal(out1, out2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Comprehensive Exercise: Build a Configurable Network\n",
    "\n",
    "Create a flexible neural network class that:\n",
    "1. Takes a list of layer sizes as input\n",
    "2. Supports different activation functions\n",
    "3. Optionally adds dropout after each hidden layer\n",
    "4. Optionally adds batch normalization\n",
    "\n",
    "**Requirements**:\n",
    "- Use `nn.ModuleList` for dynamic layers\n",
    "- Support ReLU, Tanh, or LeakyReLU activations\n",
    "- Clean, well-documented code\n",
    "\n",
    "**Test case**: Create a network with layers `[100, 256, 128, 64, 10]`, ReLU activation, dropout=0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "class ConfigurableNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Flexible fully connected neural network.\n",
    "    \n",
    "    Args:\n",
    "        layer_sizes: List of layer sizes [input, hidden1, hidden2, ..., output]\n",
    "        activation: 'relu', 'tanh', or 'leaky_relu'\n",
    "        dropout: Dropout probability (0 = no dropout)\n",
    "        batch_norm: Whether to use batch normalization\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, layer_sizes, activation='relu', dropout=0.0, batch_norm=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.dropout_p = dropout\n",
    "        self.use_batch_norm = batch_norm\n",
    "        \n",
    "        # Choose activation\n",
    "        if activation == 'relu':\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = nn.Tanh()\n",
    "        elif activation == 'leaky_relu':\n",
    "            self.activation = nn.LeakyReLU()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown activation: {activation}\")\n",
    "        \n",
    "        # Build layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.batch_norms = nn.ModuleList()\n",
    "        self.dropouts = nn.ModuleList()\n",
    "        \n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            # Linear layer\n",
    "            self.layers.append(nn.Linear(layer_sizes[i], layer_sizes[i+1]))\n",
    "            \n",
    "            # Batch norm (not for output layer)\n",
    "            if batch_norm and i < len(layer_sizes) - 2:\n",
    "                self.batch_norms.append(nn.BatchNorm1d(layer_sizes[i+1]))\n",
    "            else:\n",
    "                self.batch_norms.append(None)\n",
    "            \n",
    "            # Dropout (not for output layer)\n",
    "            if dropout > 0 and i < len(layer_sizes) - 2:\n",
    "                self.dropouts.append(nn.Dropout(dropout))\n",
    "            else:\n",
    "                self.dropouts.append(None)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Pass through all layers except last\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            x = self.layers[i](x)\n",
    "            \n",
    "            if self.batch_norms[i] is not None:\n",
    "                x = self.batch_norms[i](x)\n",
    "            \n",
    "            x = self.activation(x)\n",
    "            \n",
    "            if self.dropouts[i] is not None:\n",
    "                x = self.dropouts[i](x)\n",
    "        \n",
    "        # Output layer (no activation/dropout/batchnorm)\n",
    "        x = self.layers[-1](x)\n",
    "        return x\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return (f\"ConfigurableNet(layers={self.layer_sizes}, \"\n",
    "                f\"dropout={self.dropout_p}, batch_norm={self.use_batch_norm})\")\n",
    "\n",
    "\n",
    "# Test case 1: Basic network\n",
    "model1 = ConfigurableNet([100, 256, 128, 64, 10], activation='relu', dropout=0.3)\n",
    "print(\"Model 1:\")\n",
    "print(model1)\n",
    "print()\n",
    "\n",
    "x = torch.randn(5, 100)\n",
    "output = model1(x)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print()\n",
    "\n",
    "# Test case 2: With batch norm\n",
    "model2 = ConfigurableNet([50, 128, 64, 10], activation='leaky_relu', \n",
    "                         dropout=0.5, batch_norm=True)\n",
    "print(\"Model 2:\")\n",
    "print(model2)\n",
    "print()\n",
    "\n",
    "# Count parameters\n",
    "total = sum(p.numel() for p in model1.parameters())\n",
    "print(f\"Model 1 parameters: {total:,}\")\n",
    "\n",
    "total = sum(p.numel() for p in model2.parameters())\n",
    "print(f\"Model 2 parameters: {total:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **nn.Module is everything**: Base class for all neural network components\n",
    "2. **Two parts**: `__init__` (define layers) and `forward` (define computation)\n",
    "3. **Automatic parameter tracking**: All `nn.Module` objects are tracked\n",
    "4. **Activation functions**: Add non-linearity, essential for learning complex patterns\n",
    "5. **Sequential vs Module**: Sequential for simple chains, Module for flexibility\n",
    "6. **train() vs eval()**: Always switch modes appropriately\n",
    "7. **Composability**: Build complex models from simple modules\n",
    "8. **ModuleList/ModuleDict**: For dynamic architectures\n",
    "\n",
    "### Building Blocks Learned\n",
    "\n",
    "- `nn.Linear`: Fully connected layer\n",
    "- `nn.ReLU`, `nn.Sigmoid`, `nn.Tanh`: Activation functions\n",
    "- `nn.Dropout`: Regularization\n",
    "- `nn.BatchNorm1d`: Normalization\n",
    "- `nn.Sequential`: Container for linear stacks\n",
    "- `nn.ModuleList`: Container for dynamic layers\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "You can now build neural network architectures! Next, we'll learn about **loss functions** - how to measure and optimize model performance.\n",
    "\n",
    "Continue to: [Topic 4: Loss Functions - A Comprehensive Guide](04_loss_functions.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "- [PyTorch nn.Module Documentation](https://pytorch.org/docs/stable/generated/torch.nn.Module.html)\n",
    "- [PyTorch Neural Network Tutorial](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html)\n",
    "- [CS231n: Neural Networks](http://cs231n.github.io/neural-networks-1/)\n",
    "- [Activation Functions Explained](https://mlfromscratch.com/activation-functions-explained/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
