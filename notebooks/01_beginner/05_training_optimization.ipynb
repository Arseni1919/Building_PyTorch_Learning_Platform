{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic 5: Training Loop & Optimization\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Understand the complete training loop workflow\n",
    "- Know WHY each step in the training loop is necessary\n",
    "- Use PyTorch optimizers (SGD, Adam, and variants)\n",
    "- Implement proper train/validation splits\n",
    "- Track and visualize training metrics\n",
    "- Understand learning rates and scheduling\n",
    "- Build a complete end-to-end classification system\n",
    "- Avoid common training pitfalls\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Big Picture: The Training Loop\n",
    "\n",
    "### Why Do We Need a Training Loop?\n",
    "\n",
    "You've learned all the pieces:\n",
    "- **Tensors**: Data representation\n",
    "- **Autograd**: Gradient computation\n",
    "- **nn.Module**: Model architecture\n",
    "- **Loss functions**: Error measurement\n",
    "\n",
    "Now we put it all together into a **training loop** - the heart of deep learning!\n",
    "\n",
    "### The Standard PyTorch Training Loop\n",
    "\n",
    "```python\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_x, batch_y in dataloader:\n",
    "        # 1. Forward pass: compute predictions\n",
    "        predictions = model(batch_x)\n",
    "        \n",
    "        # 2. Compute loss\n",
    "        loss = loss_function(predictions, batch_y)\n",
    "        \n",
    "        # 3. Zero gradients (they accumulate!)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 4. Backward pass: compute gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # 5. Update weights\n",
    "        optimizer.step()\n",
    "```\n",
    "\n",
    "### Why Each Step?\n",
    "\n",
    "1. **Forward pass**: Get predictions from current model\n",
    "2. **Compute loss**: Measure how wrong we are\n",
    "3. **Zero gradients**: Clear old gradients (they accumulate by default)\n",
    "4. **Backward pass**: Compute gradients using autograd\n",
    "5. **Update weights**: Move parameters in direction of lower loss\n",
    "\n",
    "**Key insight**: This is gradient descent, but with mini-batches and automation!\n",
    "\n",
    "### Epochs vs Iterations\n",
    "\n",
    "- **Iteration**: One forward + backward pass on one batch\n",
    "- **Epoch**: One complete pass through entire dataset\n",
    "- If dataset has 1000 samples and batch size is 100:\n",
    "  - 1 epoch = 10 iterations\n",
    "  - 5 epochs = 50 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification, make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Optimizers: The Engine of Learning\n",
    "\n",
    "### What Is an Optimizer?\n",
    "\n",
    "An optimizer implements the **weight update rule**:\n",
    "$$\\theta_{\\text{new}} = \\theta_{\\text{old}} - \\text{update}$$\n",
    "\n",
    "Different optimizers compute the update differently.\n",
    "\n",
    "### Why Not Manual Updates?\n",
    "\n",
    "You could do:\n",
    "```python\n",
    "for param in model.parameters():\n",
    "    param.data -= learning_rate * param.grad\n",
    "```\n",
    "\n",
    "But optimizers provide:\n",
    "- Sophisticated update rules (momentum, adaptive learning rates)\n",
    "- Handling of multiple parameter groups\n",
    "- Learning rate scheduling\n",
    "- Numerical stability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Stochastic Gradient Descent (SGD)\n",
    "\n",
    "**Formula**: $\\theta = \\theta - \\eta \\nabla L(\\theta)$\n",
    "\n",
    "**When to use**:\n",
    "- Simple problems\n",
    "- When you want full control\n",
    "- Computer vision (with momentum)\n",
    "\n",
    "**Pros**:\n",
    "- Simple, well-understood\n",
    "- Can generalize well with proper tuning\n",
    "- Memory efficient\n",
    "\n",
    "**Cons**:\n",
    "- Requires careful learning rate tuning\n",
    "- Can be slow to converge\n",
    "- Sensitive to initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD example\n",
    "model = nn.Linear(10, 5)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "print(f\"Optimizer: {optimizer}\")\n",
    "print(f\"\\nParameter groups: {len(optimizer.param_groups)}\")\n",
    "print(f\"Learning rate: {optimizer.param_groups[0]['lr']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 SGD with Momentum\n",
    "\n",
    "**Formula**: \n",
    "$$v_t = \\beta v_{t-1} + \\nabla L(\\theta)$$\n",
    "$$\\theta = \\theta - \\eta v_t$$\n",
    "\n",
    "**Why momentum?** Accelerates in consistent directions, dampens oscillations.\n",
    "\n",
    "Think of a ball rolling down a hill - it builds up speed!\n",
    "\n",
    "**When to use**:\n",
    "- Computer vision (ResNet, VGG, etc.)\n",
    "- When loss surface has valleys\n",
    "\n",
    "**Typical momentum**: 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD with momentum\n",
    "optimizer_momentum = torch.optim.SGD(\n",
    "    model.parameters(), \n",
    "    lr=0.01, \n",
    "    momentum=0.9\n",
    ")\n",
    "\n",
    "print(f\"SGD with momentum: {optimizer_momentum}\")\n",
    "print(\"Momentum helps escape local minima and speeds up convergence!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Adam (Adaptive Moment Estimation)\n",
    "\n",
    "**Formula**: Combines momentum + adaptive learning rates per parameter\n",
    "\n",
    "**When to use**:\n",
    "- **Default choice for most problems**\n",
    "- NLP (Transformers, LLMs)\n",
    "- When you don't want to tune learning rate carefully\n",
    "- Works well out-of-the-box\n",
    "\n",
    "**Pros**:\n",
    "- Adaptive learning rates (different per parameter)\n",
    "- Combines best of momentum and RMSProp\n",
    "- Robust to hyperparameter choices\n",
    "- Usually converges fast\n",
    "\n",
    "**Cons**:\n",
    "- Can sometimes generalize worse than SGD\n",
    "- Uses more memory (maintains extra state)\n",
    "\n",
    "**Typical hyperparameters**: lr=0.001, betas=(0.9, 0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam optimizer\n",
    "optimizer_adam = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=0.001,\n",
    "    betas=(0.9, 0.999),  # momentum parameters\n",
    "    eps=1e-8  # numerical stability\n",
    ")\n",
    "\n",
    "print(f\"Adam: {optimizer_adam}\")\n",
    "print(\"\\nAdam is the 'safe default' - works well for most problems!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 AdamW (Adam with Weight Decay)\n",
    "\n",
    "**What's different?** Fixes weight decay implementation in Adam.\n",
    "\n",
    "**When to use**:\n",
    "- **Prefer this over Adam!**\n",
    "- Modern best practice (used in GPT, BERT, etc.)\n",
    "- When you want L2 regularization\n",
    "\n",
    "**Typical hyperparameters**: lr=0.001, weight_decay=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdamW optimizer (recommended over Adam)\n",
    "optimizer_adamw = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=0.001,\n",
    "    weight_decay=0.01  # L2 regularization\n",
    ")\n",
    "\n",
    "print(f\"AdamW: {optimizer_adamw}\")\n",
    "print(\"\\nAdamW is the modern default - use this instead of Adam!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Guide: Which Optimizer?\n",
    "\n",
    "| Task | Recommended | Why |\n",
    "|------|-------------|-----|\n",
    "| **General/NLP** | AdamW | Robust, adaptive, standard for Transformers |\n",
    "| **Computer Vision** | SGD + Momentum | Better generalization for CNNs |\n",
    "| **Research/New Problems** | Try both | Adam for fast iteration, SGD for final model |\n",
    "| **Small datasets** | AdamW | Less sensitive to hyperparameters |\n",
    "| **Large datasets** | SGD + Momentum | Often better final performance |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Simple Training Loop Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate toy dataset: binary classification\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=1000, noise=0.2, random_state=42)\n",
    "X = torch.FloatTensor(X)\n",
    "y = torch.LongTensor(y)\n",
    "\n",
    "print(f\"X shape: {X.shape}\")  # (1000, 2)\n",
    "print(f\"y shape: {y.shape}\")  # (1000,)\n",
    "print(f\"Classes: {y.unique()}\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[y==0, 0], X[y==0, 1], c='blue', label='Class 0', alpha=0.6)\n",
    "plt.scatter(X[y==1, 0], X[y==1, 1], c='red', label='Class 1', alpha=0.6)\n",
    "plt.xlabel('Feature 1', fontsize=12)\n",
    "plt.ylabel('Feature 2', fontsize=12)\n",
    "plt.title('Two Moons Dataset', fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Create model, loss, optimizer\n",
    "model = SimpleClassifier(input_size=2, hidden_size=16, num_classes=2)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "print(f\"Model:\\n{model}\")\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 100\n",
    "losses = []\n",
    "accuracies = []\n",
    "\n",
    "print(\"Training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    # 1. Forward pass\n",
    "    outputs = model(X)\n",
    "    loss = criterion(outputs, y)\n",
    "    \n",
    "    # 2. Backward pass\n",
    "    optimizer.zero_grad()  # Clear old gradients\n",
    "    loss.backward()        # Compute new gradients\n",
    "    optimizer.step()       # Update weights\n",
    "    \n",
    "    # 3. Track metrics\n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        predicted = outputs.argmax(dim=1)\n",
    "        accuracy = (predicted == y).float().mean()\n",
    "        accuracies.append(accuracy.item())\n",
    "    \n",
    "    # 4. Print progress\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Accuracy: {accuracy.item():.4f}\")\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training progress\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Plot loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(losses, 'b-', linewidth=2)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.title('Training Loss', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(accuracies, 'g-', linewidth=2)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.title('Training Accuracy', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final loss: {losses[-1]:.4f}\")\n",
    "print(f\"Final accuracy: {accuracies[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Train/Validation Split: Why It Matters\n",
    "\n",
    "### The Overfitting Problem\n",
    "\n",
    "**Problem**: Model memorizes training data but fails on new data.\n",
    "\n",
    "**Solution**: Split data into:\n",
    "- **Training set** (70-80%): Used to update weights\n",
    "- **Validation set** (10-15%): Used to evaluate during training\n",
    "- **Test set** (10-15%): Used ONLY at the very end\n",
    "\n",
    "**Why validation?**\n",
    "- Monitor overfitting (train accuracy ↑, val accuracy ↓)\n",
    "- Early stopping (stop when val performance degrades)\n",
    "- Hyperparameter tuning (choose best based on val performance)\n",
    "\n",
    "**Golden rule**: NEVER use test set during training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proper train/val/test split\n",
    "X, y = make_moons(n_samples=1000, noise=0.2, random_state=42)\n",
    "\n",
    "# Split: 70% train, 15% val, 15% test\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.15, random_state=42\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.15/0.85, random_state=42\n",
    ")\n",
    "\n",
    "# Convert to tensors\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "y_train = torch.LongTensor(y_train)\n",
    "X_val = torch.FloatTensor(X_val)\n",
    "y_val = torch.LongTensor(y_val)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "y_test = torch.LongTensor(y_test)\n",
    "\n",
    "print(f\"Training set:   {len(X_train)} samples\")\n",
    "print(f\"Validation set: {len(X_val)} samples\")\n",
    "print(f\"Test set:       {len(X_test)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop with Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create fresh model\n",
    "model = SimpleClassifier(input_size=2, hidden_size=16, num_classes=2)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "num_epochs = 150\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accs = []\n",
    "val_accs = []\n",
    "\n",
    "print(\"Training with validation...\\n\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # ===== TRAINING MODE =====\n",
    "    model.train()  # Enable dropout, batchnorm in training mode\n",
    "    \n",
    "    # Forward pass\n",
    "    train_outputs = model(X_train)\n",
    "    train_loss = criterion(train_outputs, y_train)\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # ===== VALIDATION MODE =====\n",
    "    model.eval()  # Disable dropout, batchnorm in eval mode\n",
    "    \n",
    "    with torch.no_grad():  # Don't compute gradients for validation\n",
    "        # Validation loss\n",
    "        val_outputs = model(X_val)\n",
    "        val_loss = criterion(val_outputs, y_val)\n",
    "        \n",
    "        # Training accuracy\n",
    "        train_pred = train_outputs.argmax(dim=1)\n",
    "        train_acc = (train_pred == y_train).float().mean()\n",
    "        \n",
    "        # Validation accuracy\n",
    "        val_pred = val_outputs.argmax(dim=1)\n",
    "        val_acc = (val_pred == y_val).float().mean()\n",
    "    \n",
    "    # Track metrics\n",
    "    train_losses.append(train_loss.item())\n",
    "    val_losses.append(val_loss.item())\n",
    "    train_accs.append(train_acc.item())\n",
    "    val_accs.append(val_acc.item())\n",
    "    \n",
    "    # Print progress\n",
    "    if (epoch + 1) % 30 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "        print(f\"  Train Loss: {train_loss.item():.4f}, Train Acc: {train_acc.item():.4f}\")\n",
    "        print(f\"  Val Loss:   {val_loss.item():.4f}, Val Acc:   {val_acc.item():.4f}\")\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize train vs validation\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Plot losses\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, 'b-', linewidth=2, label='Train Loss')\n",
    "plt.plot(val_losses, 'r-', linewidth=2, label='Val Loss')\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.title('Loss: Train vs Validation', fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot accuracies\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accs, 'b-', linewidth=2, label='Train Acc')\n",
    "plt.plot(val_accs, 'r-', linewidth=2, label='Val Acc')\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.title('Accuracy: Train vs Validation', fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key observations:\")\n",
    "print(\"1. If train acc >> val acc: OVERFITTING\")\n",
    "print(\"2. If both low: UNDERFITTING (need bigger model or more training)\")\n",
    "print(\"3. If val loss starts increasing: STOP TRAINING (early stopping)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Test Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set (ONLY ONCE!)\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(X_test)\n",
    "    test_loss = criterion(test_outputs, y_test)\n",
    "    test_pred = test_outputs.argmax(dim=1)\n",
    "    test_acc = (test_pred == y_test).float().mean()\n",
    "\n",
    "print(f\"Test Loss: {test_loss.item():.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc.item():.4f}\")\n",
    "print(f\"\\nThis is the final performance estimate!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Mini-Batch Training with DataLoader\n",
    "\n",
    "### Why Mini-Batches?\n",
    "\n",
    "**Options**:\n",
    "1. **Batch Gradient Descent**: Use entire dataset\n",
    "   - Pros: Stable gradients\n",
    "   - Cons: Slow, requires lots of memory\n",
    "\n",
    "2. **Stochastic Gradient Descent**: Use one sample\n",
    "   - Pros: Fast updates\n",
    "   - Cons: Noisy gradients, unstable\n",
    "\n",
    "3. **Mini-Batch Gradient Descent**: Use small batches (16-256)\n",
    "   - Pros: Balance of speed and stability\n",
    "   - Cons: None! This is the standard\n",
    "\n",
    "**Typical batch sizes**: 32, 64, 128, 256\n",
    "\n",
    "### PyTorch DataLoader\n",
    "\n",
    "`DataLoader` handles:\n",
    "- Batching\n",
    "- Shuffling\n",
    "- Parallel data loading\n",
    "- Memory management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets and dataloaders\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")\n",
    "print()\n",
    "\n",
    "# Peek at one batch\n",
    "batch_x, batch_y = next(iter(train_loader))\n",
    "print(f\"Batch shape: {batch_x.shape}\")\n",
    "print(f\"Labels shape: {batch_y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete Training Loop with DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = SimpleClassifier(input_size=2, hidden_size=32, num_classes=2)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 50\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "print(\"Training with DataLoader...\\n\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # ===== TRAINING =====\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    \n",
    "    for batch_x, batch_y in train_loader:\n",
    "        # Forward pass\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track metrics\n",
    "        train_loss += loss.item() * batch_x.size(0)\n",
    "        train_correct += (outputs.argmax(dim=1) == batch_y).sum().item()\n",
    "        train_total += batch_y.size(0)\n",
    "    \n",
    "    # Average over epoch\n",
    "    train_loss /= train_total\n",
    "    train_acc = train_correct / train_total\n",
    "    \n",
    "    # ===== VALIDATION =====\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in val_loader:\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            \n",
    "            val_loss += loss.item() * batch_x.size(0)\n",
    "            val_correct += (outputs.argmax(dim=1) == batch_y).sum().item()\n",
    "            val_total += batch_y.size(0)\n",
    "    \n",
    "    val_loss /= val_total\n",
    "    val_acc = val_correct / val_total\n",
    "    \n",
    "    # Track\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # Print\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "        print(f\"  Train: Loss={train_loss:.4f}, Acc={train_acc:.4f}\")\n",
    "        print(f\"  Val:   Loss={val_loss:.4f}, Acc={val_acc:.4f}\")\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Learning Rate Scheduling\n",
    "\n",
    "### Why Schedule Learning Rate?\n",
    "\n",
    "**Problem**: Fixed learning rate is suboptimal\n",
    "- Too high: Never converge (bouncing around)\n",
    "- Too low: Slow convergence\n",
    "\n",
    "**Solution**: Start high, then gradually decrease\n",
    "\n",
    "### Common Schedules\n",
    "\n",
    "1. **StepLR**: Decrease by factor every N epochs\n",
    "2. **ExponentialLR**: Multiply by factor each epoch\n",
    "3. **CosineAnnealingLR**: Cosine curve (smooth decrease)\n",
    "4. **ReduceLROnPlateau**: Decrease when metric stops improving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: StepLR scheduler\n",
    "model = SimpleClassifier(2, 32, 2)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# Reduce LR by 0.1 every 20 epochs\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer, \n",
    "    step_size=20,  # Every 20 epochs\n",
    "    gamma=0.1      # Multiply by 0.1\n",
    ")\n",
    "\n",
    "# Simulate learning rate schedule\n",
    "lrs = []\n",
    "for epoch in range(100):\n",
    "    lrs.append(optimizer.param_groups[0]['lr'])\n",
    "    scheduler.step()  # Update learning rate\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(lrs, 'b-', linewidth=2)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Learning Rate', fontsize=12)\n",
    "plt.title('StepLR Schedule', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "\n",
    "print(\"Learning rate is reduced every 20 epochs!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: CosineAnnealingLR (smooth decay)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=50,  # Number of epochs\n",
    "    eta_min=0.0001  # Minimum learning rate\n",
    ")\n",
    "\n",
    "# Simulate\n",
    "lrs = []\n",
    "for epoch in range(50):\n",
    "    lrs.append(optimizer.param_groups[0]['lr'])\n",
    "    scheduler.step()\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(lrs, 'g-', linewidth=2)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Learning Rate', fontsize=12)\n",
    "plt.title('CosineAnnealingLR Schedule', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"Smooth decay following cosine curve - popular for transformers!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Mini Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Fix the Broken Training Loop\n",
    "\n",
    "The following training loop has 3 bugs. Find and fix them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BROKEN CODE (DO NOT RUN)\n",
    "model = nn.Linear(10, 5)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "X = torch.randn(100, 10)\n",
    "y = torch.randn(100, 5)\n",
    "\n",
    "for epoch in range(10):\n",
    "    outputs = model(X)\n",
    "    loss = criterion(outputs, y)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # Missing something?\n",
    "    \n",
    "    print(f\"Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your fixed code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "model = nn.Linear(10, 5)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "X = torch.randn(100, 10)\n",
    "y = torch.randn(100, 5)\n",
    "\n",
    "for epoch in range(10):\n",
    "    outputs = model(X)\n",
    "    loss = criterion(outputs, y)\n",
    "    \n",
    "    optimizer.zero_grad()  # BUG 1: Must zero gradients!\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"Loss: {loss.item()}\")  # BUG 2: Use .item() to get scalar\n",
    "\n",
    "print(\"\\nFixed bugs:\")\n",
    "print(\"1. Missing optimizer.zero_grad() - gradients accumulate!\")\n",
    "print(\"2. Printing loss tensor instead of scalar (use .item())\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Implement Early Stopping\n",
    "\n",
    "Add early stopping to the training loop:\n",
    "- Stop training if validation loss doesn't improve for 5 epochs\n",
    "- Save the best model (lowest val loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "import copy\n",
    "\n",
    "model = SimpleClassifier(2, 32, 2)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 5\n",
    "best_val_loss = float('inf')\n",
    "epochs_without_improvement = 0\n",
    "best_model = None\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_outputs = model(X_train)\n",
    "    train_loss = criterion(train_outputs, y_train)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(X_val)\n",
    "        val_loss = criterion(val_outputs, y_val)\n",
    "    \n",
    "    # Early stopping check\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = copy.deepcopy(model.state_dict())\n",
    "        epochs_without_improvement = 0\n",
    "        print(f\"Epoch {epoch+1}: New best! Val Loss = {val_loss:.4f}\")\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "    \n",
    "    # Stop if no improvement\n",
    "    if epochs_without_improvement >= patience:\n",
    "        print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "        print(f\"Best val loss: {best_val_loss:.4f}\")\n",
    "        break\n",
    "\n",
    "# Restore best model\n",
    "model.load_state_dict(best_model)\n",
    "print(\"\\nRestored best model!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Compare Optimizers\n",
    "\n",
    "Train the same model with SGD, SGD+Momentum, Adam, and AdamW.\n",
    "Plot training curves to compare convergence speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "def train_with_optimizer(optimizer_name, lr=0.01):\n",
    "    \"\"\"Train model with specified optimizer\"\"\"\n",
    "    model = SimpleClassifier(2, 32, 2)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Create optimizer\n",
    "    if optimizer_name == 'SGD':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    elif optimizer_name == 'SGD+Momentum':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    elif optimizer_name == 'Adam':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    elif optimizer_name == 'AdamW':\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(50):\n",
    "        model.train()\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "    \n",
    "    return losses\n",
    "\n",
    "# Train with each optimizer\n",
    "results = {}\n",
    "for opt in ['SGD', 'SGD+Momentum', 'Adam', 'AdamW']:\n",
    "    print(f\"Training with {opt}...\")\n",
    "    results[opt] = train_with_optimizer(opt)\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "for opt, losses in results.items():\n",
    "    plt.plot(losses, linewidth=2, label=opt)\n",
    "\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.title('Optimizer Comparison', fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservations:\")\n",
    "print(\"- SGD: Slowest convergence\")\n",
    "print(\"- SGD+Momentum: Faster than vanilla SGD\")\n",
    "print(\"- Adam/AdamW: Fastest convergence (adaptive learning rates)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Comprehensive Exercise: Complete Classification Pipeline\n",
    "\n",
    "Build a complete classification system from scratch:\n",
    "\n",
    "**Task**: Binary classification on make_moons dataset\n",
    "\n",
    "**Requirements**:\n",
    "1. Train/val/test split (70/15/15)\n",
    "2. DataLoader with batch_size=32\n",
    "3. Model: 2 hidden layers (64, 32 neurons), ReLU, dropout=0.2\n",
    "4. Optimizer: AdamW with weight_decay=0.01\n",
    "5. Learning rate scheduler: CosineAnnealingLR\n",
    "6. Train for 100 epochs\n",
    "7. Track train/val loss and accuracy\n",
    "8. Implement early stopping (patience=10)\n",
    "9. Evaluate on test set\n",
    "10. Visualize decision boundary\n",
    "\n",
    "**Bonus**: Add weight initialization and gradient clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your complete solution here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution (will be quite long - this is comprehensive!)\n",
    "# Complete this based on all concepts learned\n",
    "# This would be a great capstone exercise for students\n",
    "\n",
    "print(\"This is left as a comprehensive exercise for the learner!\")\n",
    "print(\"Combine all concepts from topics 1-5 to build a complete pipeline.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Training loop structure**: Forward → Loss → Zero grad → Backward → Step\n",
    "2. **Always zero gradients**: They accumulate by default!\n",
    "3. **Train/val/test split**: Essential to detect overfitting\n",
    "4. **model.train() vs model.eval()**: Switch modes appropriately\n",
    "5. **torch.no_grad()**: Disable gradients during evaluation\n",
    "6. **Optimizers**:\n",
    "   - AdamW: Default choice (robust, fast)\n",
    "   - SGD+Momentum: Computer vision\n",
    "7. **DataLoader**: Handles batching, shuffling, parallel loading\n",
    "8. **Learning rate scheduling**: Improves convergence\n",
    "9. **Early stopping**: Prevents overfitting\n",
    "10. **Monitoring**: Track metrics to diagnose issues\n",
    "\n",
    "### Common Mistakes\n",
    "\n",
    "1. **Forgetting `optimizer.zero_grad()`** → Gradients accumulate!\n",
    "2. **Not switching to eval mode** → Dropout/BatchNorm behave wrong\n",
    "3. **Computing gradients during validation** → Wastes memory\n",
    "4. **Using test set for hyperparameter tuning** → Overfitting to test set\n",
    "5. **Too high learning rate** → Divergence\n",
    "6. **Too low learning rate** → Slow convergence\n",
    "\n",
    "### Training Loop Checklist\n",
    "\n",
    "```python\n",
    "for epoch in range(num_epochs):\n",
    "    # TRAINING\n",
    "    model.train()  ✓\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()  ✓\n",
    "        output = model(batch)  ✓\n",
    "        loss = criterion(output, target)  ✓\n",
    "        loss.backward()  ✓\n",
    "        optimizer.step()  ✓\n",
    "    \n",
    "    # VALIDATION\n",
    "    model.eval()  ✓\n",
    "    with torch.no_grad():  ✓\n",
    "        # Evaluate on validation set\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Congratulations! You've completed the PyTorch fundamentals. You can now:\n",
    "- Build neural networks\n",
    "- Choose appropriate loss functions\n",
    "- Train models properly\n",
    "- Evaluate performance\n",
    "\n",
    "**Next topics** (Intermediate level):\n",
    "- Custom datasets and data augmentation\n",
    "- Convolutional Neural Networks (CNNs)\n",
    "- Transfer learning\n",
    "- Advanced optimization techniques\n",
    "\n",
    "---\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "- [PyTorch Optimization Tutorial](https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html)\n",
    "- [Adam Paper](https://arxiv.org/abs/1412.6980)\n",
    "- [AdamW Paper](https://arxiv.org/abs/1711.05101)\n",
    "- [CS231n: Training Neural Networks](http://cs231n.github.io/neural-networks-3/)\n",
    "- [Learning Rate Scheduling](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
