{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic 2: Automatic Differentiation & Backpropagation\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Understand WHY automatic differentiation is the \"magic\" behind neural networks\n",
    "- Compute gradients using PyTorch's autograd system\n",
    "- Understand the computational graph and how backpropagation works\n",
    "- Know when to use `requires_grad`, `.backward()`, and `.detach()`\n",
    "- Avoid common gradient-related bugs\n",
    "- Implement gradient descent from scratch\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Big Picture: Why Automatic Differentiation?\n",
    "\n",
    "### The Neural Network Training Problem\n",
    "\n",
    "Imagine you're training a neural network to recognize cats. The network makes predictions, but they're wrong at first. How do you improve it?\n",
    "\n",
    "**The solution**: Calculate how much each parameter (weight) contributes to the error, then adjust it to reduce the error. This requires computing **gradients** (derivatives).\n",
    "\n",
    "### The Challenge\n",
    "\n",
    "A modern neural network might have **billions of parameters**. Computing gradients manually is:\n",
    "- **Tedious**: You'd need to derive formulas for every layer type\n",
    "- **Error-prone**: One mistake breaks everything\n",
    "- **Inflexible**: Changing your model means re-deriving everything\n",
    "\n",
    "### The Solution: Automatic Differentiation (Autograd)\n",
    "\n",
    "**Autograd does calculus for you automatically!** You write forward pass code, and PyTorch:\n",
    "1. Builds a computational graph tracking operations\n",
    "2. Computes gradients automatically using the chain rule\n",
    "3. Stores gradients in `.grad` attributes\n",
    "\n",
    "**This is the key innovation that made modern deep learning practical.**\n",
    "\n",
    "### Why PyTorch's Approach?\n",
    "\n",
    "PyTorch uses **dynamic computational graphs** (define-by-run):\n",
    "- Graph is built during execution\n",
    "- Easy to debug (just print values!)\n",
    "- Flexible for dynamic models (different computation per input)\n",
    "- Pythonic and intuitive\n",
    "\n",
    "Compare to TensorFlow 1.x (static graphs): Had to define entire graph first, harder to debug."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Gradients: The Math Behind Learning\n",
    "\n",
    "### What Is a Gradient?\n",
    "\n",
    "A **gradient** tells you how much a function's output changes when you change its input.\n",
    "\n",
    "**Mathematical definition**: For function $f(x)$, the gradient (derivative) is:\n",
    "$$\\frac{df}{dx} = \\lim_{h \\to 0} \\frac{f(x + h) - f(x)}{h}$$\n",
    "\n",
    "**Intuitive meaning**: \n",
    "- Positive gradient: Increasing $x$ increases $f(x)$\n",
    "- Negative gradient: Increasing $x$ decreases $f(x)$\n",
    "- Large gradient: $f(x)$ is very sensitive to $x$\n",
    "- Zero gradient: $f(x)$ is at a minimum, maximum, or saddle point\n",
    "\n",
    "### Why Gradients for Learning?\n",
    "\n",
    "To minimize loss $L(\\theta)$ where $\\theta$ are parameters:\n",
    "1. Compute gradient: $\\frac{\\partial L}{\\partial \\theta}$\n",
    "2. Update parameters: $\\theta_{\\text{new}} = \\theta_{\\text{old}} - \\eta \\frac{\\partial L}{\\partial \\theta}$\n",
    "\n",
    "This is **gradient descent** - we move parameters in the direction that reduces loss!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a function and its gradient\n",
    "# Function: f(x) = x^2\n",
    "# Gradient: df/dx = 2x\n",
    "\n",
    "x = np.linspace(-5, 5, 100)\n",
    "y = x**2\n",
    "gradient = 2*x\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Plot function\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x, y, 'b-', linewidth=2, label='f(x) = x²')\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.ylabel('f(x)', fontsize=12)\n",
    "plt.title('Function: f(x) = x²', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend(fontsize=12)\n",
    "\n",
    "# Mark some points\n",
    "test_points = [-3, 0, 3]\n",
    "for xp in test_points:\n",
    "    yp = xp**2\n",
    "    plt.plot(xp, yp, 'ro', markersize=8)\n",
    "    plt.annotate(f'({xp}, {yp})', xy=(xp, yp), xytext=(xp+0.5, yp+2),\n",
    "                fontsize=10, color='red')\n",
    "\n",
    "# Plot gradient\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x, gradient, 'r-', linewidth=2, label='df/dx = 2x')\n",
    "plt.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "plt.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.ylabel('df/dx', fontsize=12)\n",
    "plt.title('Gradient: df/dx = 2x', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend(fontsize=12)\n",
    "\n",
    "# Mark gradients at test points\n",
    "for xp in test_points:\n",
    "    grad = 2*xp\n",
    "    plt.plot(xp, grad, 'ro', markersize=8)\n",
    "    plt.annotate(f'grad={grad}', xy=(xp, grad), xytext=(xp+0.5, grad+0.5),\n",
    "                fontsize=10, color='red')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key observations:\")\n",
    "print(\"1. At x=0: gradient=0 (minimum point)\")\n",
    "print(\"2. At x=-3: gradient=-6 (negative, so f(x) is decreasing)\")\n",
    "print(\"3. At x=3: gradient=6 (positive, so f(x) is increasing)\")\n",
    "print(\"\\nFor gradient descent: move opposite to gradient direction!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. PyTorch's Autograd: Basic Usage\n",
    "\n",
    "### The Core Concept: `requires_grad=True`\n",
    "\n",
    "Tell PyTorch which tensors you want gradients for by setting `requires_grad=True`.\n",
    "\n",
    "**When to use**:\n",
    "- Model parameters (weights, biases): `requires_grad=True`\n",
    "- Input data: Usually `requires_grad=False` (we don't update inputs)\n",
    "- Intermediate computations: Automatically inherit from inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Compute gradient of f(x) = x^2 at x = 3.0\n",
    "\n",
    "# Create a tensor that requires gradient\n",
    "x = torch.tensor(3.0, requires_grad=True)\n",
    "print(f\"x: {x}\")\n",
    "print(f\"x.requires_grad: {x.requires_grad}\")\n",
    "print()\n",
    "\n",
    "# Forward pass: compute function\n",
    "y = x ** 2\n",
    "print(f\"y = x^2: {y}\")\n",
    "print(f\"y.requires_grad: {y.requires_grad}\")  # Inherited from x!\n",
    "print()\n",
    "\n",
    "# Backward pass: compute gradient\n",
    "y.backward()  # Computes dy/dx\n",
    "\n",
    "# Access gradient\n",
    "print(f\"Gradient dy/dx: {x.grad}\")\n",
    "print(f\"Expected (2*x): {2*3.0}\")\n",
    "print(f\"Match: {x.grad.item() == 6.0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Just Happened?\n",
    "\n",
    "1. **Forward pass**: PyTorch built a computational graph:\n",
    "   ```\n",
    "   x (leaf) → [PowBackward] → y\n",
    "   ```\n",
    "\n",
    "2. **Backward pass**: Starting from `y`, PyTorch:\n",
    "   - Applied chain rule backwards through graph\n",
    "   - Computed $\\frac{dy}{dx} = 2x = 6$\n",
    "   - Stored result in `x.grad`\n",
    "\n",
    "3. **Key point**: You wrote `x**2`, PyTorch figured out the derivative automatically!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More complex example: f(x, y) = x^2 + 3xy + y^2\n",
    "\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "# Forward pass\n",
    "z = x**2 + 3*x*y + y**2\n",
    "print(f\"z = x^2 + 3xy + y^2 = {z}\")\n",
    "print()\n",
    "\n",
    "# Backward pass\n",
    "z.backward()\n",
    "\n",
    "# Gradients\n",
    "print(f\"dz/dx: {x.grad}\")\n",
    "print(f\"Expected (2x + 3y): {2*2.0 + 3*3.0}\")\n",
    "print()\n",
    "\n",
    "print(f\"dz/dy: {y.grad}\")\n",
    "print(f\"Expected (3x + 2y): {3*2.0 + 2*3.0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. The Computational Graph\n",
    "\n",
    "### What Is the Computational Graph?\n",
    "\n",
    "PyTorch builds a **directed acyclic graph (DAG)** where:\n",
    "- **Nodes**: Tensors (data)\n",
    "- **Edges**: Operations (functions)\n",
    "- **Direction**: Forward (data flow) and backward (gradient flow)\n",
    "\n",
    "### Example Graph\n",
    "\n",
    "For: `z = (x + y) * w`\n",
    "\n",
    "```\n",
    "Forward:\n",
    "x ──┐\n",
    "    ├─→ [Add] ─→ a ─→ [Mul] ─→ z\n",
    "y ──┘              ↑\n",
    "                   │\n",
    "w ─────────────────┘\n",
    "\n",
    "Backward:\n",
    "         ∂z/∂x ←── [AddBackward] ←┐\n",
    "         ∂z/∂y ←──────────────────┤\n",
    "         ∂z/∂w ←── [MulBackward] ←┘ ── ∂z/∂z = 1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a computational graph\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = torch.tensor(3.0, requires_grad=True)\n",
    "w = torch.tensor(4.0, requires_grad=True)\n",
    "\n",
    "# Forward: z = (x + y) * w\n",
    "a = x + y      # Intermediate node\n",
    "z = a * w      # Output\n",
    "\n",
    "print(\"Computational Graph:\")\n",
    "print(f\"a = x + y = {a}\")\n",
    "print(f\"z = a * w = {z}\")\n",
    "print()\n",
    "\n",
    "# Inspect the graph\n",
    "print(\"Graph structure:\")\n",
    "print(f\"z.grad_fn: {z.grad_fn}\")\n",
    "print(f\"a.grad_fn: {a.grad_fn}\")\n",
    "print()\n",
    "\n",
    "# Backward pass\n",
    "z.backward()\n",
    "\n",
    "# Gradients via chain rule:\n",
    "# dz/dx = dz/da * da/dx = w * 1 = 4\n",
    "# dz/dy = dz/da * da/dy = w * 1 = 4  \n",
    "# dz/dw = a * 1 = 5\n",
    "\n",
    "print(\"Gradients:\")\n",
    "print(f\"dz/dx: {x.grad} (expected: {w.item()})\")\n",
    "print(f\"dz/dy: {y.grad} (expected: {w.item()})\")\n",
    "print(f\"dz/dw: {w.grad} (expected: {a.item()})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Graphs: The PyTorch Advantage\n",
    "\n",
    "The graph is built **during execution**, so you can use normal Python control flow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic graph example: different computation based on input\n",
    "def dynamic_function(x, condition):\n",
    "    if condition:\n",
    "        return x ** 2  # Graph: x -> [Pow] -> output\n",
    "    else:\n",
    "        return x ** 3  # Graph: x -> [Pow] -> output\n",
    "\n",
    "# Test both paths\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "# Path 1: x^2\n",
    "y1 = dynamic_function(x, condition=True)\n",
    "y1.backward()\n",
    "print(f\"Path 1 (x^2): gradient = {x.grad} (expected: 2x = 4)\")\n",
    "\n",
    "# Reset gradient\n",
    "x.grad.zero_()\n",
    "\n",
    "# Path 2: x^3\n",
    "y2 = dynamic_function(x, condition=False)\n",
    "y2.backward()\n",
    "print(f\"Path 2 (x^3): gradient = {x.grad} (expected: 3x^2 = 12)\")\n",
    "\n",
    "print(\"\\nThis flexibility is why PyTorch is great for research!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. The Chain Rule: How Backpropagation Works\n",
    "\n",
    "### The Chain Rule in Calculus\n",
    "\n",
    "For composite functions: $f(g(x))$\n",
    "\n",
    "$$\\frac{df}{dx} = \\frac{df}{dg} \\cdot \\frac{dg}{dx}$$\n",
    "\n",
    "### In Neural Networks\n",
    "\n",
    "For a 3-layer network: input → layer1 → layer2 → layer3 → loss\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w_1} = \\frac{\\partial L}{\\partial \\text{layer3}} \\cdot \\frac{\\partial \\text{layer3}}{\\partial \\text{layer2}} \\cdot \\frac{\\partial \\text{layer2}}{\\partial \\text{layer1}} \\cdot \\frac{\\partial \\text{layer1}}{\\partial w_1}$$\n",
    "\n",
    "**Backpropagation = Efficient application of chain rule**\n",
    "\n",
    "### Why \"Backpropagation\"?\n",
    "\n",
    "We compute gradients starting from the output and propagating **backwards** through the network:\n",
    "1. Start at loss: $\\frac{\\partial L}{\\partial L} = 1$\n",
    "2. Compute gradients layer by layer going backwards\n",
    "3. Each layer uses the gradient from the next layer (chain rule!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual chain rule example: f(g(x)) = (x^2 + 1)^3\n",
    "# Let g(x) = x^2 + 1, f(g) = g^3\n",
    "# df/dx = df/dg * dg/dx = 3g^2 * 2x = 3(x^2 + 1)^2 * 2x\n",
    "\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "# Forward pass\n",
    "g = x**2 + 1        # g = 5\n",
    "f = g**3            # f = 125\n",
    "\n",
    "print(f\"x: {x.item()}\")\n",
    "print(f\"g = x^2 + 1: {g.item()}\")\n",
    "print(f\"f = g^3: {f.item()}\")\n",
    "print()\n",
    "\n",
    "# Automatic differentiation\n",
    "f.backward()\n",
    "\n",
    "# Manual calculation\n",
    "df_dg = 3 * g**2     # = 3 * 25 = 75\n",
    "dg_dx = 2 * x        # = 4\n",
    "df_dx_manual = df_dg * dg_dx  # = 75 * 4 = 300\n",
    "\n",
    "print(\"Chain rule breakdown:\")\n",
    "print(f\"df/dg = 3g^2: {df_dg.item()}\")\n",
    "print(f\"dg/dx = 2x: {dg_dx.item()}\")\n",
    "print(f\"df/dx = df/dg * dg/dx: {df_dx_manual.item()}\")\n",
    "print()\n",
    "print(f\"PyTorch computed: {x.grad.item()}\")\n",
    "print(f\"Match: {torch.isclose(x.grad, df_dx_manual)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Vector and Matrix Gradients\n",
    "\n",
    "### Jacobian Matrix\n",
    "\n",
    "For vector function $\\mathbf{y} = f(\\mathbf{x})$ where $\\mathbf{x} \\in \\mathbb{R}^n$, $\\mathbf{y} \\in \\mathbb{R}^m$:\n",
    "\n",
    "The Jacobian is:\n",
    "$$J = \\begin{bmatrix}\n",
    "\\frac{\\partial y_1}{\\partial x_1} & \\cdots & \\frac{\\partial y_1}{\\partial x_n} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial y_m}{\\partial x_1} & \\cdots & \\frac{\\partial y_m}{\\partial x_n}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "Neural networks work with vectors/matrices, not scalars. Understanding vector gradients helps you:\n",
    "- Debug shape mismatches\n",
    "- Understand backpropagation in matrix form\n",
    "- Implement custom layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector-to-scalar function: most common in ML (loss functions)\n",
    "# Example: L(x) = sum(x^2) where x is a vector\n",
    "\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "print(f\"x: {x}\")\n",
    "\n",
    "# Compute loss (scalar output)\n",
    "L = (x ** 2).sum()  # L = 1 + 4 + 9 = 14\n",
    "print(f\"L = sum(x^2): {L}\")\n",
    "print()\n",
    "\n",
    "# Backward pass\n",
    "L.backward()\n",
    "\n",
    "# Gradient is a vector: dL/dx = [2x1, 2x2, 2x3]\n",
    "print(f\"Gradient dL/dx: {x.grad}\")\n",
    "print(f\"Expected [2x1, 2x2, 2x3]: {2*x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix gradient example: Linear layer y = Wx\n",
    "# W: (2, 3) weight matrix\n",
    "# x: (3,) input vector  \n",
    "# y: (2,) output vector\n",
    "\n",
    "W = torch.tensor([[1.0, 2.0, 3.0],\n",
    "                  [4.0, 5.0, 6.0]], requires_grad=True)\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "\n",
    "# Forward pass\n",
    "y = W @ x  # Matrix-vector multiplication\n",
    "print(f\"y = Wx: {y}\")\n",
    "\n",
    "# To compute gradients, we need a scalar loss\n",
    "loss = y.sum()  # Simple loss: sum of outputs\n",
    "print(f\"loss = sum(y): {loss}\")\n",
    "print()\n",
    "\n",
    "# Backward pass\n",
    "loss.backward()\n",
    "\n",
    "print(f\"Gradient dL/dx: {x.grad}\")\n",
    "print(f\"Shape: {x.grad.shape}\")\n",
    "print()\n",
    "\n",
    "print(f\"Gradient dL/dW:\\n{W.grad}\")\n",
    "print(f\"Shape: {W.grad.shape}\")\n",
    "\n",
    "# Key insight: Gradients have the same shape as the original tensors!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important: Non-Scalar Outputs\n",
    "\n",
    "`backward()` only works on scalars by default. For vector outputs, you need to provide a gradient vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector output requires gradient argument\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "y = x ** 2  # Vector output: [1, 4, 9]\n",
    "\n",
    "print(f\"y: {y}\")\n",
    "print(f\"y.shape: {y.shape}\")\n",
    "print()\n",
    "\n",
    "# This would fail:\n",
    "# y.backward()  # RuntimeError: grad can be implicitly created only for scalar outputs\n",
    "\n",
    "# Need to provide gradient vector (often called \"vector-Jacobian product\")\n",
    "# Interpretation: How much does each output affect our final scalar?\n",
    "# For equal weighting: use ones\n",
    "gradient_vector = torch.ones_like(y)\n",
    "y.backward(gradient=gradient_vector)\n",
    "\n",
    "print(f\"Gradient: {x.grad}\")\n",
    "print(f\"Expected [2x1, 2x2, 2x3]: {2*x}\")\n",
    "\n",
    "# In practice, you usually have a scalar loss, so this is automatic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Common Autograd Operations\n",
    "\n",
    "### Gradient Accumulation\n",
    "\n",
    "**Important**: PyTorch **accumulates** gradients by default (adds to existing `.grad`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate gradient accumulation\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "# First computation\n",
    "y1 = x ** 2\n",
    "y1.backward()\n",
    "print(f\"After first backward: x.grad = {x.grad}\")\n",
    "\n",
    "# Second computation (without zeroing gradients!)\n",
    "y2 = x ** 3\n",
    "y2.backward()\n",
    "print(f\"After second backward: x.grad = {x.grad}\")  # 4 + 12 = 16\n",
    "print()\n",
    "\n",
    "# This is why we need to zero gradients in training loops!\n",
    "x.grad.zero_()  # Reset to zero\n",
    "print(f\"After zero_(): x.grad = {x.grad}\")\n",
    "\n",
    "# Or set to None (more memory efficient)\n",
    "x.grad = None\n",
    "print(f\"After setting None: x.grad = {x.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detaching from the Graph\n",
    "\n",
    "Sometimes you want to use a tensor's value without tracking gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detach example: Using a computed value without gradients\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = x ** 2\n",
    "\n",
    "# Detach y from the computational graph\n",
    "y_detached = y.detach()\n",
    "\n",
    "print(f\"y: {y}, requires_grad: {y.requires_grad}\")\n",
    "print(f\"y_detached: {y_detached}, requires_grad: {y_detached.requires_grad}\")\n",
    "print()\n",
    "\n",
    "# Use detached value in new computation\n",
    "z = y_detached ** 2  # z = (x^2)^2 = x^4, but gradient stops at y\n",
    "\n",
    "z.backward()\n",
    "print(f\"x.grad: {x.grad}\")  # None! Gradient flow was stopped\n",
    "\n",
    "# Common use cases:\n",
    "# 1. Computing metrics during training (accuracy, etc.)\n",
    "# 2. Implementing stop-gradient techniques\n",
    "# 3. Target networks in RL (frozen copies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No-Grad Context\n",
    "\n",
    "Disable gradient computation for a block of code (more efficient)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.no_grad() context manager\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "# Normal computation (tracks gradients)\n",
    "y1 = x ** 2\n",
    "print(f\"y1.requires_grad: {y1.requires_grad}\")\n",
    "\n",
    "# Inside no_grad block\n",
    "with torch.no_grad():\n",
    "    y2 = x ** 2\n",
    "    print(f\"y2.requires_grad: {y2.requires_grad}\")\n",
    "    \n",
    "# Use cases:\n",
    "# 1. Evaluation/inference (don't need gradients)\n",
    "# 2. Updating parameters (optimizer.step() uses this internally)\n",
    "# 3. Computing metrics\n",
    "\n",
    "print(\"\\nBenefit: Saves memory and speeds up computation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Gradient Descent from Scratch\n",
    "\n",
    "Now let's implement gradient descent manually to see how it all fits together!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem: Find minimum of f(x) = (x - 3)^2 + 5\n",
    "# We know analytically: minimum at x = 3, value = 5\n",
    "# Let's use gradient descent to find it!\n",
    "\n",
    "def f(x):\n",
    "    \"\"\"Function to minimize\"\"\"\n",
    "    return (x - 3)**2 + 5\n",
    "\n",
    "# Initialize parameter\n",
    "x = torch.tensor(0.0, requires_grad=True)  # Start at x=0\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.1\n",
    "n_iterations = 50\n",
    "\n",
    "# Track history for visualization\n",
    "history_x = []\n",
    "history_f = []\n",
    "\n",
    "print(\"Iteration | x      | f(x)   | gradient\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    # Forward pass\n",
    "    loss = f(x)\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Save history\n",
    "    history_x.append(x.item())\n",
    "    history_f.append(loss.item())\n",
    "    \n",
    "    # Print progress\n",
    "    if i % 10 == 0:\n",
    "        print(f\"{i:9d} | {x.item():6.3f} | {loss.item():6.3f} | {x.grad.item():8.3f}\")\n",
    "    \n",
    "    # Update parameters (gradient descent step)\n",
    "    with torch.no_grad():\n",
    "        x -= learning_rate * x.grad  # x_new = x_old - lr * gradient\n",
    "    \n",
    "    # Zero gradients for next iteration\n",
    "    x.grad.zero_()\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"\\nFinal result:\")\n",
    "print(f\"x = {x.item():.6f} (expected: 3.0)\")\n",
    "print(f\"f(x) = {f(x).item():.6f} (expected: 5.0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize gradient descent\n",
    "x_plot = np.linspace(-1, 6, 100)\n",
    "y_plot = (x_plot - 3)**2 + 5\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot 1: Function and optimization path\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x_plot, y_plot, 'b-', linewidth=2, label='f(x) = (x-3)² + 5')\n",
    "plt.plot(history_x, history_f, 'ro-', markersize=4, linewidth=1, label='Gradient descent path')\n",
    "plt.plot(history_x[0], history_f[0], 'go', markersize=10, label='Start')\n",
    "plt.plot(history_x[-1], history_f[-1], 'r*', markersize=15, label='End')\n",
    "plt.plot(3, 5, 'bs', markersize=10, label='True minimum')\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.ylabel('f(x)', fontsize=12)\n",
    "plt.title('Gradient Descent Optimization', fontsize=14)\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Loss over iterations\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_f, 'b-', linewidth=2)\n",
    "plt.axhline(y=5, color='r', linestyle='--', label='Minimum value')\n",
    "plt.xlabel('Iteration', fontsize=12)\n",
    "plt.ylabel('f(x)', fontsize=12)\n",
    "plt.title('Loss vs Iterations', fontsize=14)\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Observations:\")\n",
    "print(\"1. Started far from minimum (x=0)\")\n",
    "print(\"2. Gradually converged to x=3\")\n",
    "print(\"3. Larger steps at start (large gradient), smaller near minimum\")\n",
    "print(\"4. This is how neural networks learn!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Mini Exercises\n",
    "\n",
    "Test your understanding with these exercises!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Compute Gradients\n",
    "\n",
    "Given $f(x, y) = 2x^2 + xy + 3y^2$, compute:\n",
    "1. $\\frac{\\partial f}{\\partial x}$ at $(x=1, y=2)$\n",
    "2. $\\frac{\\partial f}{\\partial y}$ at $(x=1, y=2)$\n",
    "\n",
    "Use PyTorch's autograd and verify against manual calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "y = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "# Forward pass\n",
    "f = 2*x**2 + x*y + 3*y**2\n",
    "print(f\"f(1, 2) = {f.item()}\")\n",
    "print()\n",
    "\n",
    "# Backward pass\n",
    "f.backward()\n",
    "\n",
    "# Results\n",
    "print(f\"df/dx: {x.grad.item()}\")\n",
    "print(f\"df/dy: {y.grad.item()}\")\n",
    "print()\n",
    "\n",
    "# Manual calculation:\n",
    "# df/dx = 4x + y = 4(1) + 2 = 6\n",
    "# df/dy = x + 6y = 1 + 6(2) = 13\n",
    "print(\"Expected:\")\n",
    "print(f\"df/dx = 4x + y = {4*1 + 2}\")\n",
    "print(f\"df/dy = x + 6y = {1 + 6*2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Gradient Accumulation\n",
    "\n",
    "Create a tensor `x = 3.0` and:\n",
    "1. Compute `y = x**2` and get gradient\n",
    "2. **Without resetting gradient**, compute `z = x**3` and get gradient\n",
    "3. What is `x.grad` now? Why?\n",
    "4. Reset gradient and verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "x = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "# 1. First computation\n",
    "y = x**2\n",
    "y.backward()\n",
    "print(f\"After y=x^2: x.grad = {x.grad.item()}\")\n",
    "print(f\"Expected (2x): {2*3}\")\n",
    "print()\n",
    "\n",
    "# 2. Second computation WITHOUT resetting\n",
    "z = x**3\n",
    "z.backward()\n",
    "print(f\"After z=x^3: x.grad = {x.grad.item()}\")\n",
    "print(f\"Expected (2x + 3x^2): {2*3 + 3*3**2}\")\n",
    "print()\n",
    "\n",
    "# 3. Explanation\n",
    "print(\"Why? Gradients accumulate!\")\n",
    "print(\"grad from y = 6\")\n",
    "print(\"grad from z = 27\") \n",
    "print(\"total = 6 + 27 = 33\")\n",
    "print()\n",
    "\n",
    "# 4. Reset and verify\n",
    "x.grad.zero_()\n",
    "z = x**3\n",
    "z.backward()\n",
    "print(f\"After reset and z=x^3: x.grad = {x.grad.item()}\")\n",
    "print(f\"Expected (3x^2): {3*3**2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Matrix Gradients\n",
    "\n",
    "Implement a simple linear transformation:\n",
    "- Weight matrix `W` of shape `(3, 4)` (random)\n",
    "- Input vector `x` of shape `(4,)` (random)\n",
    "- Compute `y = Wx`\n",
    "- Loss: `L = (y**2).sum()`\n",
    "- Compute gradients of `L` with respect to `W` and `x`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Create tensors\n",
    "W = torch.randn(3, 4, requires_grad=True)\n",
    "x = torch.randn(4, requires_grad=True)\n",
    "\n",
    "print(f\"W shape: {W.shape}\")\n",
    "print(f\"x shape: {x.shape}\")\n",
    "print()\n",
    "\n",
    "# Forward pass\n",
    "y = W @ x\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"y: {y}\")\n",
    "print()\n",
    "\n",
    "# Loss\n",
    "L = (y**2).sum()\n",
    "print(f\"Loss: {L.item()}\")\n",
    "print()\n",
    "\n",
    "# Backward pass\n",
    "L.backward()\n",
    "\n",
    "# Gradients\n",
    "print(f\"dL/dW shape: {W.grad.shape}\")  # Same as W: (3, 4)\n",
    "print(f\"dL/dW:\\n{W.grad}\")\n",
    "print()\n",
    "\n",
    "print(f\"dL/dx shape: {x.grad.shape}\")  # Same as x: (4,)\n",
    "print(f\"dL/dx: {x.grad}\")\n",
    "\n",
    "print(\"\\nKey insight: Gradient shapes match parameter shapes!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Comprehensive Exercise: Linear Regression with Gradient Descent\n",
    "\n",
    "Implement linear regression from scratch using autograd!\n",
    "\n",
    "**Problem**: Given data points $(x, y)$, find the best line $y = wx + b$\n",
    "\n",
    "**Steps**:\n",
    "1. Generate synthetic data: $y = 3x + 2 + \\text{noise}$\n",
    "2. Initialize parameters `w` and `b` randomly\n",
    "3. For each iteration:\n",
    "   - Compute predictions: `y_pred = w * x + b`\n",
    "   - Compute loss: MSE = $\\frac{1}{n}\\sum(y_{\\text{pred}} - y)^2$\n",
    "   - Compute gradients using `.backward()`\n",
    "   - Update parameters using gradient descent\n",
    "4. Visualize results\n",
    "\n",
    "**Goal**: Recover `w ≈ 3` and `b ≈ 2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# 1. Generate data\n",
    "n_samples = 100\n",
    "true_w = 3.0\n",
    "true_b = 2.0\n",
    "\n",
    "x = torch.randn(n_samples)\n",
    "y = true_w * x + true_b + torch.randn(n_samples) * 0.5  # Add noise\n",
    "\n",
    "print(f\"Generated {n_samples} data points\")\n",
    "print(f\"True parameters: w={true_w}, b={true_b}\")\n",
    "print()\n",
    "\n",
    "# 2. Initialize parameters\n",
    "w = torch.randn(1, requires_grad=True)\n",
    "b = torch.randn(1, requires_grad=True)\n",
    "\n",
    "print(f\"Initial w: {w.item():.3f}\")\n",
    "print(f\"Initial b: {b.item():.3f}\")\n",
    "print()\n",
    "\n",
    "# 3. Training loop\n",
    "learning_rate = 0.01\n",
    "n_iterations = 200\n",
    "losses = []\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    # Forward pass\n",
    "    y_pred = w * x + b\n",
    "    \n",
    "    # Compute loss (MSE)\n",
    "    loss = ((y_pred - y)**2).mean()\n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update parameters\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "        b -= learning_rate * b.grad\n",
    "    \n",
    "    # Zero gradients\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "    \n",
    "    # Print progress\n",
    "    if (i+1) % 40 == 0:\n",
    "        print(f\"Iter {i+1}: loss={loss.item():.4f}, w={w.item():.3f}, b={b.item():.3f}\")\n",
    "\n",
    "print()\n",
    "print(f\"Final w: {w.item():.3f} (true: {true_w})\")\n",
    "print(f\"Final b: {b.item():.3f} (true: {true_b})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Visualize results\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Data and fitted line\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(x.numpy(), y.numpy(), alpha=0.5, label='Data')\n",
    "\n",
    "# True line\n",
    "x_line = torch.linspace(x.min(), x.max(), 100)\n",
    "y_true_line = true_w * x_line + true_b\n",
    "plt.plot(x_line.numpy(), y_true_line.numpy(), 'g--', linewidth=2, label='True line')\n",
    "\n",
    "# Learned line\n",
    "with torch.no_grad():\n",
    "    y_learned_line = w * x_line + b\n",
    "plt.plot(x_line.numpy(), y_learned_line.numpy(), 'r-', linewidth=2, label='Learned line')\n",
    "\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.ylabel('y', fontsize=12)\n",
    "plt.title('Linear Regression: Data and Fit', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Loss curve\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(losses, 'b-', linewidth=2)\n",
    "plt.xlabel('Iteration', fontsize=12)\n",
    "plt.ylabel('Loss (MSE)', fontsize=12)\n",
    "plt.title('Training Loss', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Success! We've implemented linear regression using only autograd.\")\n",
    "print(\"This is the foundation of how all neural networks learn!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Autograd is the magic**: Automatically computes gradients using chain rule\n",
    "2. **Computational graph**: PyTorch tracks operations to enable backpropagation\n",
    "3. **Dynamic graphs**: Built during execution, making PyTorch flexible and Pythonic\n",
    "4. **requires_grad=True**: Mark tensors you want gradients for\n",
    "5. **.backward()**: Computes all gradients in one call\n",
    "6. **Gradients accumulate**: Always zero them between iterations!\n",
    "7. **torch.no_grad()**: Disable gradient tracking for efficiency\n",
    "8. **Gradient descent**: The fundamental optimization algorithm for neural networks\n",
    "\n",
    "### Connection to Neural Networks\n",
    "\n",
    "What we learned:\n",
    "- Parameters have `requires_grad=True`\n",
    "- Forward pass computes loss\n",
    "- `.backward()` computes gradients\n",
    "- Update parameters: `param -= lr * param.grad`\n",
    "- Zero gradients for next iteration\n",
    "\n",
    "This is **exactly** how neural networks train! The only difference is scale and abstraction.\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "You now understand the engine behind neural network training! Next, we'll build actual neural networks using PyTorch's `nn.Module`.\n",
    "\n",
    "Continue to: [Topic 3: Building Neural Networks with nn.Module](03_neural_networks.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "- [PyTorch Autograd Documentation](https://pytorch.org/docs/stable/autograd.html)\n",
    "- [Automatic Differentiation Explained](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html)\n",
    "- [Calculus on Computational Graphs](http://colah.github.io/posts/2015-08-Backprop/)\n",
    "- [CS231n: Backpropagation](http://cs231n.github.io/optimization-2/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
