{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic 4: Loss Functions - A Comprehensive Guide\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Understand WHY loss functions are the \"North Star\" of neural network training\n",
    "- Know WHEN to use each major loss function (with real-world scenarios)\n",
    "- Understand the mathematics behind common loss functions\n",
    "- Implement custom loss functions\n",
    "- Avoid common loss function mistakes\n",
    "- Combine multiple loss functions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Big Picture: Why Loss Functions?\n",
    "\n",
    "### The Training Problem\n",
    "\n",
    "You've built a neural network, but how do you make it better? You need:\n",
    "1. A way to **measure** how wrong your predictions are\n",
    "2. A **single number** to optimize (can't optimize multiple conflicting goals)\n",
    "3. A function that's **differentiable** (so we can compute gradients)\n",
    "\n",
    "**This is the loss function!**\n",
    "\n",
    "### Loss Function = Objective Function = Cost Function\n",
    "\n",
    "Different names, same concept:\n",
    "- **Loss**: How much error on one example\n",
    "- **Cost**: Average loss over dataset\n",
    "- **Objective**: What we're trying to minimize\n",
    "\n",
    "### The Training Loop (Preview)\n",
    "\n",
    "```python\n",
    "for epoch in range(num_epochs):\n",
    "    # 1. Forward pass: make predictions\n",
    "    predictions = model(inputs)\n",
    "    \n",
    "    # 2. Compute loss: how wrong are we?\n",
    "    loss = loss_function(predictions, targets)\n",
    "    \n",
    "    # 3. Backward pass: compute gradients\n",
    "    loss.backward()\n",
    "    \n",
    "    # 4. Update weights\n",
    "    optimizer.step()\n",
    "```\n",
    "\n",
    "**Loss is the bridge between predictions and learning!**\n",
    "\n",
    "### Key Principle: Choose Loss Based on Problem Type\n",
    "\n",
    "| Problem Type | Loss Function | Why |\n",
    "|-------------|---------------|-----|\n",
    "| Binary Classification | BCELoss, BCEWithLogitsLoss | Probability of two classes |\n",
    "| Multi-class Classification | CrossEntropyLoss | Probability distribution over classes |\n",
    "| Regression | MSELoss, L1Loss, SmoothL1Loss | Minimize distance to target |\n",
    "| Ranking/Similarity | ContrastiveLoss, TripletLoss | Learn embeddings |\n",
    "\n",
    "We'll explore each in detail!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Set random seed\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Regression Loss Functions\n",
    "\n",
    "### When to Use Regression?\n",
    "\n",
    "Predicting **continuous values**:\n",
    "- House prices\n",
    "- Temperature\n",
    "- Stock prices\n",
    "- Age estimation\n",
    "- Object coordinates (bounding boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Mean Squared Error (MSE) Loss\n",
    "\n",
    "**Formula**: $\\text{MSE} = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2$\n",
    "\n",
    "**When to use**:\n",
    "- Standard regression problems\n",
    "- When large errors should be penalized heavily (due to squaring)\n",
    "- When your target values are continuous and unbounded\n",
    "\n",
    "**Pros**:\n",
    "- Smooth, differentiable everywhere\n",
    "- Heavily penalizes large errors\n",
    "- Well-studied, standard choice\n",
    "\n",
    "**Cons**:\n",
    "- Sensitive to outliers (squared term)\n",
    "- Can explode with large errors\n",
    "\n",
    "**Real-world scenarios**:\n",
    "- Predicting house prices (no extreme outliers)\n",
    "- Image reconstruction (pixel values 0-255)\n",
    "- Temperature forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE Loss example\n",
    "predictions = torch.tensor([2.5, 0.0, 2.0, 8.0])\n",
    "targets = torch.tensor([3.0, -0.5, 2.0, 7.0])\n",
    "\n",
    "# Method 1: Using nn.MSELoss\n",
    "mse_loss = nn.MSELoss()\n",
    "loss = mse_loss(predictions, targets)\n",
    "print(f\"MSE Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Method 2: Manual calculation\n",
    "manual_loss = ((predictions - targets) ** 2).mean()\n",
    "print(f\"Manual MSE: {manual_loss.item():.4f}\")\n",
    "\n",
    "# Show individual squared errors\n",
    "squared_errors = (predictions - targets) ** 2\n",
    "print(f\"\\nIndividual squared errors: {squared_errors}\")\n",
    "print(f\"Mean: {squared_errors.mean().item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Mean Absolute Error (MAE) / L1 Loss\n",
    "\n",
    "**Formula**: $\\text{MAE} = \\frac{1}{n}\\sum_{i=1}^{n}|y_i - \\hat{y}_i|$\n",
    "\n",
    "**When to use**:\n",
    "- When your data has outliers\n",
    "- When all errors should be weighted equally (no squaring)\n",
    "- When you want more robust predictions\n",
    "\n",
    "**Pros**:\n",
    "- Robust to outliers\n",
    "- Same units as your target variable (interpretable)\n",
    "- Uniform penalty for errors\n",
    "\n",
    "**Cons**:\n",
    "- Not differentiable at zero (can slow convergence)\n",
    "- Doesn't penalize large errors as much as MSE\n",
    "\n",
    "**Real-world scenarios**:\n",
    "- Predicting house prices with outliers (mansions)\n",
    "- Financial forecasting (outlier events common)\n",
    "- Sensor data with occasional bad readings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1 Loss example\n",
    "predictions = torch.tensor([2.5, 0.0, 2.0, 8.0])\n",
    "targets = torch.tensor([3.0, -0.5, 2.0, 7.0])\n",
    "\n",
    "# Using nn.L1Loss\n",
    "l1_loss = nn.L1Loss()\n",
    "loss = l1_loss(predictions, targets)\n",
    "print(f\"L1 Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Manual calculation\n",
    "manual_loss = (predictions - targets).abs().mean()\n",
    "print(f\"Manual L1: {manual_loss.item():.4f}\")\n",
    "\n",
    "# Compare with MSE\n",
    "mse = nn.MSELoss()(predictions, targets)\n",
    "print(f\"\\nFor comparison, MSE: {mse.item():.4f}\")\n",
    "print(f\"L1 is smaller because it doesn't square the errors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Smooth L1 Loss (Huber Loss)\n",
    "\n",
    "**Formula**: \n",
    "$$\\text{SmoothL1}(x) = \\begin{cases}\n",
    "0.5x^2 & \\text{if } |x| < 1 \\\\\n",
    "|x| - 0.5 & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "**When to use**:\n",
    "- Best of both worlds: MSE for small errors, L1 for large errors\n",
    "- Object detection (bounding box regression)\n",
    "- When you want robustness to outliers but smooth gradients\n",
    "\n",
    "**Pros**:\n",
    "- Combines advantages of MSE and L1\n",
    "- Smooth gradients everywhere\n",
    "- Less sensitive to outliers than MSE\n",
    "\n",
    "**Cons**:\n",
    "- Slightly more complex\n",
    "- Threshold parameter (beta) needs tuning\n",
    "\n",
    "**Real-world scenarios**:\n",
    "- Object detection (Faster R-CNN uses this)\n",
    "- Reinforcement learning (Q-learning)\n",
    "- Any regression with potential outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smooth L1 Loss example\n",
    "predictions = torch.tensor([2.5, 0.0, 2.0, 8.0])\n",
    "targets = torch.tensor([3.0, -0.5, 2.0, 7.0])\n",
    "\n",
    "smooth_l1 = nn.SmoothL1Loss()\n",
    "loss = smooth_l1(predictions, targets)\n",
    "print(f\"Smooth L1 Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Compare all three\n",
    "mse = nn.MSELoss()(predictions, targets)\n",
    "l1 = nn.L1Loss()(predictions, targets)\n",
    "\n",
    "print(f\"\\nComparison:\")\n",
    "print(f\"MSE Loss:      {mse.item():.4f}\")\n",
    "print(f\"L1 Loss:       {l1.item():.4f}\")\n",
    "print(f\"Smooth L1:     {loss.item():.4f}\")\n",
    "print(f\"\\nSmooth L1 is between MSE and L1!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the three regression losses\n",
    "errors = torch.linspace(-3, 3, 100)\n",
    "\n",
    "# Compute losses for different error values\n",
    "mse_values = errors ** 2\n",
    "l1_values = errors.abs()\n",
    "smooth_l1_values = torch.where(\n",
    "    errors.abs() < 1,\n",
    "    0.5 * errors ** 2,\n",
    "    errors.abs() - 0.5\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot losses\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(errors.numpy(), mse_values.numpy(), 'b-', linewidth=2, label='MSE (xÂ²)')\n",
    "plt.plot(errors.numpy(), l1_values.numpy(), 'r-', linewidth=2, label='L1 (|x|)')\n",
    "plt.plot(errors.numpy(), smooth_l1_values.numpy(), 'g-', linewidth=2, label='Smooth L1')\n",
    "plt.xlabel('Error (prediction - target)', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.title('Regression Loss Functions', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "plt.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "\n",
    "# Plot gradients\n",
    "plt.subplot(1, 2, 2)\n",
    "# MSE gradient: 2x\n",
    "mse_grad = 2 * errors\n",
    "# L1 gradient: sign(x)\n",
    "l1_grad = torch.sign(errors)\n",
    "# Smooth L1 gradient\n",
    "smooth_l1_grad = torch.where(\n",
    "    errors.abs() < 1,\n",
    "    errors,\n",
    "    torch.sign(errors)\n",
    ")\n",
    "\n",
    "plt.plot(errors.numpy(), mse_grad.numpy(), 'b-', linewidth=2, label='MSE gradient')\n",
    "plt.plot(errors.numpy(), l1_grad.numpy(), 'r-', linewidth=2, label='L1 gradient')\n",
    "plt.plot(errors.numpy(), smooth_l1_grad.numpy(), 'g-', linewidth=2, label='Smooth L1 gradient')\n",
    "plt.xlabel('Error', fontsize=12)\n",
    "plt.ylabel('Gradient', fontsize=12)\n",
    "plt.title('Loss Gradients', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "plt.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "plt.ylim(-3, 3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key observations:\")\n",
    "print(\"1. MSE grows quadratically (large errors dominate)\")\n",
    "print(\"2. L1 grows linearly (treats all errors equally)\")\n",
    "print(\"3. Smooth L1 combines both (quadratic near 0, linear far away)\")\n",
    "print(\"4. MSE gradient grows linearly (can explode!)\")\n",
    "print(\"5. L1 gradient is constant (can be slow near optimum)\")\n",
    "print(\"6. Smooth L1 gradient is best of both worlds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Binary Classification Loss Functions\n",
    "\n",
    "### When to Use Binary Classification?\n",
    "\n",
    "Predicting **one of two classes**:\n",
    "- Spam detection (spam/not spam)\n",
    "- Medical diagnosis (disease/healthy)\n",
    "- Sentiment analysis (positive/negative)\n",
    "- Fraud detection (fraud/legitimate)\n",
    "- Image contains cat (yes/no)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Binary Cross Entropy (BCE) Loss\n",
    "\n",
    "**Formula**: $\\text{BCE} = -\\frac{1}{n}\\sum_{i=1}^{n}[y_i \\log(\\hat{y}_i) + (1-y_i)\\log(1-\\hat{y}_i)]$\n",
    "\n",
    "**When to use**:\n",
    "- Binary classification where predictions are probabilities (0 to 1)\n",
    "- After sigmoid activation\n",
    "- Multi-label classification (each label independently binary)\n",
    "\n",
    "**Pros**:\n",
    "- Probabilistic interpretation\n",
    "- Works well with sigmoid output\n",
    "- Standard for binary problems\n",
    "\n",
    "**Cons**:\n",
    "- Requires predictions in [0, 1] range\n",
    "- Can be numerically unstable (log of small numbers)\n",
    "\n",
    "**Real-world scenarios**:\n",
    "- Email spam classifier\n",
    "- Medical test (disease present?)\n",
    "- Ad click prediction (will user click?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BCE Loss example\n",
    "# Predictions MUST be probabilities (0-1)\n",
    "predictions = torch.tensor([0.9, 0.2, 0.8, 0.1])  # After sigmoid\n",
    "targets = torch.tensor([1.0, 0.0, 1.0, 0.0])     # Ground truth labels\n",
    "\n",
    "bce_loss = nn.BCELoss()\n",
    "loss = bce_loss(predictions, targets)\n",
    "print(f\"BCE Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Manual calculation\n",
    "manual_loss = -(targets * torch.log(predictions) + \n",
    "                (1 - targets) * torch.log(1 - predictions)).mean()\n",
    "print(f\"Manual BCE: {manual_loss.item():.4f}\")\n",
    "\n",
    "# Show why this makes sense\n",
    "print(f\"\\nBreakdown:\")\n",
    "for i in range(len(predictions)):\n",
    "    pred, target = predictions[i].item(), targets[i].item()\n",
    "    if target == 1:\n",
    "        loss_i = -np.log(pred)\n",
    "        print(f\"Sample {i}: target=1, pred={pred:.2f} â loss={loss_i:.4f}\")\n",
    "    else:\n",
    "        loss_i = -np.log(1 - pred)\n",
    "        print(f\"Sample {i}: target=0, pred={pred:.2f} â loss={loss_i:.4f}\")\n",
    "\n",
    "print(f\"\\nKey insight: High confidence correct predictions have low loss!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 BCE With Logits Loss (RECOMMENDED)\n",
    "\n",
    "**Formula**: Combines sigmoid + BCE in one function\n",
    "\n",
    "**When to use**:\n",
    "- **Always prefer this over BCE Loss!**\n",
    "- Binary classification with raw network outputs (logits)\n",
    "- More numerically stable than sigmoid â BCE\n",
    "\n",
    "**Pros**:\n",
    "- Numerically stable (uses log-sum-exp trick)\n",
    "- No need to apply sigmoid manually\n",
    "- Better gradients\n",
    "\n",
    "**Cons**:\n",
    "- None! Always use this over BCE\n",
    "\n",
    "**Real-world scenarios**:\n",
    "- Same as BCE, but this is the preferred implementation\n",
    "\n",
    "**Important**: Your model should output logits (raw values), not probabilities!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BCE With Logits example\n",
    "# Predictions are RAW outputs (logits), not probabilities\n",
    "logits = torch.tensor([2.5, -1.0, 1.5, -2.0])  # Raw network output\n",
    "targets = torch.tensor([1.0, 0.0, 1.0, 0.0])\n",
    "\n",
    "bce_with_logits = nn.BCEWithLogitsLoss()\n",
    "loss = bce_with_logits(logits, targets)\n",
    "print(f\"BCE With Logits Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Compare with manual sigmoid + BCE\n",
    "predictions = torch.sigmoid(logits)\n",
    "bce = nn.BCELoss()(predictions, targets)\n",
    "print(f\"Sigmoid + BCE Loss:   {bce.item():.4f}\")\n",
    "print(f\"\\nThey're the same! But BCEWithLogitsLoss is more stable.\")\n",
    "\n",
    "# Show the probabilities for interpretation\n",
    "print(f\"\\nLogits â Probabilities:\")\n",
    "for i, (logit, prob, target) in enumerate(zip(logits, predictions, targets)):\n",
    "    print(f\"Sample {i}: logit={logit:.2f} â prob={prob:.4f}, target={target:.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize BCE loss\n",
    "predictions = torch.linspace(0.01, 0.99, 100)  # Avoid 0 and 1 (log undefined)\n",
    "\n",
    "# Loss when target = 1\n",
    "loss_target_1 = -torch.log(predictions)\n",
    "\n",
    "# Loss when target = 0\n",
    "loss_target_0 = -torch.log(1 - predictions)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot losses\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(predictions.numpy(), loss_target_1.numpy(), 'b-', linewidth=2, label='Target = 1')\n",
    "plt.plot(predictions.numpy(), loss_target_0.numpy(), 'r-', linewidth=2, label='Target = 0')\n",
    "plt.xlabel('Prediction (probability)', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.title('Binary Cross Entropy Loss', fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim(0, 5)\n",
    "\n",
    "# Add annotations\n",
    "plt.annotate('Wrong prediction\\nhigh loss', xy=(0.1, 2.3), fontsize=10,\n",
    "            bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.5))\n",
    "plt.annotate('Correct prediction\\nlow loss', xy=(0.85, 0.2), fontsize=10,\n",
    "            bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.5))\n",
    "\n",
    "# Plot decision boundary\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.axvline(x=0.5, color='k', linestyle='--', linewidth=2, label='Decision boundary')\n",
    "plt.fill_between([0, 0.5], 0, 10, alpha=0.3, color='red', label='Predict 0')\n",
    "plt.fill_between([0.5, 1], 0, 10, alpha=0.3, color='blue', label='Predict 1')\n",
    "plt.xlabel('Prediction (probability)', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.title('Decision Regions', fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key observations:\")\n",
    "print(\"1. Loss is 0 only when prediction perfectly matches target\")\n",
    "print(\"2. Loss â â as prediction â wrong answer\")\n",
    "print(\"3. Asymmetric: worse to be confident and wrong!\")\n",
    "print(\"4. Encourages high confidence correct predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Multi-Class Classification Loss\n",
    "\n",
    "### When to Use Multi-Class Classification?\n",
    "\n",
    "Predicting **one class from many**:\n",
    "- Image classification (cat/dog/bird/...)\n",
    "- Digit recognition (0-9)\n",
    "- Language identification (English/Spanish/French/...)\n",
    "- Medical diagnosis (disease A/B/C/healthy)\n",
    "\n",
    "**Key difference from binary**: Exactly one class is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Cross Entropy Loss (MOST IMPORTANT)\n",
    "\n",
    "**Formula**: $\\text{CE} = -\\sum_{i=1}^{C} y_i \\log(\\hat{y}_i)$\n",
    "\n",
    "Where $y_i$ is 1 for correct class, 0 otherwise.\n",
    "\n",
    "**When to use**:\n",
    "- **Default choice for multi-class classification**\n",
    "- Image classification\n",
    "- Text classification\n",
    "- Any problem with mutually exclusive classes\n",
    "\n",
    "**Pros**:\n",
    "- Combines LogSoftmax + NLLLoss (numerically stable)\n",
    "- Directly optimizes probability distribution\n",
    "- Standard in all deep learning frameworks\n",
    "\n",
    "**Important**: \n",
    "- Expects **logits** (raw outputs), not probabilities\n",
    "- Applies softmax internally\n",
    "- Targets are class indices (0, 1, 2, ..., C-1)\n",
    "\n",
    "**Real-world scenarios**:\n",
    "- MNIST digit classification (10 classes)\n",
    "- ImageNet (1000 classes)\n",
    "- Sentiment classification (positive/neutral/negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Entropy Loss example\n",
    "# Model outputs: raw logits for 3 classes\n",
    "logits = torch.tensor([\n",
    "    [2.0, 1.0, 0.1],  # Sample 1\n",
    "    [0.5, 2.5, 1.0],  # Sample 2\n",
    "    [0.1, 0.2, 3.0]   # Sample 3\n",
    "])\n",
    "\n",
    "# Targets: class indices (NOT one-hot!)\n",
    "targets = torch.tensor([0, 1, 2])  # Correct classes\n",
    "\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "loss = ce_loss(logits, targets)\n",
    "print(f\"Cross Entropy Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Show what's happening\n",
    "print(f\"\\nLogits:\")\n",
    "print(logits)\n",
    "print(f\"\\nTargets (class indices): {targets}\")\n",
    "\n",
    "# Convert logits to probabilities (for interpretation)\n",
    "probs = F.softmax(logits, dim=1)\n",
    "print(f\"\\nProbabilities (after softmax):\")\n",
    "print(probs)\n",
    "\n",
    "# Show prediction vs target\n",
    "predicted_classes = logits.argmax(dim=1)\n",
    "print(f\"\\nPredicted classes: {predicted_classes}\")\n",
    "print(f\"Target classes:    {targets}\")\n",
    "print(f\"Correct: {(predicted_classes == targets).sum().item()}/{len(targets)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understanding CrossEntropyLoss internals\n",
    "# It's actually: LogSoftmax + NLLLoss\n",
    "\n",
    "logits = torch.tensor([\n",
    "    [2.0, 1.0, 0.1],\n",
    "    [0.5, 2.5, 1.0],\n",
    "    [0.1, 0.2, 3.0]\n",
    "])\n",
    "targets = torch.tensor([0, 1, 2])\n",
    "\n",
    "# Method 1: CrossEntropyLoss (recommended)\n",
    "ce_loss = nn.CrossEntropyLoss()(logits, targets)\n",
    "\n",
    "# Method 2: Manual (LogSoftmax + NLLLoss)\n",
    "log_probs = F.log_softmax(logits, dim=1)\n",
    "nll_loss = F.nll_loss(log_probs, targets)\n",
    "\n",
    "print(f\"CrossEntropyLoss:  {ce_loss.item():.6f}\")\n",
    "print(f\"LogSoftmax + NLL:  {nll_loss.item():.6f}\")\n",
    "print(f\"\\nThey're identical!\")\n",
    "\n",
    "# Manual calculation (fully explicit)\n",
    "probs = F.softmax(logits, dim=1)\n",
    "log_probs_manual = torch.log(probs)\n",
    "manual_loss = -log_probs_manual[range(len(targets)), targets].mean()\n",
    "\n",
    "print(f\"Fully manual:      {manual_loss.item():.6f}\")\n",
    "print(f\"\\nConclusion: CrossEntropyLoss is just a convenient wrapper!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Common Mistake: Softmax + CrossEntropy\n",
    "\n",
    "**DO NOT** apply softmax before CrossEntropyLoss!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRONG: Applying softmax before CrossEntropyLoss\n",
    "logits = torch.tensor([[2.0, 1.0, 0.1]])\n",
    "targets = torch.tensor([0])\n",
    "\n",
    "# Correct way\n",
    "loss_correct = nn.CrossEntropyLoss()(logits, targets)\n",
    "print(f\"Correct (logits â CE): {loss_correct.item():.4f}\")\n",
    "\n",
    "# WRONG way (double softmax!)\n",
    "probs = F.softmax(logits, dim=1)  # DON'T DO THIS!\n",
    "loss_wrong = nn.CrossEntropyLoss()(probs, targets)\n",
    "print(f\"Wrong (softmax â CE): {loss_wrong.item():.4f}\")\n",
    "\n",
    "print(f\"\\nThe losses are different! CrossEntropyLoss applies softmax internally.\")\n",
    "print(f\"Always pass raw logits to CrossEntropyLoss!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Multi-Label Classification Loss\n",
    "\n",
    "### Multi-Class vs Multi-Label\n",
    "\n",
    "- **Multi-class**: Exactly ONE class (cat XOR dog XOR bird)\n",
    "- **Multi-label**: MULTIPLE classes possible (cat AND dog AND bird)\n",
    "\n",
    "**When to use multi-label**:\n",
    "- Image tagging (photo contains: person, dog, outdoor)\n",
    "- Text categorization (article is: politics, economy, international)\n",
    "- Medical diagnosis (patient has: diabetes, hypertension, obesity)\n",
    "\n",
    "**Key difference**: Each label is independent binary classification!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 BCE Loss for Multi-Label\n",
    "\n",
    "**When to use**: Multi-label classification (each label independent)\n",
    "\n",
    "**Approach**: Apply BCE to each label independently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-label classification example\n",
    "# Each sample can have multiple labels\n",
    "\n",
    "# Raw outputs (logits) for 4 labels\n",
    "logits = torch.tensor([\n",
    "    [2.0, -1.0, 0.5, 1.5],   # Sample 1: has labels 0, 2, 3\n",
    "    [-0.5, 2.0, -1.0, 0.8],  # Sample 2: has labels 1, 3\n",
    "    [1.0, 1.0, 1.0, -2.0]    # Sample 3: has labels 0, 1, 2\n",
    "])\n",
    "\n",
    "# Targets: binary for each label (can have multiple 1s)\n",
    "targets = torch.tensor([\n",
    "    [1.0, 0.0, 1.0, 1.0],  # Sample 1\n",
    "    [0.0, 1.0, 0.0, 1.0],  # Sample 2\n",
    "    [1.0, 1.0, 1.0, 0.0]   # Sample 3\n",
    "])\n",
    "\n",
    "# Use BCEWithLogitsLoss (treats each label independently)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "loss = loss_fn(logits, targets)\n",
    "print(f\"Multi-label BCE Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Show predictions\n",
    "probs = torch.sigmoid(logits)\n",
    "predictions = (probs > 0.5).float()  # Threshold at 0.5\n",
    "\n",
    "print(f\"\\nProbabilities:\")\n",
    "print(probs)\n",
    "print(f\"\\nPredictions (>0.5):\")\n",
    "print(predictions)\n",
    "print(f\"\\nTargets:\")\n",
    "print(targets)\n",
    "\n",
    "# Compute accuracy per label\n",
    "correct = (predictions == targets).sum(dim=0)\n",
    "total = len(targets)\n",
    "print(f\"\\nAccuracy per label: {correct / total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Decision Guide: Which Loss Function?\n",
    "\n",
    "### Flowchart\n",
    "\n",
    "```\n",
    "What's your task?\n",
    "â\n",
    "ââ Regression (continuous values)\n",
    "â  ââ Standard case â MSELoss\n",
    "â  ââ Has outliers â L1Loss or SmoothL1Loss\n",
    "â  ââ Object detection â SmoothL1Loss\n",
    "â\n",
    "ââ Binary Classification (2 classes)\n",
    "â  ââ ALWAYS use BCEWithLogitsLoss (most stable)\n",
    "â\n",
    "ââ Multi-Class Classification (1 of N classes)\n",
    "â  ââ ALWAYS use CrossEntropyLoss\n",
    "â\n",
    "ââ Multi-Label Classification (multiple classes)\n",
    "   ââ Use BCEWithLogitsLoss\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Reference Table\n",
    "\n",
    "| Task | Loss Function | Model Output | Target Format | Example |\n",
    "|------|---------------|--------------|---------------|----------|\n",
    "| Regression | `MSELoss` | Raw values | Float values | House price prediction |\n",
    "| Regression (outliers) | `L1Loss` or `SmoothL1Loss` | Raw values | Float values | Stock price with outliers |\n",
    "| Binary Classification | `BCEWithLogitsLoss` | Logits (1 value) | 0 or 1 | Spam detection |\n",
    "| Multi-Class | `CrossEntropyLoss` | Logits (C values) | Class index (0 to C-1) | MNIST digits |\n",
    "| Multi-Label | `BCEWithLogitsLoss` | Logits (L values) | Binary per label | Image tagging |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Advanced: Custom Loss Functions\n",
    "\n",
    "Sometimes you need to create your own loss function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Weighted MSE (some samples more important)\n",
    "class WeightedMSELoss(nn.Module):\n",
    "    \"\"\"MSE where each sample has a weight\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, predictions, targets, weights):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            predictions: (N,) tensor\n",
    "            targets: (N,) tensor\n",
    "            weights: (N,) tensor of importance weights\n",
    "        \"\"\"\n",
    "        squared_error = (predictions - targets) ** 2\n",
    "        weighted_error = squared_error * weights\n",
    "        return weighted_error.mean()\n",
    "\n",
    "# Test it\n",
    "predictions = torch.tensor([1.0, 2.0, 3.0])\n",
    "targets = torch.tensor([1.2, 2.5, 2.8])\n",
    "weights = torch.tensor([1.0, 2.0, 0.5])  # Middle sample is more important\n",
    "\n",
    "loss_fn = WeightedMSELoss()\n",
    "loss = loss_fn(predictions, targets, weights)\n",
    "print(f\"Weighted MSE Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Compare with regular MSE\n",
    "regular_mse = nn.MSELoss()(predictions, targets)\n",
    "print(f\"Regular MSE Loss:  {regular_mse.item():.4f}\")\n",
    "print(f\"\\nWeighted version emphasizes important samples!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Combined loss (multiple objectives)\n",
    "class CombinedLoss(nn.Module):\n",
    "    \"\"\"Combine classification and regression losses\"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=0.5):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha  # Balance between tasks\n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "    \n",
    "    def forward(self, class_logits, class_targets, reg_pred, reg_targets):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            class_logits: (N, C) classification logits\n",
    "            class_targets: (N,) class indices\n",
    "            reg_pred: (N,) regression predictions\n",
    "            reg_targets: (N,) regression targets\n",
    "        \"\"\"\n",
    "        ce = self.ce_loss(class_logits, class_targets)\n",
    "        mse = self.mse_loss(reg_pred, reg_targets)\n",
    "        \n",
    "        # Weighted combination\n",
    "        total = self.alpha * ce + (1 - self.alpha) * mse\n",
    "        return total, ce, mse\n",
    "\n",
    "# Test it\n",
    "class_logits = torch.randn(5, 3)  # 5 samples, 3 classes\n",
    "class_targets = torch.randint(0, 3, (5,))\n",
    "reg_pred = torch.randn(5)\n",
    "reg_targets = torch.randn(5)\n",
    "\n",
    "loss_fn = CombinedLoss(alpha=0.7)  # 70% classification, 30% regression\n",
    "total_loss, ce_loss, mse_loss = loss_fn(class_logits, class_targets, reg_pred, reg_targets)\n",
    "\n",
    "print(f\"Classification Loss: {ce_loss.item():.4f}\")\n",
    "print(f\"Regression Loss:     {mse_loss.item():.4f}\")\n",
    "print(f\"Total Loss:          {total_loss.item():.4f}\")\n",
    "print(f\"\\nUse case: Multi-task learning (e.g., detect object + estimate size)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Mini Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Choose the Right Loss\n",
    "\n",
    "For each scenario, choose the appropriate loss function:\n",
    "\n",
    "1. Predicting tomorrow's temperature (Celsius)\n",
    "2. Classifying emails as spam or not spam\n",
    "3. Recognizing handwritten digits (0-9)\n",
    "4. Detecting all objects in an image (person, car, tree, etc.)\n",
    "5. Predicting house prices where some houses are mansions (outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answers here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solutions\n",
    "print(\"1. Temperature prediction:\")\n",
    "print(\"   â MSELoss (standard regression)\")\n",
    "print()\n",
    "\n",
    "print(\"2. Spam classification:\")\n",
    "print(\"   â BCEWithLogitsLoss (binary classification)\")\n",
    "print()\n",
    "\n",
    "print(\"3. Digit recognition (0-9):\")\n",
    "print(\"   â CrossEntropyLoss (multi-class, one correct digit)\")\n",
    "print()\n",
    "\n",
    "print(\"4. Object detection (multiple objects):\")\n",
    "print(\"   â BCEWithLogitsLoss (multi-label, multiple objects present)\")\n",
    "print(\"   â PLUS SmoothL1Loss for bounding box coordinates\")\n",
    "print()\n",
    "\n",
    "print(\"5. House prices with outliers:\")\n",
    "print(\"   â L1Loss or SmoothL1Loss (robust to outliers)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Compute Loss Manually\n",
    "\n",
    "Given predictions and targets, compute CrossEntropyLoss manually (without using PyTorch's function).\n",
    "\n",
    "Predictions (logits): `[[1.0, 2.0, 0.5], [0.5, 0.5, 2.0]]`  \n",
    "Targets: `[1, 2]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "logits = torch.tensor([[1.0, 2.0, 0.5], [0.5, 0.5, 2.0]])\n",
    "targets = torch.tensor([1, 2])\n",
    "\n",
    "# Step 1: Compute softmax probabilities\n",
    "probs = F.softmax(logits, dim=1)\n",
    "print(f\"Probabilities:\\n{probs}\")\n",
    "print()\n",
    "\n",
    "# Step 2: Take log of probabilities\n",
    "log_probs = torch.log(probs)\n",
    "print(f\"Log probabilities:\\n{log_probs}\")\n",
    "print()\n",
    "\n",
    "# Step 3: Select log prob of correct class for each sample\n",
    "# Sample 0: class 1 â log_probs[0, 1]\n",
    "# Sample 1: class 2 â log_probs[1, 2]\n",
    "correct_log_probs = log_probs[range(len(targets)), targets]\n",
    "print(f\"Log probs of correct classes: {correct_log_probs}\")\n",
    "print()\n",
    "\n",
    "# Step 4: Negative mean\n",
    "manual_loss = -correct_log_probs.mean()\n",
    "print(f\"Manual CrossEntropy: {manual_loss.item():.4f}\")\n",
    "\n",
    "# Verify with PyTorch\n",
    "pytorch_loss = nn.CrossEntropyLoss()(logits, targets)\n",
    "print(f\"PyTorch CrossEntropy: {pytorch_loss.item():.4f}\")\n",
    "print(f\"\\nMatch: {torch.isclose(manual_loss, pytorch_loss)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Implement Focal Loss\n",
    "\n",
    "Focal Loss is used for handling class imbalance (e.g., 99% negative, 1% positive).\n",
    "\n",
    "Formula: $FL(p_t) = -(1-p_t)^\\gamma \\log(p_t)$\n",
    "\n",
    "Where $p_t$ is the probability of the correct class, and $\\gamma$ (typically 2) focuses on hard examples.\n",
    "\n",
    "Implement FocalLoss as a custom nn.Module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"Focal Loss for handling class imbalance\"\"\"\n",
    "    \n",
    "    def __init__(self, gamma=2.0, alpha=0.25):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def forward(self, logits, targets):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            logits: (N, C) raw predictions\n",
    "            targets: (N,) class indices\n",
    "        \"\"\"\n",
    "        # Compute probabilities\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        \n",
    "        # Get probability of correct class\n",
    "        correct_probs = probs[range(len(targets)), targets]\n",
    "        \n",
    "        # Compute focal loss\n",
    "        focal_weight = (1 - correct_probs) ** self.gamma\n",
    "        ce_loss = -torch.log(correct_probs)\n",
    "        focal_loss = focal_weight * ce_loss\n",
    "        \n",
    "        return focal_loss.mean()\n",
    "\n",
    "# Test it\n",
    "logits = torch.tensor([\n",
    "    [3.0, 0.1, 0.1],  # Easy example (high confidence)\n",
    "    [0.6, 0.5, 0.4],  # Hard example (low confidence)\n",
    "])\n",
    "targets = torch.tensor([0, 0])\n",
    "\n",
    "focal = FocalLoss(gamma=2.0)\n",
    "focal_loss = focal(logits, targets)\n",
    "\n",
    "ce = nn.CrossEntropyLoss()(logits, targets)\n",
    "\n",
    "print(f\"Focal Loss: {focal_loss.item():.4f}\")\n",
    "print(f\"CE Loss:    {ce.item():.4f}\")\n",
    "print(f\"\\nFocal loss focuses more on hard examples!\")\n",
    "\n",
    "# Show individual losses\n",
    "probs = F.softmax(logits, dim=1)[:, 0]\n",
    "print(f\"\\nSample 0: p={probs[0]:.4f} (easy)\")\n",
    "print(f\"Sample 1: p={probs[1]:.4f} (hard)\")\n",
    "print(f\"Focal loss weights hard examples more heavily!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Comprehensive Exercise: Loss Function Comparison\n",
    "\n",
    "Generate synthetic regression data with outliers and compare:\n",
    "1. MSELoss\n",
    "2. L1Loss  \n",
    "3. SmoothL1Loss\n",
    "\n",
    "Fit a simple linear model to the data and visualize the results.\n",
    "\n",
    "**Steps**:\n",
    "1. Generate data: `y = 2x + 1 + noise`, with 10% outliers\n",
    "2. Train 3 models (one per loss function)\n",
    "3. Plot data and fitted lines\n",
    "4. Compare which loss handles outliers best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# 1. Generate data with outliers\n",
    "n_samples = 100\n",
    "x = torch.randn(n_samples, 1)\n",
    "y = 2 * x + 1 + torch.randn(n_samples, 1) * 0.5  # y = 2x + 1 + noise\n",
    "\n",
    "# Add outliers (10% of data)\n",
    "n_outliers = n_samples // 10\n",
    "outlier_indices = torch.randperm(n_samples)[:n_outliers]\n",
    "y[outlier_indices] += torch.randn(n_outliers, 1) * 5  # Large noise\n",
    "\n",
    "print(f\"Generated {n_samples} samples with {n_outliers} outliers\")\n",
    "\n",
    "# 2. Train models with different losses\n",
    "def train_model(loss_fn, n_iter=1000, lr=0.01):\n",
    "    \"\"\"Train simple linear model\"\"\"\n",
    "    model = nn.Linear(1, 1)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    \n",
    "    for _ in range(n_iter):\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(x)\n",
    "        loss = loss_fn(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Train with different losses\n",
    "print(\"\\nTraining models...\")\n",
    "model_mse = train_model(nn.MSELoss())\n",
    "model_l1 = train_model(nn.L1Loss())\n",
    "model_smooth = train_model(nn.SmoothL1Loss())\n",
    "\n",
    "# Extract parameters\n",
    "w_mse, b_mse = model_mse.weight.item(), model_mse.bias.item()\n",
    "w_l1, b_l1 = model_l1.weight.item(), model_l1.bias.item()\n",
    "w_smooth, b_smooth = model_smooth.weight.item(), model_smooth.bias.item()\n",
    "\n",
    "print(f\"\\nTrue parameters: w=2.0, b=1.0\")\n",
    "print(f\"MSE:      w={w_mse:.3f}, b={b_mse:.3f}\")\n",
    "print(f\"L1:       w={w_l1:.3f}, b={b_l1:.3f}\")\n",
    "print(f\"Smooth L1: w={w_smooth:.3f}, b={b_smooth:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Visualize results\n",
    "x_line = torch.linspace(x.min(), x.max(), 100).reshape(-1, 1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_mse = model_mse(x_line)\n",
    "    y_l1 = model_l1(x_line)\n",
    "    y_smooth = model_smooth(x_line)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot data and fits\n",
    "plt.subplot(1, 2, 1)\n",
    "# Separate normal and outlier points\n",
    "mask = torch.ones(n_samples, dtype=torch.bool)\n",
    "mask[outlier_indices] = False\n",
    "\n",
    "plt.scatter(x[mask].numpy(), y[mask].numpy(), alpha=0.6, label='Normal data', s=30)\n",
    "plt.scatter(x[~mask].numpy(), y[~mask].numpy(), color='red', alpha=0.6, \n",
    "           label='Outliers', s=50, marker='x')\n",
    "\n",
    "# Plot fitted lines\n",
    "plt.plot(x_line.numpy(), y_mse.numpy(), 'b-', linewidth=2, label=f'MSE: y={w_mse:.2f}x+{b_mse:.2f}')\n",
    "plt.plot(x_line.numpy(), y_l1.numpy(), 'g-', linewidth=2, label=f'L1: y={w_l1:.2f}x+{b_l1:.2f}')\n",
    "plt.plot(x_line.numpy(), y_smooth.numpy(), 'm-', linewidth=2, label=f'Smooth L1: y={w_smooth:.2f}x+{b_smooth:.2f}')\n",
    "\n",
    "# True line\n",
    "y_true = 2 * x_line + 1\n",
    "plt.plot(x_line.numpy(), y_true.numpy(), 'k--', linewidth=2, alpha=0.5, label='True: y=2x+1')\n",
    "\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.ylabel('y', fontsize=12)\n",
    "plt.title('Regression with Outliers', fontsize=14)\n",
    "plt.legend(fontsize=9)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot loss comparison\n",
    "plt.subplot(1, 2, 2)\n",
    "with torch.no_grad():\n",
    "    pred_mse = model_mse(x)\n",
    "    pred_l1 = model_l1(x)\n",
    "    pred_smooth = model_smooth(x)\n",
    "    \n",
    "    loss_mse = nn.MSELoss()(pred_mse, y).item()\n",
    "    loss_l1 = nn.L1Loss()(pred_l1, y).item()\n",
    "    loss_smooth = nn.SmoothL1Loss()(pred_smooth, y).item()\n",
    "\n",
    "losses = [loss_mse, loss_l1, loss_smooth]\n",
    "labels = ['MSE', 'L1', 'Smooth L1']\n",
    "colors = ['blue', 'green', 'magenta']\n",
    "\n",
    "plt.bar(labels, losses, color=colors, alpha=0.7)\n",
    "plt.ylabel('Final Loss', fontsize=12)\n",
    "plt.title('Loss Comparison', fontsize=14)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nConclusions:\")\n",
    "print(\"1. MSE is pulled by outliers (fitted line deviates from true line)\")\n",
    "print(\"2. L1 and Smooth L1 are more robust to outliers\")\n",
    "print(\"3. Smooth L1 often provides best balance\")\n",
    "print(\"4. Choose loss based on your data characteristics!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Loss function = learning objective**: Tells the model what \"good\" means\n",
    "2. **Choose based on task type**:\n",
    "   - Regression â MSE, L1, or Smooth L1\n",
    "   - Binary classification â BCEWithLogitsLoss\n",
    "   - Multi-class â CrossEntropyLoss\n",
    "   - Multi-label â BCEWithLogitsLoss\n",
    "3. **Model output matters**:\n",
    "   - CrossEntropyLoss expects logits (raw outputs)\n",
    "   - BCELoss expects probabilities (after sigmoid)\n",
    "   - BCEWithLogitsLoss expects logits (more stable)\n",
    "4. **Outliers matter**: Use L1 or Smooth L1 when data has outliers\n",
    "5. **Custom losses**: Easy to implement for special requirements\n",
    "6. **Combined losses**: Can optimize multiple objectives simultaneously\n",
    "\n",
    "### Common Mistakes to Avoid\n",
    "\n",
    "1. **Applying softmax before CrossEntropyLoss** (it does it internally!)\n",
    "2. **Using BCELoss with logits** (use BCEWithLogitsLoss instead)\n",
    "3. **Wrong target format** (class indices vs one-hot vs binary)\n",
    "4. **Ignoring data characteristics** (outliers, class imbalance)\n",
    "5. **Mixing regression and classification losses** incorrectly\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "You now understand how to measure model performance! Next, we'll put everything together and build complete **training loops with optimization**.\n",
    "\n",
    "Continue to: [Topic 5: Training Loop & Optimization](05_training_optimization.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "- [PyTorch Loss Functions Documentation](https://pytorch.org/docs/stable/nn.html#loss-functions)\n",
    "- [Cross Entropy Loss Explained](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)\n",
    "- [Focal Loss Paper](https://arxiv.org/abs/1708.02002)\n",
    "- [Loss Functions for Classification](https://machinelearningmastery.com/loss-functions-for-classification/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
