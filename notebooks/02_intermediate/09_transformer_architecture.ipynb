{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic 9: The Transformer Architecture\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Understand the **complete transformer architecture** from 30,000 feet to implementation\n",
    "- Learn **why** each component exists and what problem it solves\n",
    "- Build an encoder-decoder transformer **from scratch**\n",
    "- Understand encoder-only (BERT), decoder-only (GPT), and encoder-decoder (T5) variants\n",
    "- Visualize information flow through the transformer\n",
    "- Connect transformers to modern LLMs and understand their evolution\n",
    "- Appreciate why transformers revolutionized AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Big Picture: Why Transformers Changed Everything\n",
    "\n",
    "### Before Transformers: The RNN/LSTM Era (Pre-2017)\n",
    "\n",
    "**The dominant architecture**:\n",
    "```\n",
    "Input sequence → RNN/LSTM → Hidden states → Output\n",
    "```\n",
    "\n",
    "**Critical limitations**:\n",
    "1. **Sequential processing**: Must process tokens one-by-one (slow, can't parallelize)\n",
    "2. **Vanishing gradients**: Hard to learn long-range dependencies\n",
    "3. **Memory bottleneck**: All information compressed into fixed hidden state\n",
    "4. **Training time**: Takes weeks to train large models\n",
    "\n",
    "**Example failure**:\n",
    "```\n",
    "Input: \"The cat, which was very fluffy and loved to play with yarn, sat on the mat.\"\n",
    "Problem: By the time LSTM reaches \"sat\", it may have forgotten \"cat\" (the subject)\n",
    "```\n",
    "\n",
    "### The Transformer Revolution (2017)\n",
    "\n",
    "**\"Attention is All You Need\"** (Vaswani et al., 2017)\n",
    "\n",
    "**Key insight**: Replace recurrence with **self-attention**\n",
    "\n",
    "**Revolutionary properties**:\n",
    "1. **Parallel processing**: All tokens processed simultaneously\n",
    "2. **Direct connections**: Any token can attend to any other token\n",
    "3. **Scalability**: Can train 100B+ parameter models\n",
    "4. **Transfer learning**: Pre-train once, fine-tune for many tasks\n",
    "\n",
    "**Impact timeline**:\n",
    "- **2017**: Original Transformer (machine translation)\n",
    "- **2018**: BERT (masked language modeling) + GPT (autoregressive LM)\n",
    "- **2019**: GPT-2, T5, RoBERTa\n",
    "- **2020**: GPT-3 (175B params), Vision Transformers\n",
    "- **2021**: Multimodal transformers (CLIP, DALL-E)\n",
    "- **2022**: ChatGPT, LLaMA, Stable Diffusion\n",
    "- **2023-2025**: GPT-4, Claude, LLaMA 2/3, Gemini, 100K+ context models\n",
    "\n",
    "**Why transformers cannot be skipped**: They are the foundation of modern AI. Every major breakthrough since 2017—GPT, BERT, CLIP, AlphaFold, Stable Diffusion, ChatGPT, Claude—is built on transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Transformer Architecture: Overview\n",
    "\n",
    "### The Complete Picture\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                    TRANSFORMER                               │\n",
    "│                                                              │\n",
    "│  ┌──────────────────────┐      ┌──────────────────────┐   │\n",
    "│  │      ENCODER         │      │      DECODER         │   │\n",
    "│  │                      │      │                      │   │\n",
    "│  │  Input Embedding     │      │  Output Embedding    │   │\n",
    "│  │         +            │      │         +            │   │\n",
    "│  │  Positional Encoding │      │  Positional Encoding │   │\n",
    "│  │         ↓            │      │         ↓            │   │\n",
    "│  │  ┌──────────────┐   │      │  ┌──────────────┐   │   │\n",
    "│  │  │ Encoder Layer│ ×N│      │  │ Decoder Layer│ ×N│   │\n",
    "│  │  │              │   │──────→  │              │   │   │\n",
    "│  │  │ • Self-Attn  │   │ Context │ • Self-Attn  │   │   │\n",
    "│  │  │ • Feed-Fwd   │   │      │  │ • Cross-Attn │   │   │\n",
    "│  │  └──────────────┘   │      │  │ • Feed-Fwd   │   │   │\n",
    "│  │         ↓            │      │  └──────────────┘   │   │\n",
    "│  │  Encoder Output      │      │         ↓            │   │\n",
    "│  └──────────────────────┘      │  Linear + Softmax    │   │\n",
    "│                                 │         ↓            │   │\n",
    "│                                 │  Output Probabilities│   │\n",
    "│                                 └──────────────────────┘   │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### Key Components (We'll build each from scratch)\n",
    "\n",
    "**1. Input/Output Embeddings**:\n",
    "- Convert tokens to vectors\n",
    "- Why? Neural networks need numeric inputs\n",
    "\n",
    "**2. Positional Encoding**:\n",
    "- Inject position information\n",
    "- Why? Attention is permutation-invariant\n",
    "\n",
    "**3. Encoder Layer**:\n",
    "- **Self-attention**: Understand relationships within input\n",
    "- **Feed-forward**: Process each position independently\n",
    "- Why? Build rich representations of input\n",
    "\n",
    "**4. Decoder Layer**:\n",
    "- **Masked self-attention**: Generate output autoregressively\n",
    "- **Cross-attention**: Attend to encoder output\n",
    "- **Feed-forward**: Process each position\n",
    "- Why? Generate output while looking at input\n",
    "\n",
    "**5. Residual Connections & Layer Norm**:\n",
    "- Skip connections around each sub-layer\n",
    "- Why? Enable deep networks, stabilize training\n",
    "\n",
    "### Three Architectural Variants\n",
    "\n",
    "**Encoder-only (BERT)**:\n",
    "- Use only encoder stack\n",
    "- Bidirectional context\n",
    "- Best for: Classification, NER, sentence embeddings\n",
    "\n",
    "**Decoder-only (GPT)**:\n",
    "- Use only decoder stack (without cross-attention)\n",
    "- Causal (left-to-right) generation\n",
    "- Best for: Text generation, language modeling\n",
    "\n",
    "**Encoder-decoder (T5, BART)**:\n",
    "- Full transformer with both stacks\n",
    "- Best for: Translation, summarization, seq2seq tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Block 1: Multi-Head Attention (Recap)\n",
    "\n",
    "We've already covered attention in detail. Here's a compact implementation for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head attention mechanism.\n",
    "    \n",
    "    Key insight: Multiple attention patterns in parallel.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.W_Q = nn.Linear(d_model, d_model)\n",
    "        self.W_K = nn.Linear(d_model, d_model)\n",
    "        self.W_V = nn.Linear(d_model, d_model)\n",
    "        self.W_O = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "        return x.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "    \n",
    "    def combine_heads(self, x):\n",
    "        batch_size, num_heads, seq_len, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            query: (batch, seq_len_q, d_model)\n",
    "            key: (batch, seq_len_k, d_model)\n",
    "            value: (batch, seq_len_v, d_model)\n",
    "            mask: Optional attention mask\n",
    "        \n",
    "        Returns:\n",
    "            output: (batch, seq_len_q, d_model)\n",
    "            attention_weights: (batch, num_heads, seq_len_q, seq_len_k)\n",
    "        \"\"\"\n",
    "        Q = self.split_heads(self.W_Q(query))\n",
    "        K = self.split_heads(self.W_K(key))\n",
    "        V = self.split_heads(self.W_V(value))\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        \n",
    "        attn_output = torch.matmul(attention_weights, V)\n",
    "        attn_output = self.combine_heads(attn_output)\n",
    "        \n",
    "        output = self.W_O(attn_output)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "print(\"Multi-Head Attention: ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Block 2: Position-wise Feed-Forward Network\n",
    "\n",
    "### What is it?\n",
    "\n",
    "A simple 2-layer fully-connected network applied to **each position independently**:\n",
    "\n",
    "$$FFN(x) = \\text{ReLU}(xW_1 + b_1)W_2 + b_2$$\n",
    "\n",
    "Or equivalently: Two `Conv1d` with kernel_size=1\n",
    "\n",
    "### Why do we need it?\n",
    "\n",
    "**Attention aggregates information, but doesn't transform it deeply!**\n",
    "\n",
    "**Problem**: Attention is a weighted sum (linear operation)\n",
    "- It mixes information from different positions\n",
    "- But doesn't apply non-linear transformations\n",
    "\n",
    "**Solution**: Feed-forward network adds:\n",
    "1. **Non-linearity**: ReLU/GELU introduces non-linear transformations\n",
    "2. **Depth**: Two layers with expansion (typically 4x) add capacity\n",
    "3. **Position-wise processing**: Each position transformed independently\n",
    "\n",
    "**Why independent processing?**\n",
    "- Attention mixed information across positions\n",
    "- FFN processes each position's representation\n",
    "- Alternating attention + FFN = mixing + processing\n",
    "\n",
    "**Standard configuration**:\n",
    "- Hidden dimension = 4 × d_model (expansion factor)\n",
    "- Activation: ReLU (original) or GELU (modern)\n",
    "- Why 4x? Empirically found to work well (more capacity without too many params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Position-wise feed-forward network.\n",
    "    \n",
    "    Applied to each position independently and identically.\n",
    "    \n",
    "    FFN(x) = max(0, xW1 + b1)W2 + b2\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: Model dimension (e.g., 512)\n",
    "            d_ff: Hidden dimension (typically 4 × d_model = 2048)\n",
    "            dropout: Dropout probability\n",
    "        \"\"\"\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        \n",
    "        # Two linear layers with expansion\n",
    "        # Why 4x expansion? Adds capacity while keeping model manageable\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Modern transformers often use GELU instead of ReLU\n",
    "        # GELU is smoother and works better in practice\n",
    "        self.activation = nn.GELU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch, seq_len, d_model)\n",
    "        \n",
    "        Returns:\n",
    "            output: (batch, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        # x: (batch, seq_len, d_model)\n",
    "        # -> (batch, seq_len, d_ff) [expand]\n",
    "        # -> (batch, seq_len, d_model) [project back]\n",
    "        \n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Test FFN\n",
    "d_model = 512\n",
    "d_ff = 2048  # 4x expansion\n",
    "\n",
    "ffn = PositionWiseFeedForward(d_model, d_ff)\n",
    "x = torch.randn(2, 10, d_model)\n",
    "output = ffn(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"\\nFFN parameters: {sum(p.numel() for p in ffn.parameters()):,}\")\n",
    "print(f\"  W1: {d_model} × {d_ff} = {d_model * d_ff:,}\")\n",
    "print(f\"  W2: {d_ff} × {d_model} = {d_ff * d_model:,}\")\n",
    "print(f\"\\nNote: FFN has most of the transformer's parameters!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Block 3: Residual Connections & Layer Normalization\n",
    "\n",
    "### Why Residual Connections?\n",
    "\n",
    "**Problem**: Deep networks suffer from degradation\n",
    "- Gradients vanish or explode\n",
    "- Deeper doesn't always mean better\n",
    "\n",
    "**Solution**: Skip connections (ResNet-style)\n",
    "```python\n",
    "output = x + SubLayer(x)\n",
    "```\n",
    "\n",
    "**Why this works**:\n",
    "1. **Gradient flow**: Gradients can flow directly through skip connection\n",
    "2. **Identity mapping**: Network can learn to do nothing if needed\n",
    "3. **Easier optimization**: Each layer only learns residual (delta)\n",
    "\n",
    "### Why Layer Normalization?\n",
    "\n",
    "**Batch Norm vs Layer Norm**:\n",
    "- **Batch Norm**: Normalize across batch dimension (doesn't work well for sequences)\n",
    "- **Layer Norm**: Normalize across feature dimension (works for variable-length sequences)\n",
    "\n",
    "**Layer Norm formula**:\n",
    "$$\\text{LayerNorm}(x) = \\gamma \\frac{x - \\mu}{\\sigma} + \\beta$$\n",
    "\n",
    "Where $\\mu$ and $\\sigma$ are computed **per sample, per layer**\n",
    "\n",
    "**Why it's crucial**:\n",
    "1. **Stable training**: Prevents activation explosion/vanishing\n",
    "2. **Faster convergence**: Normalizes gradient flow\n",
    "3. **Higher learning rates**: Can train with larger learning rates\n",
    "\n",
    "### Pre-LN vs Post-LN\n",
    "\n",
    "**Post-LN (Original Transformer)**:\n",
    "```python\n",
    "output = LayerNorm(x + SubLayer(x))\n",
    "```\n",
    "\n",
    "**Pre-LN (Modern, more stable)**:\n",
    "```python\n",
    "output = x + SubLayer(LayerNorm(x))\n",
    "```\n",
    "\n",
    "**Why Pre-LN is better**:\n",
    "- More stable training for very deep networks\n",
    "- Gradients flow better\n",
    "- Used in GPT-2, GPT-3, and most modern models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    Residual connection followed by layer norm.\n",
    "    \n",
    "    Implements: LayerNorm(x + Sublayer(x))\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, dropout=0.1):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, sublayer):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input (batch, seq_len, d_model)\n",
    "            sublayer: Function (attention or FFN)\n",
    "        \n",
    "        Returns:\n",
    "            output: (batch, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        # Apply sublayer, dropout, add residual, then normalize\n",
    "        return self.norm(x + self.dropout(sublayer(x)))\n",
    "\n",
    "# Demonstrate residual connection benefit\n",
    "x = torch.randn(2, 10, 512)\n",
    "sublayer = lambda x: torch.zeros_like(x)  # Sublayer that outputs zeros\n",
    "\n",
    "sublayer_conn = SublayerConnection(d_model=512)\n",
    "output = sublayer_conn(x, sublayer)\n",
    "\n",
    "print(f\"Input mean: {x.mean().item():.4f}\")\n",
    "print(f\"Sublayer output mean: {sublayer(x).mean().item():.4f}\")\n",
    "print(f\"With residual connection mean: {output.mean().item():.4f}\")\n",
    "print(\"\\nEven if sublayer outputs zeros, information flows through residual!\")\n",
    "print(\"This is why deep transformers train successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Encoder Layer: Putting It Together\n",
    "\n",
    "### Structure\n",
    "\n",
    "Each encoder layer has two sub-layers:\n",
    "1. **Multi-head self-attention**: Understand relationships in input\n",
    "2. **Position-wise feed-forward**: Process each position\n",
    "\n",
    "Each sub-layer has:\n",
    "- Residual connection\n",
    "- Layer normalization\n",
    "- Dropout\n",
    "\n",
    "```\n",
    "EncoderLayer:\n",
    "  x -> [Self-Attention] -> Add & Norm -> \n",
    "       [Feed-Forward]   -> Add & Norm -> output\n",
    "```\n",
    "\n",
    "### Why This Design?\n",
    "\n",
    "**Two-stage processing**:\n",
    "1. **Attention**: Mix information across positions (global context)\n",
    "2. **FFN**: Transform each position independently (local processing)\n",
    "\n",
    "**Alternating global + local** = powerful hierarchical representations\n",
    "\n",
    "**Why multiple layers?**\n",
    "- Layer 1: Low-level patterns (syntax, basic relationships)\n",
    "- Layer 2-6: Mid-level patterns (phrases, dependencies)\n",
    "- Layer 7-12: High-level patterns (semantics, discourse)\n",
    "\n",
    "Standard: 6-12 layers (BERT-base: 12, GPT-3: 96!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Single encoder layer.\n",
    "    \n",
    "    Architecture:\n",
    "      1. Multi-head self-attention\n",
    "      2. Add & Norm\n",
    "      3. Position-wise FFN\n",
    "      4. Add & Norm\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        \n",
    "        # Sub-layer 1: Self-attention\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        \n",
    "        # Sub-layer 2: Feed-forward\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff, dropout)\n",
    "        \n",
    "        # Sublayer connections (residual + norm)\n",
    "        self.sublayer1 = SublayerConnection(d_model, dropout)\n",
    "        self.sublayer2 = SublayerConnection(d_model, dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input (batch, seq_len, d_model)\n",
    "            mask: Optional attention mask\n",
    "        \n",
    "        Returns:\n",
    "            output: (batch, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        # Self-attention sub-layer\n",
    "        # Why self-attention? Input attends to itself\n",
    "        x = self.sublayer1(x, lambda x: self.self_attn(x, x, x, mask)[0])\n",
    "        \n",
    "        # Feed-forward sub-layer\n",
    "        # Why FFN? Add depth and non-linearity\n",
    "        x = self.sublayer2(x, self.feed_forward)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Test encoder layer\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "d_ff = 2048\n",
    "\n",
    "encoder_layer = EncoderLayer(d_model, num_heads, d_ff)\n",
    "x = torch.randn(2, 10, d_model)\n",
    "output = encoder_layer(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"\\nEncoder layer parameters: {sum(p.numel() for p in encoder_layer.parameters()):,}\")\n",
    "print(\"\\nThis single layer can be stacked 6-12 times to form the encoder!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Decoder Layer: More Complex\n",
    "\n",
    "### Structure\n",
    "\n",
    "Each decoder layer has **three** sub-layers:\n",
    "1. **Masked self-attention**: Generate output autoregressively\n",
    "2. **Cross-attention**: Attend to encoder output\n",
    "3. **Position-wise feed-forward**: Process each position\n",
    "\n",
    "```\n",
    "DecoderLayer:\n",
    "  x -> [Masked Self-Attention] -> Add & Norm -> \n",
    "       [Cross-Attention to Encoder] -> Add & Norm -> \n",
    "       [Feed-Forward] -> Add & Norm -> output\n",
    "```\n",
    "\n",
    "### Why Three Sub-layers?\n",
    "\n",
    "**1. Masked Self-Attention**:\n",
    "- **Purpose**: Understand relationships in output generated so far\n",
    "- **Why masked?**: Can't look at future tokens (we're generating them!)\n",
    "- **Causal masking**: Position i can only attend to positions ≤ i\n",
    "\n",
    "**2. Cross-Attention**:\n",
    "- **Purpose**: Look at encoder output (input sequence)\n",
    "- **Query**: From decoder (what do I need?)\n",
    "- **Key, Value**: From encoder (what information is available?)\n",
    "- **Why crucial**: This is how decoder \"reads\" the input!\n",
    "\n",
    "**3. Feed-Forward**:\n",
    "- Same as encoder\n",
    "- Process each position independently\n",
    "\n",
    "### Information Flow\n",
    "\n",
    "```\n",
    "Decoder position i:\n",
    "  1. Look at previous decoder outputs (positions 0...i)\n",
    "  2. Look at ALL encoder outputs (entire input)\n",
    "  3. Process combined information\n",
    "  -> Predict next token\n",
    "```\n",
    "\n",
    "This is the magic of sequence-to-sequence learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Single decoder layer.\n",
    "    \n",
    "    Architecture:\n",
    "      1. Masked multi-head self-attention\n",
    "      2. Add & Norm\n",
    "      3. Multi-head cross-attention to encoder\n",
    "      4. Add & Norm\n",
    "      5. Position-wise FFN\n",
    "      6. Add & Norm\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        \n",
    "        # Sub-layer 1: Masked self-attention\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        \n",
    "        # Sub-layer 2: Cross-attention to encoder\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        \n",
    "        # Sub-layer 3: Feed-forward\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff, dropout)\n",
    "        \n",
    "        # Sublayer connections\n",
    "        self.sublayer1 = SublayerConnection(d_model, dropout)\n",
    "        self.sublayer2 = SublayerConnection(d_model, dropout)\n",
    "        self.sublayer3 = SublayerConnection(d_model, dropout)\n",
    "    \n",
    "    def forward(self, x, encoder_output, src_mask=None, tgt_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Decoder input (batch, tgt_len, d_model)\n",
    "            encoder_output: Encoder output (batch, src_len, d_model)\n",
    "            src_mask: Source attention mask\n",
    "            tgt_mask: Target (causal) mask\n",
    "        \n",
    "        Returns:\n",
    "            output: (batch, tgt_len, d_model)\n",
    "        \"\"\"\n",
    "        # Sub-layer 1: Masked self-attention\n",
    "        # Why masked? Can't attend to future tokens during generation\n",
    "        x = self.sublayer1(x, lambda x: self.self_attn(x, x, x, tgt_mask)[0])\n",
    "        \n",
    "        # Sub-layer 2: Cross-attention to encoder\n",
    "        # Query from decoder, Key/Value from encoder\n",
    "        # This is where decoder \"reads\" the input!\n",
    "        x = self.sublayer2(x, lambda x: self.cross_attn(x, encoder_output, encoder_output, src_mask)[0])\n",
    "        \n",
    "        # Sub-layer 3: Feed-forward\n",
    "        x = self.sublayer3(x, self.feed_forward)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Test decoder layer\n",
    "decoder_layer = DecoderLayer(d_model=512, num_heads=8, d_ff=2048)\n",
    "\n",
    "# Decoder input (target sequence so far)\n",
    "tgt = torch.randn(2, 8, 512)\n",
    "\n",
    "# Encoder output (source sequence)\n",
    "encoder_out = torch.randn(2, 10, 512)\n",
    "\n",
    "# Create causal mask for target\n",
    "tgt_len = tgt.size(1)\n",
    "tgt_mask = torch.tril(torch.ones(1, 1, tgt_len, tgt_len))\n",
    "\n",
    "output = decoder_layer(tgt, encoder_out, tgt_mask=tgt_mask)\n",
    "\n",
    "print(f\"Decoder input shape: {tgt.shape}\")\n",
    "print(f\"Encoder output shape: {encoder_out.shape}\")\n",
    "print(f\"Decoder output shape: {output.shape}\")\n",
    "print(f\"\\nDecoder layer parameters: {sum(p.numel() for p in decoder_layer.parameters()):,}\")\n",
    "print(\"\\nNote: Decoder is more complex than encoder (3 sub-layers vs 2)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Encoder-Decoder Transformer\n",
    "\n",
    "Now let's put everything together into the full transformer architecture!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Sinusoidal positional encoding.\"\"\"\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete Transformer model (Encoder-Decoder).\n",
    "    \n",
    "    This is the full architecture from 'Attention is All You Need'.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 src_vocab_size,\n",
    "                 tgt_vocab_size,\n",
    "                 d_model=512,\n",
    "                 num_heads=8,\n",
    "                 num_encoder_layers=6,\n",
    "                 num_decoder_layers=6,\n",
    "                 d_ff=2048,\n",
    "                 dropout=0.1,\n",
    "                 max_len=5000):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src_vocab_size: Source vocabulary size\n",
    "            tgt_vocab_size: Target vocabulary size\n",
    "            d_model: Model dimension (512 in original paper)\n",
    "            num_heads: Number of attention heads (8 in original)\n",
    "            num_encoder_layers: Encoder stack depth (6 in original)\n",
    "            num_decoder_layers: Decoder stack depth (6 in original)\n",
    "            d_ff: Feed-forward hidden dimension (2048 in original)\n",
    "            dropout: Dropout probability\n",
    "            max_len: Maximum sequence length\n",
    "        \"\"\"\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Embeddings\n",
    "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
    "        \n",
    "        # Encoder stack\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            EncoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_encoder_layers)\n",
    "        ])\n",
    "        \n",
    "        # Decoder stack\n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            DecoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_decoder_layers)\n",
    "        ])\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_projection = nn.Linear(d_model, tgt_vocab_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self._init_parameters()\n",
    "    \n",
    "    def _init_parameters(self):\n",
    "        \"\"\"Initialize parameters with Xavier uniform.\"\"\"\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    def encode(self, src, src_mask=None):\n",
    "        \"\"\"\n",
    "        Encode source sequence.\n",
    "        \n",
    "        Args:\n",
    "            src: Source token indices (batch, src_len)\n",
    "            src_mask: Source attention mask\n",
    "        \n",
    "        Returns:\n",
    "            encoder_output: (batch, src_len, d_model)\n",
    "        \"\"\"\n",
    "        # Embed and add positional encoding\n",
    "        # Why sqrt(d_model)? Scales embedding to same magnitude as PE\n",
    "        x = self.src_embedding(src) * math.sqrt(self.d_model)\n",
    "        x = self.pos_encoding(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Pass through encoder layers\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x, src_mask)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def decode(self, tgt, encoder_output, src_mask=None, tgt_mask=None):\n",
    "        \"\"\"\n",
    "        Decode target sequence.\n",
    "        \n",
    "        Args:\n",
    "            tgt: Target token indices (batch, tgt_len)\n",
    "            encoder_output: Encoder output (batch, src_len, d_model)\n",
    "            src_mask: Source attention mask\n",
    "            tgt_mask: Target (causal) mask\n",
    "        \n",
    "        Returns:\n",
    "            decoder_output: (batch, tgt_len, d_model)\n",
    "        \"\"\"\n",
    "        # Embed and add positional encoding\n",
    "        x = self.tgt_embedding(tgt) * math.sqrt(self.d_model)\n",
    "        x = self.pos_encoding(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Pass through decoder layers\n",
    "        for layer in self.decoder_layers:\n",
    "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
    "        \"\"\"\n",
    "        Full forward pass.\n",
    "        \n",
    "        Args:\n",
    "            src: Source sequence (batch, src_len)\n",
    "            tgt: Target sequence (batch, tgt_len)\n",
    "            src_mask: Source attention mask\n",
    "            tgt_mask: Target (causal) mask\n",
    "        \n",
    "        Returns:\n",
    "            logits: (batch, tgt_len, tgt_vocab_size)\n",
    "        \"\"\"\n",
    "        # Encode\n",
    "        encoder_output = self.encode(src, src_mask)\n",
    "        \n",
    "        # Decode\n",
    "        decoder_output = self.decode(tgt, encoder_output, src_mask, tgt_mask)\n",
    "        \n",
    "        # Project to vocabulary\n",
    "        logits = self.output_projection(decoder_output)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "print(\"Complete Transformer: ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Complete Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create transformer\n",
    "src_vocab_size = 10000\n",
    "tgt_vocab_size = 10000\n",
    "\n",
    "model = Transformer(\n",
    "    src_vocab_size=src_vocab_size,\n",
    "    tgt_vocab_size=tgt_vocab_size,\n",
    "    d_model=512,\n",
    "    num_heads=8,\n",
    "    num_encoder_layers=6,\n",
    "    num_decoder_layers=6,\n",
    "    d_ff=2048,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "# Create dummy input\n",
    "batch_size = 2\n",
    "src_len = 10\n",
    "tgt_len = 8\n",
    "\n",
    "src = torch.randint(0, src_vocab_size, (batch_size, src_len))\n",
    "tgt = torch.randint(0, tgt_vocab_size, (batch_size, tgt_len))\n",
    "\n",
    "# Create causal mask for target\n",
    "tgt_mask = torch.tril(torch.ones(1, 1, tgt_len, tgt_len))\n",
    "\n",
    "# Forward pass\n",
    "logits = model(src, tgt, tgt_mask=tgt_mask)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"COMPLETE TRANSFORMER TEST\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nSource sequence shape: {src.shape}\")\n",
    "print(f\"Target sequence shape: {tgt.shape}\")\n",
    "print(f\"Output logits shape: {logits.shape}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "\n",
    "# Break down by component\n",
    "embed_params = sum(p.numel() for p in model.src_embedding.parameters()) + \\\n",
    "               sum(p.numel() for p in model.tgt_embedding.parameters())\n",
    "encoder_params = sum(p.numel() for p in model.encoder_layers.parameters())\n",
    "decoder_params = sum(p.numel() for p in model.decoder_layers.parameters())\n",
    "output_params = sum(p.numel() for p in model.output_projection.parameters())\n",
    "\n",
    "print(f\"\\nParameter breakdown:\")\n",
    "print(f\"  Embeddings: {embed_params:,} ({100*embed_params/total_params:.1f}%)\")\n",
    "print(f\"  Encoder: {encoder_params:,} ({100*encoder_params/total_params:.1f}%)\")\n",
    "print(f\"  Decoder: {decoder_params:,} ({100*decoder_params/total_params:.1f}%)\")\n",
    "print(f\"  Output projection: {output_params:,} ({100*output_params/total_params:.1f}%)\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"ARCHITECTURE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Model dimension (d_model): 512\")\n",
    "print(f\"Number of attention heads: 8\")\n",
    "print(f\"Encoder layers: 6\")\n",
    "print(f\"Decoder layers: 6\")\n",
    "print(f\"Feed-forward dimension: 2048\")\n",
    "print(f\"\\nThis is the exact architecture from 'Attention is All You Need'!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architectural Variants\n",
    "\n",
    "### 1. Encoder-Only (BERT)\n",
    "\n",
    "**Use only the encoder stack**\n",
    "\n",
    "```python\n",
    "# BERT architecture\n",
    "x = embeddings + positional_encoding\n",
    "for layer in encoder_layers:\n",
    "    x = layer(x)  # Self-attention + FFN\n",
    "output = x  # Use for classification, NER, etc.\n",
    "```\n",
    "\n",
    "**Why encoder-only?**\n",
    "- **Bidirectional context**: Can attend to both past and future\n",
    "- **Better representations**: Sees full context for each token\n",
    "- **Best for**: Classification, NER, question answering\n",
    "\n",
    "**Examples**: BERT, RoBERTa, DeBERTa\n",
    "\n",
    "### 2. Decoder-Only (GPT)\n",
    "\n",
    "**Use only the decoder stack (remove cross-attention)**\n",
    "\n",
    "```python\n",
    "# GPT architecture\n",
    "x = embeddings + positional_encoding\n",
    "for layer in decoder_layers:\n",
    "    x = masked_self_attention(x)  # Causal masking\n",
    "    x = ffn(x)\n",
    "logits = output_projection(x)\n",
    "```\n",
    "\n",
    "**Why decoder-only?**\n",
    "- **Autoregressive generation**: Natural for text generation\n",
    "- **Simpler**: No encoder, no cross-attention\n",
    "- **Scalable**: Easier to scale to 100B+ parameters\n",
    "- **Best for**: Text generation, language modeling\n",
    "\n",
    "**Examples**: GPT-2, GPT-3, GPT-4, LLaMA, Mistral\n",
    "\n",
    "**Why GPT-style dominates in 2023-2025**:\n",
    "- Pre-train on next-token prediction (simple, scalable)\n",
    "- Can be fine-tuned for any task\n",
    "- Emergent abilities at scale\n",
    "\n",
    "### 3. Encoder-Decoder (T5)\n",
    "\n",
    "**Full transformer with both stacks**\n",
    "\n",
    "**Why encoder-decoder?**\n",
    "- **Best for seq2seq**: Translation, summarization\n",
    "- **Flexible**: Can handle different input/output lengths\n",
    "- **Best for**: Tasks with clear input → output structure\n",
    "\n",
    "**Examples**: T5, BART, mT5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick comparison\n",
    "print(\"TRANSFORMER ARCHITECTURAL VARIANTS\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "variants = [\n",
    "    {\n",
    "        'name': 'Encoder-Only (BERT)',\n",
    "        'structure': 'Embeddings → Encoder × N → Task head',\n",
    "        'attention': 'Bidirectional self-attention',\n",
    "        'best_for': 'Classification, NER, embeddings',\n",
    "        'examples': 'BERT, RoBERTa, DeBERTa'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Decoder-Only (GPT)',\n",
    "        'structure': 'Embeddings → Decoder × N → LM head',\n",
    "        'attention': 'Causal (masked) self-attention',\n",
    "        'best_for': 'Text generation, language modeling',\n",
    "        'examples': 'GPT-2/3/4, LLaMA, Mistral'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Encoder-Decoder (T5)',\n",
    "        'structure': 'Encoder × N → Decoder × N',\n",
    "        'attention': 'Bi + Cross + Causal attention',\n",
    "        'best_for': 'Translation, summarization, seq2seq',\n",
    "        'examples': 'T5, BART, mT5'\n",
    "    }\n",
    "]\n",
    "\n",
    "for v in variants:\n",
    "    print(f\"\\n{v['name']}\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"  Structure:  {v['structure']}\")\n",
    "    print(f\"  Attention:  {v['attention']}\")\n",
    "    print(f\"  Best for:   {v['best_for']}\")\n",
    "    print(f\"  Examples:   {v['examples']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"\\nTrend in 2023-2025: Decoder-only models dominate!\")\n",
    "print(\"Why? Scalability + emergent abilities + simpler architecture\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Count Attention Operations\n",
    "\n",
    "For a sequence of length N with d_model dimension:\n",
    "- How many attention operations in encoder-only with 12 layers?\n",
    "- How many in decoder-only?\n",
    "- How many in encoder-decoder (6 + 6 layers)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "# SOLUTION\n",
    "def show_solution_1():\n",
    "    N = 512  # Sequence length\n",
    "    \n",
    "    print(\"Attention operation complexity: O(N²·d)\\n\")\n",
    "    \n",
    "    # Encoder-only (BERT)\n",
    "    bert_layers = 12\n",
    "    bert_attn_per_layer = 1  # Self-attention only\n",
    "    bert_total = bert_layers * bert_attn_per_layer\n",
    "    \n",
    "    # Decoder-only (GPT)\n",
    "    gpt_layers = 12\n",
    "    gpt_attn_per_layer = 1  # Masked self-attention only\n",
    "    gpt_total = gpt_layers * gpt_attn_per_layer\n",
    "    \n",
    "    # Encoder-decoder (T5)\n",
    "    enc_layers = 6\n",
    "    dec_layers = 6\n",
    "    t5_total = (enc_layers * 1) + (dec_layers * 2)  # Decoder has self + cross\n",
    "    \n",
    "    print(f\"For sequence length N={N}:\\n\")\n",
    "    print(f\"Encoder-only (BERT, 12 layers):\")\n",
    "    print(f\"  {bert_total} attention operations\")\n",
    "    print(f\"  All are bidirectional self-attention\\n\")\n",
    "    \n",
    "    print(f\"Decoder-only (GPT, 12 layers):\")\n",
    "    print(f\"  {gpt_total} attention operations\")\n",
    "    print(f\"  All are causal self-attention\\n\")\n",
    "    \n",
    "    print(f\"Encoder-decoder (T5, 6+6 layers):\")\n",
    "    print(f\"  Encoder: {enc_layers} self-attention\")\n",
    "    print(f\"  Decoder: {dec_layers} self-attention + {dec_layers} cross-attention\")\n",
    "    print(f\"  Total: {t5_total} attention operations\\n\")\n",
    "    \n",
    "    print(\"Why this matters:\")\n",
    "    print(\"- Attention is O(N²) - the bottleneck for long sequences\")\n",
    "    print(\"- Modern optimizations: Flash Attention, sparse attention\")\n",
    "    print(\"- This is why context length matters for cost!\")\n",
    "\n",
    "# Uncomment to see solution:\n",
    "# show_solution_1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Implement a Simple GPT-style Decoder\n",
    "\n",
    "Build a decoder-only model (GPT-style) by removing the cross-attention from decoder layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "# SOLUTION\n",
    "def show_solution_2():\n",
    "    class GPTBlock(nn.Module):\n",
    "        \"\"\"GPT-style decoder block (no cross-attention).\"\"\"\n",
    "        def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "            super(GPTBlock, self).__init__()\n",
    "            \n",
    "            self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "            self.feed_forward = PositionWiseFeedForward(d_model, d_ff, dropout)\n",
    "            \n",
    "            self.sublayer1 = SublayerConnection(d_model, dropout)\n",
    "            self.sublayer2 = SublayerConnection(d_model, dropout)\n",
    "        \n",
    "        def forward(self, x, mask=None):\n",
    "            # Causal self-attention\n",
    "            x = self.sublayer1(x, lambda x: self.self_attn(x, x, x, mask)[0])\n",
    "            # Feed-forward\n",
    "            x = self.sublayer2(x, self.feed_forward)\n",
    "            return x\n",
    "    \n",
    "    class GPT(nn.Module):\n",
    "        \"\"\"Simple GPT-style language model.\"\"\"\n",
    "        def __init__(self, vocab_size, d_model=768, num_heads=12, \n",
    "                     num_layers=12, d_ff=3072, max_len=1024, dropout=0.1):\n",
    "            super(GPT, self).__init__()\n",
    "            \n",
    "            self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "            self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
    "            \n",
    "            self.blocks = nn.ModuleList([\n",
    "                GPTBlock(d_model, num_heads, d_ff, dropout)\n",
    "                for _ in range(num_layers)\n",
    "            ])\n",
    "            \n",
    "            self.ln_f = nn.LayerNorm(d_model)  # Final layer norm\n",
    "            self.lm_head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "            \n",
    "            # Tie weights (share embeddings and output projection)\n",
    "            self.lm_head.weight = self.embedding.weight\n",
    "        \n",
    "        def forward(self, x):\n",
    "            seq_len = x.size(1)\n",
    "            \n",
    "            # Create causal mask\n",
    "            mask = torch.tril(torch.ones(1, 1, seq_len, seq_len, device=x.device))\n",
    "            \n",
    "            # Embed and add position\n",
    "            x = self.embedding(x)\n",
    "            x = self.pos_encoding(x)\n",
    "            \n",
    "            # Pass through blocks\n",
    "            for block in self.blocks:\n",
    "                x = block(x, mask)\n",
    "            \n",
    "            # Final norm and project to vocab\n",
    "            x = self.ln_f(x)\n",
    "            logits = self.lm_head(x)\n",
    "            \n",
    "            return logits\n",
    "    \n",
    "    # Test GPT\n",
    "    vocab_size = 50257  # GPT-2 vocab size\n",
    "    gpt = GPT(vocab_size, d_model=768, num_heads=12, num_layers=12)\n",
    "    \n",
    "    x = torch.randint(0, vocab_size, (2, 64))  # Batch of 2, sequence length 64\n",
    "    logits = gpt(x)\n",
    "    \n",
    "    print(\"GPT-style Decoder:\")\n",
    "    print(f\"Input: {x.shape}\")\n",
    "    print(f\"Output logits: {logits.shape}\")\n",
    "    print(f\"\\nTotal parameters: {sum(p.numel() for p in gpt.parameters()):,}\")\n",
    "    print(\"\\nKey differences from full transformer:\")\n",
    "    print(\"✓ No encoder (decoder-only)\")\n",
    "    print(\"✓ No cross-attention (only self-attention)\")\n",
    "    print(\"✓ Causal masking (can't see future)\")\n",
    "    print(\"✓ Tied embeddings (same weights for input/output)\")\n",
    "\n",
    "# Uncomment to see solution:\n",
    "# show_solution_2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Visualize Attention Patterns\n",
    "\n",
    "Extract and visualize attention patterns from different layers to see what the model learns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This would require running a trained model and extracting attention weights\n",
    "# Left as an advanced exercise for you to explore with pre-trained models!\n",
    "\n",
    "print(\"Attention visualization is a powerful debugging tool!\")\n",
    "print(\"\\nWith a trained transformer, you can:\")\n",
    "print(\"1. See which tokens attend to each other\")\n",
    "print(\"2. Understand what each head learns\")\n",
    "print(\"3. Debug model behavior\")\n",
    "print(\"4. Gain insights into language structure\")\n",
    "print(\"\\nTools: BertViz, exBERT, Transformer Interpret\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### Core Concepts\n",
    "\n",
    "**1. Why transformers revolutionized AI**:\n",
    "- **Parallelization**: Process all tokens simultaneously (vs sequential RNNs)\n",
    "- **Long-range dependencies**: Direct connections via attention\n",
    "- **Scalability**: Can train models with 100B+ parameters\n",
    "- **Transfer learning**: Pre-train once, fine-tune for many tasks\n",
    "\n",
    "**2. Complete architecture** (encoder-decoder):\n",
    "- **Embeddings + Positional Encoding**: Convert tokens to positioned vectors\n",
    "- **Encoder**: N layers of (self-attention + FFN)\n",
    "- **Decoder**: N layers of (masked self-attention + cross-attention + FFN)\n",
    "- **Residual connections + Layer norm**: Enable deep stacks\n",
    "\n",
    "**3. Three architectural variants**:\n",
    "- **Encoder-only (BERT)**: Bidirectional, best for understanding tasks\n",
    "- **Decoder-only (GPT)**: Causal, best for generation (dominates 2023-2025)\n",
    "- **Encoder-decoder (T5)**: Full transformer, best for seq2seq\n",
    "\n",
    "**4. Key design principles**:\n",
    "- **Attention**: Mix information across positions\n",
    "- **FFN**: Process each position independently\n",
    "- **Alternating**: Global mixing + local processing = hierarchical features\n",
    "- **Residuals**: Enable gradient flow in deep networks\n",
    "- **Layer norm**: Stabilize training\n",
    "\n",
    "**5. Why each component matters**:\n",
    "- **Multi-head attention**: Multiple perspectives on relationships\n",
    "- **Feed-forward**: Non-linear transformation and capacity\n",
    "- **Positional encoding**: Inject sequence order\n",
    "- **Residual connections**: Cannot train deep networks without them\n",
    "- **Layer normalization**: Cannot train stably without it\n",
    "\n",
    "### Modern Landscape (2023-2025)\n",
    "\n",
    "**Decoder-only models dominate**:\n",
    "- GPT-4, Claude, LLaMA, Mistral all use decoder-only\n",
    "- Why? Simpler, scales better, emergent abilities\n",
    "\n",
    "**Architectural innovations**:\n",
    "- **RoPE**: Better positional encoding\n",
    "- **Flash Attention**: Faster attention computation\n",
    "- **GQA**: Fewer KV heads for efficiency\n",
    "- **MoE**: Conditional computation for scaling\n",
    "\n",
    "**Context length explosion**:\n",
    "- 2017: 512 tokens (original Transformer)\n",
    "- 2023: 100K+ tokens (GPT-4, Claude)\n",
    "- 2025: 1M+ tokens emerging\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "You've mastered the transformer! Next topics:\n",
    "- **Advanced attention**: Flash Attention, GQA, MoE\n",
    "- **Training at scale**: Distributed training, optimization\n",
    "- **Building LLMs**: Complete implementations\n",
    "\n",
    "Understanding transformers deeply is your foundation for all of modern AI!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "\n",
    "### Essential Papers\n",
    "1. **Vaswani et al. (2017)**: \"Attention is All You Need\" (Original transformer)\n",
    "2. **Devlin et al. (2018)**: \"BERT\" (Encoder-only)\n",
    "3. **Radford et al. (2018/2019)**: \"GPT/GPT-2\" (Decoder-only)\n",
    "4. **Brown et al. (2020)**: \"GPT-3\" (Scaling decoder-only)\n",
    "5. **Raffel et al. (2019)**: \"T5\" (Encoder-decoder, text-to-text)\n",
    "\n",
    "### Tutorials and Visualizations\n",
    "6. **The Illustrated Transformer** (Jay Alammar): http://jalammar.github.io/illustrated-transformer/\n",
    "7. **The Annotated Transformer** (Harvard NLP): http://nlp.seas.harvard.edu/annotated-transformer/\n",
    "8. **Attention? Attention!** (Lilian Weng): https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/\n",
    "\n",
    "### Implementation Resources\n",
    "- **PyTorch Transformer Tutorial**: Official docs\n",
    "- **Hugging Face Transformers**: transformers library\n",
    "- **MinGPT** (Andrej Karpathy): Clean GPT implementation\n",
    "- **nanoGPT** (Andrej Karpathy): Minimal GPT for learning\n",
    "\n",
    "### Advanced Topics\n",
    "- **Efficient Transformers**: Survey of optimization techniques\n",
    "- **Scaling Laws**: How performance scales with size\n",
    "- **Emergent Abilities**: Capabilities that arise at scale"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
