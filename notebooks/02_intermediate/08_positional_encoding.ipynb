{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic 8: Positional Encoding & Embeddings\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Understand **why** position information is critical in transformers\n",
    "- Learn different positional encoding strategies and when to use each\n",
    "- Implement sinusoidal positional encoding (original Transformer)\n",
    "- Build learned positional embeddings (BERT, GPT)\n",
    "- Master Rotary Position Embeddings (RoPE) - the modern standard\n",
    "- Visualize positional encodings to build intuition\n",
    "- Connect positional encodings to modern LLM architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Big Picture: Why Do We Need Positional Encoding?\n",
    "\n",
    "### The Problem: Attention is Permutation Invariant\n",
    "\n",
    "**Critical insight**: Self-attention treats input as a **set**, not a **sequence**!\n",
    "\n",
    "Consider these two sentences:\n",
    "```\n",
    "Sentence 1: \"The cat chased the mouse\"\n",
    "Sentence 2: \"The mouse chased the cat\"\n",
    "```\n",
    "\n",
    "**Without positional encoding**: Both sentences would produce the SAME attention output!\n",
    "\n",
    "**Why?** Attention computes:\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "- This operation is **permutation invariant**\n",
    "- Swapping the order of tokens doesn't change the computation\n",
    "- Word order information is completely lost!\n",
    "\n",
    "**Real-world impact**:\n",
    "- \"Dog bites man\" vs \"Man bites dog\" → Completely different meanings!\n",
    "- \"Not good\" vs \"Good not\" → Order matters for negation\n",
    "- \"First, second, third\" → Sequence is crucial\n",
    "\n",
    "### The Solution: Inject Position Information\n",
    "\n",
    "**Key idea**: Add position-dependent information to token embeddings\n",
    "\n",
    "```python\n",
    "# Original (position-agnostic)\n",
    "input_embedding = token_embedding\n",
    "\n",
    "# With positional encoding\n",
    "input_embedding = token_embedding + positional_encoding\n",
    "```\n",
    "\n",
    "**Why this works**:\n",
    "- Each position gets a unique signal\n",
    "- Model can learn to use position information\n",
    "- Attention patterns become position-aware\n",
    "\n",
    "**Why it cannot be skipped**: Without positional encoding, transformers are just sophisticated bag-of-words models. They cannot understand sequence, order, or structure—making them useless for language, code, time series, or any sequential data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Set random seed\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Visualization setup\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy 1: Sinusoidal Positional Encoding\n",
    "\n",
    "### The Original Transformer Approach\n",
    "\n",
    "**Introduced in**: \"Attention is All You Need\" (Vaswani et al., 2017)\n",
    "\n",
    "**Key insight**: Use sine and cosine functions with different frequencies\n",
    "\n",
    "$$PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n",
    "\n",
    "$$PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n",
    "\n",
    "Where:\n",
    "- $pos$: Position in sequence (0, 1, 2, ...)\n",
    "- $i$: Dimension index (0 to $d_{model}/2$)\n",
    "- $d_{model}$: Model dimension (e.g., 512)\n",
    "\n",
    "### Why Sinusoidal Functions?\n",
    "\n",
    "**1. Unique encoding for each position**:\n",
    "- Different frequencies create unique patterns\n",
    "- No two positions have the same encoding\n",
    "\n",
    "**2. Extrapolation to longer sequences**:\n",
    "- Mathematical function continues beyond training length\n",
    "- Can handle sequences longer than seen during training\n",
    "\n",
    "**3. Relative position information**:\n",
    "- $PE_{pos+k}$ can be expressed as a linear function of $PE_{pos}$\n",
    "- Model can learn to attend by relative position\n",
    "\n",
    "**4. No learnable parameters**:\n",
    "- Fixed function (not learned during training)\n",
    "- Reduces overfitting\n",
    "- Works well out of the box\n",
    "\n",
    "**Why different frequencies?**:\n",
    "- **Low frequencies** (slow oscillation): Distinguish distant positions\n",
    "- **High frequencies** (fast oscillation): Distinguish nearby positions\n",
    "- Together, they provide multi-scale position information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Sinusoidal positional encoding as used in the original Transformer.\n",
    "    \n",
    "    This is a fixed (non-learnable) encoding.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: Dimension of embeddings\n",
    "            max_len: Maximum sequence length to precompute\n",
    "        \"\"\"\n",
    "        super(SinusoidalPositionalEncoding, self).__init__()\n",
    "        \n",
    "        # Create positional encoding matrix (max_len, d_model)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        \n",
    "        # Position indices (0, 1, 2, ..., max_len-1)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        \n",
    "        # Compute the div_term for different frequencies\n",
    "        # Why 10000? Chosen empirically to span the frequency range well\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
    "                             (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        # Apply sin to even indices (0, 2, 4, ...)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        \n",
    "        # Apply cos to odd indices (1, 3, 5, ...)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # Add batch dimension: (1, max_len, d_model)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        \n",
    "        # Register as buffer (not a parameter, but part of model state)\n",
    "        # Why buffer? It's saved with the model but not updated during training\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input embeddings (batch, seq_len, d_model)\n",
    "        \n",
    "        Returns:\n",
    "            x with positional encoding added\n",
    "        \"\"\"\n",
    "        # Add positional encoding to input\n",
    "        # [:, :x.size(1)] handles variable sequence lengths\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return x\n",
    "\n",
    "# Create and visualize\n",
    "d_model = 128\n",
    "max_len = 100\n",
    "\n",
    "pos_encoder = SinusoidalPositionalEncoding(d_model, max_len)\n",
    "\n",
    "# Extract the positional encoding matrix\n",
    "pe_matrix = pos_encoder.pe.squeeze(0).numpy()\n",
    "\n",
    "print(f\"Positional encoding shape: {pe_matrix.shape}\")\n",
    "print(f\"Each position gets a {d_model}-dimensional vector\")\n",
    "print(f\"\\nPositional encoding at position 0:\")\n",
    "print(pe_matrix[0, :10])  # First 10 dimensions\n",
    "print(f\"\\nPositional encoding at position 50:\")\n",
    "print(pe_matrix[50, :10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Sinusoidal Positional Encoding\n",
    "\n",
    "Let's visualize the patterns to build intuition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize positional encoding heatmap\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Full heatmap\n",
    "sns.heatmap(pe_matrix, cmap='RdBu', center=0, ax=axes[0], \n",
    "            cbar_kws={'label': 'Encoding Value'})\n",
    "axes[0].set_title('Sinusoidal Positional Encoding\\n(Positions × Dimensions)', fontsize=14)\n",
    "axes[0].set_xlabel('Embedding Dimension')\n",
    "axes[0].set_ylabel('Position in Sequence')\n",
    "\n",
    "# Zoom in on first 50 positions and 64 dimensions\n",
    "sns.heatmap(pe_matrix[:50, :64], cmap='RdBu', center=0, ax=axes[1],\n",
    "            cbar_kws={'label': 'Encoding Value'})\n",
    "axes[1].set_title('Zoomed View (First 50 positions, 64 dims)', fontsize=14)\n",
    "axes[1].set_xlabel('Embedding Dimension')\n",
    "axes[1].set_ylabel('Position in Sequence')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice the wave-like patterns:\")\n",
    "print(\"- Early dimensions (left): Slow oscillation (low frequency)\")\n",
    "print(\"- Later dimensions (right): Fast oscillation (high frequency)\")\n",
    "print(\"- Each position has a unique 'fingerprint' across all dimensions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize specific dimension patterns\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "positions = np.arange(max_len)\n",
    "dimensions_to_plot = [0, 10, 50, 120]  # Different frequencies\n",
    "\n",
    "for idx, dim in enumerate(dimensions_to_plot):\n",
    "    axes[idx].plot(positions, pe_matrix[:, dim], linewidth=2)\n",
    "    axes[idx].set_title(f'Dimension {dim} ({'Even (sin)' if dim % 2 == 0 else 'Odd (cos)'})', fontsize=12)\n",
    "    axes[idx].set_xlabel('Position')\n",
    "    axes[idx].set_ylabel('Encoding Value')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "    axes[idx].axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "\n",
    "plt.suptitle('Sinusoidal Patterns at Different Dimensions\\n(Notice how frequency increases)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key observations:\")\n",
    "print(\"1. Dimension 0 (lowest frequency): Very slow oscillation\")\n",
    "print(\"2. Dimension 120 (highest frequency): Rapid oscillation\")\n",
    "print(\"3. This multi-frequency approach captures both local and global position info\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy 2: Learned Positional Embeddings\n",
    "\n",
    "### The BERT/GPT Approach\n",
    "\n",
    "**Key difference**: Positional encoding is a **learnable parameter**, not a fixed function\n",
    "\n",
    "```python\n",
    "# Instead of computing sin/cos:\n",
    "position_embeddings = nn.Embedding(max_len, d_model)\n",
    "```\n",
    "\n",
    "**Why learned positional embeddings?**\n",
    "\n",
    "**Advantages**:\n",
    "1. **Task-specific**: Can adapt to specific patterns in your data\n",
    "2. **Simpler**: Just an embedding lookup (very fast)\n",
    "3. **Empirically strong**: Often performs better on fixed-length tasks\n",
    "4. **Flexible**: No mathematical constraints\n",
    "\n",
    "**Disadvantages**:\n",
    "1. **No extrapolation**: Cannot handle sequences longer than training\n",
    "2. **More parameters**: Needs to learn position information\n",
    "3. **Overfitting risk**: Can memorize instead of generalize\n",
    "\n",
    "**Used in**:\n",
    "- **BERT**: Learned absolute positions\n",
    "- **GPT-2**: Learned absolute positions\n",
    "- **ViT** (Vision Transformer): 2D learned positional embeddings\n",
    "\n",
    "**When to use**:\n",
    "- Fixed maximum sequence length (e.g., BERT's 512 tokens)\n",
    "- Sufficient training data to learn good positions\n",
    "- Don't need to extrapolate beyond training length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearnedPositionalEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Learned positional embeddings (BERT/GPT-style).\n",
    "    \n",
    "    Simple embedding lookup, learned during training.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: Dimension of embeddings\n",
    "            max_len: Maximum sequence length\n",
    "        \"\"\"\n",
    "        super(LearnedPositionalEmbedding, self).__init__()\n",
    "        \n",
    "        # Create learnable position embeddings\n",
    "        # Why Embedding? It's just a lookup table that's updated during training\n",
    "        self.position_embeddings = nn.Embedding(max_len, d_model)\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input embeddings (batch, seq_len, d_model)\n",
    "        \n",
    "        Returns:\n",
    "            x with learned positional embeddings added\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "        \n",
    "        # Check sequence length\n",
    "        assert seq_len <= self.max_len, f\"Sequence length {seq_len} exceeds max_len {self.max_len}\"\n",
    "        \n",
    "        # Create position indices [0, 1, 2, ..., seq_len-1]\n",
    "        positions = torch.arange(seq_len, device=x.device).unsqueeze(0).expand(batch_size, -1)\n",
    "        \n",
    "        # Lookup positional embeddings\n",
    "        pos_embeddings = self.position_embeddings(positions)  # (batch, seq_len, d_model)\n",
    "        \n",
    "        # Add to input\n",
    "        return x + pos_embeddings\n",
    "\n",
    "# Create learned positional embedding\n",
    "d_model = 128\n",
    "max_len = 512\n",
    "\n",
    "learned_pos = LearnedPositionalEmbedding(d_model, max_len)\n",
    "\n",
    "# Test\n",
    "x = torch.randn(2, 100, d_model)  # (batch=2, seq_len=100, d_model=128)\n",
    "x_with_pos = learned_pos(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {x_with_pos.shape}\")\n",
    "print(f\"\\nNumber of learnable parameters: {sum(p.numel() for p in learned_pos.parameters()):,}\")\n",
    "print(f\"  = max_len ({max_len}) × d_model ({d_model})\")\n",
    "\n",
    "# Visualize learned embeddings (randomly initialized)\n",
    "learned_matrix = learned_pos.position_embeddings.weight.detach().numpy()\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.heatmap(learned_matrix[:100, :64], cmap='viridis', \n",
    "            cbar_kws={'label': 'Embedding Value'})\n",
    "plt.title('Learned Positional Embeddings (Random Initialization)\\nFirst 100 positions, 64 dimensions', \n",
    "          fontsize=14)\n",
    "plt.xlabel('Embedding Dimension')\n",
    "plt.ylabel('Position')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNote: These are randomly initialized.\")\n",
    "print(\"During training, the model learns meaningful position patterns!\")\n",
    "print(\"After training, nearby positions often have similar embeddings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy 3: Rotary Position Embeddings (RoPE)\n",
    "\n",
    "### The Modern Standard (2021-2025)\n",
    "\n",
    "**Introduced in**: \"RoFormer: Enhanced Transformer with Rotary Position Embedding\" (Su et al., 2021)\n",
    "\n",
    "**Used in**: LLaMA, PaLM, GPT-NeoX, Mistral, and most modern LLMs (2023+)\n",
    "\n",
    "**Why RoPE is revolutionary**: It encodes **relative** position directly into the attention mechanism!\n",
    "\n",
    "### The Key Insight\n",
    "\n",
    "**Problem with absolute positional encoding**:\n",
    "- Token at position 5 gets the same encoding regardless of context\n",
    "- Attention doesn't naturally capture relative distances\n",
    "\n",
    "**RoPE's solution**: Rotate query and key vectors by their position\n",
    "\n",
    "**Mathematical intuition**:\n",
    "- Represent position as a rotation angle: $\\theta = pos \\times \\omega$\n",
    "- Rotate Q and K by their respective angles\n",
    "- When computing $Q \\cdot K$, the dot product naturally captures relative position!\n",
    "\n",
    "$$Q_{pos_1} \\cdot K_{pos_2} = f(pos_1 - pos_2)$$\n",
    "\n",
    "**Why this is important**:\n",
    "1. **Relative position**: Attention naturally depends on relative distance\n",
    "2. **Extrapolation**: Can handle longer sequences than training\n",
    "3. **Efficiency**: No extra addition of positional encoding\n",
    "4. **Better performance**: Empirically outperforms other methods\n",
    "\n",
    "**How it works (simplified)**:\n",
    "```python\n",
    "# Instead of: x + positional_encoding\n",
    "# We do: rotate(x, position)\n",
    "\n",
    "def rotate(x, position):\n",
    "    angle = position * frequency\n",
    "    return apply_rotation_matrix(x, angle)\n",
    "```\n",
    "\n",
    "**Why RoPE cannot be skipped in modern LLMs**:\n",
    "- **Industry standard**: Nearly all LLMs since 2023 use RoPE\n",
    "- **Superior extrapolation**: Can extend context length after training\n",
    "- **Better relative position**: More natural for language structure\n",
    "- **Proven at scale**: Works reliably up to 100K+ token contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotaryPositionalEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Rotary Position Embedding (RoPE).\n",
    "    \n",
    "    Key idea: Rotate queries and keys by their position.\n",
    "    This encodes relative position into attention!\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, max_len=2048, base=10000):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim: Dimension per head (must be even)\n",
    "            max_len: Maximum sequence length\n",
    "            base: Base for frequency computation (10000 is standard)\n",
    "        \"\"\"\n",
    "        super(RotaryPositionalEmbedding, self).__init__()\n",
    "        \n",
    "        assert dim % 2 == 0, \"Dimension must be even for RoPE\"\n",
    "        \n",
    "        self.dim = dim\n",
    "        self.max_len = max_len\n",
    "        self.base = base\n",
    "        \n",
    "        # Compute frequencies for rotation\n",
    "        # Why different frequencies? Same reason as sinusoidal PE\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer('inv_freq', inv_freq)\n",
    "        \n",
    "        # Precompute rotations for all positions up to max_len\n",
    "        self._compute_cos_sin_cache(max_len)\n",
    "    \n",
    "    def _compute_cos_sin_cache(self, max_len):\n",
    "        \"\"\"\n",
    "        Precompute cos and sin values for all positions.\n",
    "        \n",
    "        Why cache? Rotation matrices are fixed, so compute once.\n",
    "        \"\"\"\n",
    "        positions = torch.arange(max_len, dtype=torch.float32)\n",
    "        \n",
    "        # Compute angles: pos * freq for each position and frequency\n",
    "        # Shape: (max_len, dim/2)\n",
    "        angles = torch.einsum('i,j->ij', positions, self.inv_freq)\n",
    "        \n",
    "        # Duplicate for (cos, sin) pairs: (max_len, dim)\n",
    "        angles = torch.cat([angles, angles], dim=-1)\n",
    "        \n",
    "        # Precompute cos and sin\n",
    "        self.register_buffer('cos_cached', angles.cos())\n",
    "        self.register_buffer('sin_cached', angles.sin())\n",
    "    \n",
    "    def rotate_half(self, x):\n",
    "        \"\"\"\n",
    "        Rotate half the dimensions (pairs).\n",
    "        \n",
    "        Why? RoPE applies 2D rotations to consecutive dimension pairs.\n",
    "        \n",
    "        For a pair (x1, x2), rotation by θ:\n",
    "          x1' = x1*cos(θ) - x2*sin(θ)\n",
    "          x2' = x1*sin(θ) + x2*cos(θ)\n",
    "        \"\"\"\n",
    "        x1, x2 = x[..., ::2], x[..., 1::2]\n",
    "        return torch.cat([-x2, x1], dim=-1)\n",
    "    \n",
    "    def apply_rotary_pos_emb(self, x, position_ids):\n",
    "        \"\"\"\n",
    "        Apply rotary position embedding to input.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor (..., seq_len, dim)\n",
    "            position_ids: Position indices (seq_len,)\n",
    "        \n",
    "        Returns:\n",
    "            Rotated tensor with same shape as x\n",
    "        \"\"\"\n",
    "        # Get cos and sin for these positions\n",
    "        cos = self.cos_cached[position_ids].unsqueeze(0)\n",
    "        sin = self.sin_cached[position_ids].unsqueeze(0)\n",
    "        \n",
    "        # Apply rotation: x*cos + rotate_half(x)*sin\n",
    "        # This implements the 2D rotation formula efficiently\n",
    "        return (x * cos) + (self.rotate_half(x) * sin)\n",
    "    \n",
    "    def forward(self, q, k, position_ids=None):\n",
    "        \"\"\"\n",
    "        Apply RoPE to queries and keys.\n",
    "        \n",
    "        Args:\n",
    "            q: Queries (batch, num_heads, seq_len, dim)\n",
    "            k: Keys (batch, num_heads, seq_len, dim)\n",
    "            position_ids: Optional position indices\n",
    "        \n",
    "        Returns:\n",
    "            q_rot, k_rot: Rotated queries and keys\n",
    "        \"\"\"\n",
    "        seq_len = q.size(2)\n",
    "        \n",
    "        if position_ids is None:\n",
    "            position_ids = torch.arange(seq_len, device=q.device)\n",
    "        \n",
    "        # Apply rotation to Q and K\n",
    "        # Why both? The dot product Q*K captures relative position!\n",
    "        q_rot = self.apply_rotary_pos_emb(q, position_ids)\n",
    "        k_rot = self.apply_rotary_pos_emb(k, position_ids)\n",
    "        \n",
    "        return q_rot, k_rot\n",
    "\n",
    "# Test RoPE\n",
    "dim = 64  # Dimension per head\n",
    "seq_len = 20\n",
    "batch_size = 2\n",
    "num_heads = 4\n",
    "\n",
    "rope = RotaryPositionalEmbedding(dim, max_len=2048)\n",
    "\n",
    "# Create dummy Q, K\n",
    "q = torch.randn(batch_size, num_heads, seq_len, dim)\n",
    "k = torch.randn(batch_size, num_heads, seq_len, dim)\n",
    "\n",
    "# Apply RoPE\n",
    "q_rot, k_rot = rope(q, k)\n",
    "\n",
    "print(f\"Query shape: {q.shape}\")\n",
    "print(f\"Rotated query shape: {q_rot.shape}\")\n",
    "print(f\"\\nKey insight: Shape unchanged, but vectors are rotated by their position!\")\n",
    "print(f\"\\nWhen computing attention scores (Q_rot @ K_rot.T):\")\n",
    "print(f\"The dot product naturally captures RELATIVE position!\")\n",
    "\n",
    "# Verify rotation preserves norm (important property)\n",
    "q_norm = q.norm(dim=-1)\n",
    "q_rot_norm = q_rot.norm(dim=-1)\n",
    "\n",
    "print(f\"\\nOriginal Q norm: {q_norm[0, 0, 0].item():.4f}\")\n",
    "print(f\"Rotated Q norm: {q_rot_norm[0, 0, 0].item():.4f}\")\n",
    "print(\"✓ Norm preserved (rotation doesn't change magnitude)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing RoPE: How Rotation Captures Position\n",
    "\n",
    "Let's visualize how RoPE encodes position through rotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize rotation angles\n",
    "dim = 64\n",
    "max_len = 100\n",
    "\n",
    "rope = RotaryPositionalEmbedding(dim, max_len)\n",
    "\n",
    "# Extract rotation matrices\n",
    "cos_matrix = rope.cos_cached.numpy()\n",
    "sin_matrix = rope.sin_cached.numpy()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Cosine components\n",
    "sns.heatmap(cos_matrix, cmap='RdBu', center=0, ax=axes[0],\n",
    "            cbar_kws={'label': 'cos(θ)'})\n",
    "axes[0].set_title('RoPE: Cosine Components\\n(Rotation angles for each position)', fontsize=14)\n",
    "axes[0].set_xlabel('Dimension')\n",
    "axes[0].set_ylabel('Position')\n",
    "\n",
    "# Sine components\n",
    "sns.heatmap(sin_matrix, cmap='RdBu', center=0, ax=axes[1],\n",
    "            cbar_kws={'label': 'sin(θ)'})\n",
    "axes[1].set_title('RoPE: Sine Components\\n(Rotation angles for each position)', fontsize=14)\n",
    "axes[1].set_xlabel('Dimension')\n",
    "axes[1].set_ylabel('Position')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice the similarity to sinusoidal positional encoding!\")\n",
    "print(\"But instead of ADDING these values, RoPE ROTATES by these angles.\")\n",
    "print(\"\\nThis makes relative position emerge naturally in Q·K dot products!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate relative position property\n",
    "def compute_attention_scores(q, k):\n",
    "    \"\"\"Compute attention scores (before softmax)\"\"\"\n",
    "    return torch.matmul(q, k.transpose(-2, -1)) / np.sqrt(q.size(-1))\n",
    "\n",
    "# Create simple Q, K\n",
    "seq_len = 10\n",
    "dim = 64\n",
    "q = torch.randn(1, 1, seq_len, dim)\n",
    "k = torch.randn(1, 1, seq_len, dim)\n",
    "\n",
    "# Without RoPE\n",
    "scores_no_rope = compute_attention_scores(q, k)\n",
    "\n",
    "# With RoPE\n",
    "rope = RotaryPositionalEmbedding(dim)\n",
    "q_rot, k_rot = rope(q, k)\n",
    "scores_with_rope = compute_attention_scores(q_rot, k_rot)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "sns.heatmap(scores_no_rope[0, 0].detach().numpy(), annot=True, fmt='.2f',\n",
    "            cmap='YlOrRd', ax=axes[0])\n",
    "axes[0].set_title('Attention Scores WITHOUT RoPE\\n(Random patterns)', fontsize=12)\n",
    "axes[0].set_xlabel('Key position')\n",
    "axes[0].set_ylabel('Query position')\n",
    "\n",
    "sns.heatmap(scores_with_rope[0, 0].detach().numpy(), annot=True, fmt='.2f',\n",
    "            cmap='YlOrRd', ax=axes[1])\n",
    "axes[1].set_title('Attention Scores WITH RoPE\\n(Structured by relative position)', fontsize=12)\n",
    "axes[1].set_xlabel('Key position')\n",
    "axes[1].set_ylabel('Query position')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key observation:\")\n",
    "print(\"WITH RoPE: Attention scores show structure based on relative distance!\")\n",
    "print(\"Nearby positions have similar scores (diagonal pattern).\")\n",
    "print(\"\\nThis is why RoPE is so powerful for language modeling!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison: Which Positional Encoding to Use?\n",
    "\n",
    "### Quick Decision Guide\n",
    "\n",
    "| Method | Pros | Cons | Best For |\n",
    "|--------|------|------|----------|\n",
    "| **Sinusoidal** | • No parameters<br>• Extrapolates well<br>• Simple | • Absolute position<br>• Less flexible | Research, prototypes |\n",
    "| **Learned** | • Task-adaptive<br>• Simple implementation<br>• Fast | • No extrapolation<br>• More parameters | Fixed-length tasks |\n",
    "| **RoPE** | • Relative position<br>• Extrapolates well<br>• SOTA performance | • Slightly complex<br>• Requires rotation | **Modern LLMs** (recommended) |\n",
    "\n",
    "### When to Use Each\n",
    "\n",
    "**Use Sinusoidal if**:\n",
    "- Quick prototype or research\n",
    "- Want deterministic behavior\n",
    "- Don't want extra parameters\n",
    "\n",
    "**Use Learned if**:\n",
    "- Fixed maximum sequence length\n",
    "- Plenty of training data\n",
    "- Absolute position matters (rare)\n",
    "\n",
    "**Use RoPE if** (recommended for most cases):\n",
    "- Building a modern LLM or transformer\n",
    "- Need to extrapolate to longer sequences\n",
    "- Want state-of-the-art performance\n",
    "- Relative position matters (almost always in NLP)\n",
    "\n",
    "### Modern LLM Choices (2023-2025)\n",
    "\n",
    "- **LLaMA 1/2/3**: RoPE\n",
    "- **PaLM**: RoPE\n",
    "- **GPT-NeoX**: RoPE\n",
    "- **Mistral**: RoPE\n",
    "- **Qwen**: RoPE\n",
    "- **DeepSeek**: RoPE\n",
    "\n",
    "**Trend**: Nearly all modern LLMs use RoPE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison: Parameter count\n",
    "d_model = 512\n",
    "max_len = 2048\n",
    "\n",
    "# Sinusoidal: No parameters\n",
    "sin_params = 0\n",
    "\n",
    "# Learned: max_len * d_model parameters\n",
    "learned_params = max_len * d_model\n",
    "\n",
    "# RoPE: No parameters (just precomputed rotations)\n",
    "rope_params = 0\n",
    "\n",
    "print(\"Parameter Comparison:\")\n",
    "print(f\"\\nSinusoidal PE: {sin_params:,} parameters\")\n",
    "print(f\"Learned PE: {learned_params:,} parameters\")\n",
    "print(f\"RoPE: {rope_params:,} parameters\")\n",
    "\n",
    "print(f\"\\nFor a model with:\")\n",
    "print(f\"  - d_model = {d_model}\")\n",
    "print(f\"  - max_len = {max_len}\")\n",
    "print(f\"\\nLearned PE adds {learned_params:,} parameters!\")\n",
    "print(f\"For a 7B parameter model, that's {100 * learned_params / 7e9:.3f}% overhead.\")\n",
    "\n",
    "print(\"\\nWinner: RoPE (no parameters, best performance)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini Exercises\n",
    "\n",
    "Test your understanding!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Verify Extrapolation Properties\n",
    "\n",
    "Create a model with sinusoidal PE trained on sequences up to length 50. Test if it can handle length 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "# SOLUTION\n",
    "def show_solution_1():\n",
    "    # Create sinusoidal PE with max_len=50\n",
    "    sin_pe = SinusoidalPositionalEncoding(d_model=128, max_len=50)\n",
    "    \n",
    "    # Create learned PE with max_len=50\n",
    "    learned_pe = LearnedPositionalEmbedding(d_model=128, max_len=50)\n",
    "    \n",
    "    # Test with length 40 (within training)\n",
    "    x_short = torch.randn(1, 40, 128)\n",
    "    \n",
    "    # Test with length 60 (beyond training)\n",
    "    x_long = torch.randn(1, 60, 128)\n",
    "    \n",
    "    print(\"Testing extrapolation...\\n\")\n",
    "    \n",
    "    # Sinusoidal: Should work\n",
    "    try:\n",
    "        # Extend max_len for this test\n",
    "        sin_pe_extended = SinusoidalPositionalEncoding(d_model=128, max_len=100)\n",
    "        out = sin_pe_extended(x_long)\n",
    "        print(\"✓ Sinusoidal PE: Handles length 60 (trained on 50)\")\n",
    "        print(f\"  Output shape: {out.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Sinusoidal PE failed: {e}\")\n",
    "    \n",
    "    # Learned: Should fail\n",
    "    try:\n",
    "        out = learned_pe(x_long)\n",
    "        print(\"✗ Learned PE: Should have failed but didn't!\")\n",
    "    except Exception as e:\n",
    "        print(f\"✓ Learned PE: Cannot handle length 60 (trained on 50)\")\n",
    "        print(f\"  Error: {type(e).__name__}\")\n",
    "    \n",
    "    print(\"\\nConclusion:\")\n",
    "    print(\"- Sinusoidal PE can extrapolate (mathematical function continues)\")\n",
    "    print(\"- Learned PE cannot extrapolate (no embeddings beyond max_len)\")\n",
    "    print(\"- RoPE can also extrapolate (rotation formula extends)\")\n",
    "\n",
    "# Uncomment to see solution:\n",
    "# show_solution_1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Implement ALiBi (Attention with Linear Biases)\n",
    "\n",
    "ALiBi is another modern approach that adds position bias directly to attention scores. Implement it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "class ALiBiPositionalBias(nn.Module):\n",
    "    def __init__(self, num_heads):\n",
    "        super(ALiBiPositionalBias, self).__init__()\n",
    "        # Add your code\n",
    "        pass\n",
    "    \n",
    "    def forward(self, attention_scores):\n",
    "        # Add your code\n",
    "        pass\n",
    "\n",
    "\n",
    "# SOLUTION\n",
    "def show_solution_2():\n",
    "    class ALiBiPositionalBias(nn.Module):\n",
    "        \"\"\"\n",
    "        ALiBi: Attention with Linear Biases.\n",
    "        \n",
    "        Key idea: Add position-dependent bias to attention scores.\n",
    "        Bias is proportional to distance: closer positions get smaller penalty.\n",
    "        \n",
    "        Used in: BLOOM, MPT models\n",
    "        \"\"\"\n",
    "        def __init__(self, num_heads):\n",
    "            super(ALiBiPositionalBias, self).__init__()\n",
    "            \n",
    "            # Compute slopes for each head (geometric sequence)\n",
    "            # Why different slopes? Each head has different distance sensitivity\n",
    "            slopes = torch.tensor([2 ** (-(2 ** -(i + 1))) for i in range(num_heads)])\n",
    "            self.register_buffer('slopes', slopes.view(1, num_heads, 1, 1))\n",
    "        \n",
    "        def forward(self, attention_scores):\n",
    "            \"\"\"\n",
    "            Args:\n",
    "                attention_scores: (batch, num_heads, seq_len, seq_len)\n",
    "            \n",
    "            Returns:\n",
    "                Scores with ALiBi bias added\n",
    "            \"\"\"\n",
    "            seq_len = attention_scores.size(-1)\n",
    "            \n",
    "            # Create distance matrix\n",
    "            # Element [i,j] = distance from i to j\n",
    "            positions = torch.arange(seq_len, device=attention_scores.device)\n",
    "            distance = positions.unsqueeze(0) - positions.unsqueeze(1)  # Broadcasting\n",
    "            \n",
    "            # ALiBi bias: -slope * distance\n",
    "            # Negative sign: Penalize distant positions\n",
    "            alibi_bias = -self.slopes * distance.abs()\n",
    "            \n",
    "            # Add bias to scores\n",
    "            return attention_scores + alibi_bias\n",
    "    \n",
    "    # Test ALiBi\n",
    "    num_heads = 4\n",
    "    seq_len = 10\n",
    "    \n",
    "    alibi = ALiBiPositionalBias(num_heads)\n",
    "    \n",
    "    # Create dummy attention scores\n",
    "    scores = torch.randn(1, num_heads, seq_len, seq_len)\n",
    "    scores_with_alibi = alibi(scores)\n",
    "    \n",
    "    # Visualize ALiBi bias for one head\n",
    "    bias = scores_with_alibi[0, 0] - scores[0, 0]\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(bias.detach().numpy(), annot=True, fmt='.2f', cmap='RdYlGn_r',\n",
    "                center=0, cbar_kws={'label': 'Bias'})\n",
    "    plt.title('ALiBi Positional Bias (Head 1)\\nPenalizes distant positions', fontsize=14)\n",
    "    plt.xlabel('Key position')\n",
    "    plt.ylabel('Query position')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"ALiBi properties:\")\n",
    "    print(\"✓ No positional encoding added to embeddings\")\n",
    "    print(\"✓ Bias applied directly to attention scores\")\n",
    "    print(\"✓ Linear penalty based on distance\")\n",
    "    print(\"✓ Different slopes for different heads\")\n",
    "    print(\"\\nAdvantages:\")\n",
    "    print(\"- Simple and effective\")\n",
    "    print(\"- Excellent extrapolation\")\n",
    "    print(\"- Used in BLOOM (176B parameters)\")\n",
    "\n",
    "# Uncomment to see solution:\n",
    "# show_solution_2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: 2D Positional Encoding for Images\n",
    "\n",
    "Vision Transformers need 2D positional encoding for image patches. Implement it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "# SOLUTION\n",
    "def show_solution_3():\n",
    "    class Learned2DPositionalEmbedding(nn.Module):\n",
    "        \"\"\"\n",
    "        2D positional embeddings for Vision Transformers.\n",
    "        \n",
    "        Image patches have 2D structure (height, width).\n",
    "        \"\"\"\n",
    "        def __init__(self, d_model, height, width):\n",
    "            super(Learned2DPositionalEmbedding, self).__init__()\n",
    "            \n",
    "            self.height = height\n",
    "            self.width = width\n",
    "            \n",
    "            # Separate embeddings for height and width\n",
    "            # Why separate? Height and width are independent dimensions\n",
    "            self.height_embed = nn.Embedding(height, d_model)\n",
    "            self.width_embed = nn.Embedding(width, d_model)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            \"\"\"\n",
    "            Args:\n",
    "                x: Patch embeddings (batch, height*width, d_model)\n",
    "            \n",
    "            Returns:\n",
    "                x with 2D positional embeddings added\n",
    "            \"\"\"\n",
    "            batch_size, seq_len, d_model = x.size()\n",
    "            \n",
    "            # Create 2D position grid\n",
    "            h_pos = torch.arange(self.height, device=x.device).repeat_interleave(self.width)\n",
    "            w_pos = torch.arange(self.width, device=x.device).repeat(self.height)\n",
    "            \n",
    "            # Get embeddings\n",
    "            h_embed = self.height_embed(h_pos)  # (height*width, d_model)\n",
    "            w_embed = self.width_embed(w_pos)  # (height*width, d_model)\n",
    "            \n",
    "            # Combine (sum or concatenate)\n",
    "            pos_embed = h_embed + w_embed  # (height*width, d_model)\n",
    "            \n",
    "            # Add to input\n",
    "            return x + pos_embed.unsqueeze(0)\n",
    "    \n",
    "    # Test 2D positional embedding\n",
    "    d_model = 128\n",
    "    img_size = 224\n",
    "    patch_size = 16\n",
    "    height = width = img_size // patch_size  # 14x14 patches\n",
    "    \n",
    "    pos_2d = Learned2DPositionalEmbedding(d_model, height, width)\n",
    "    \n",
    "    # Dummy patch embeddings\n",
    "    patches = torch.randn(2, height * width, d_model)\n",
    "    patches_with_pos = pos_2d(patches)\n",
    "    \n",
    "    print(f\"Image size: {img_size}x{img_size}\")\n",
    "    print(f\"Patch size: {patch_size}x{patch_size}\")\n",
    "    print(f\"Number of patches: {height}x{width} = {height*width}\")\n",
    "    print(f\"\\nPatch embeddings shape: {patches.shape}\")\n",
    "    print(f\"With 2D positional encoding: {patches_with_pos.shape}\")\n",
    "    \n",
    "    # Visualize 2D positional embeddings\n",
    "    h_weights = pos_2d.height_embed.weight.detach().numpy()\n",
    "    w_weights = pos_2d.width_embed.weight.detach().numpy()\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    sns.heatmap(h_weights[:, :32], cmap='viridis', ax=axes[0])\n",
    "    axes[0].set_title('Height Embeddings (first 32 dims)', fontsize=12)\n",
    "    axes[0].set_xlabel('Dimension')\n",
    "    axes[0].set_ylabel('Height position')\n",
    "    \n",
    "    sns.heatmap(w_weights[:, :32], cmap='viridis', ax=axes[1])\n",
    "    axes[1].set_title('Width Embeddings (first 32 dims)', fontsize=12)\n",
    "    axes[1].set_xlabel('Dimension')\n",
    "    axes[1].set_ylabel('Width position')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n2D positional encoding for images:\")\n",
    "    print(\"✓ Separate embeddings for height and width\")\n",
    "    print(\"✓ Preserves 2D spatial structure\")\n",
    "    print(\"✓ Used in Vision Transformers (ViT)\")\n",
    "\n",
    "# Uncomment to see solution:\n",
    "# show_solution_3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprehensive Exercise: Build a Complete Positional Encoding Module\n",
    "\n",
    "Create a flexible positional encoding module that supports all three methods and can switch between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "# SOLUTION\n",
    "def show_comprehensive_solution():\n",
    "    class FlexiblePositionalEncoding(nn.Module):\n",
    "        \"\"\"\n",
    "        Flexible positional encoding supporting multiple methods.\n",
    "        \n",
    "        Allows easy experimentation with different approaches!\n",
    "        \"\"\"\n",
    "        def __init__(self, d_model, max_len=2048, method='rope', num_heads=8):\n",
    "            \"\"\"\n",
    "            Args:\n",
    "                d_model: Model dimension\n",
    "                max_len: Maximum sequence length\n",
    "                method: 'sinusoidal', 'learned', 'rope', or 'alibi'\n",
    "                num_heads: Number of attention heads (for RoPE/ALiBi)\n",
    "            \"\"\"\n",
    "            super(FlexiblePositionalEncoding, self).__init__()\n",
    "            \n",
    "            self.method = method\n",
    "            self.d_model = d_model\n",
    "            \n",
    "            if method == 'sinusoidal':\n",
    "                self.pos_encoder = SinusoidalPositionalEncoding(d_model, max_len)\n",
    "            elif method == 'learned':\n",
    "                self.pos_encoder = LearnedPositionalEmbedding(d_model, max_len)\n",
    "            elif method == 'rope':\n",
    "                self.pos_encoder = RotaryPositionalEmbedding(d_model // num_heads, max_len)\n",
    "            elif method == 'alibi':\n",
    "                self.pos_encoder = ALiBiPositionalBias(num_heads)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown method: {method}\")\n",
    "        \n",
    "        def forward(self, x, q=None, k=None, attention_scores=None):\n",
    "            \"\"\"\n",
    "            Apply positional encoding.\n",
    "            \n",
    "            Args depend on method:\n",
    "            - sinusoidal/learned: Only needs x\n",
    "            - rope: Needs q, k (queries, keys)\n",
    "            - alibi: Needs attention_scores\n",
    "            \"\"\"\n",
    "            if self.method in ['sinusoidal', 'learned']:\n",
    "                return self.pos_encoder(x)\n",
    "            elif self.method == 'rope':\n",
    "                assert q is not None and k is not None, \"RoPE requires q and k\"\n",
    "                return self.pos_encoder(q, k)\n",
    "            elif self.method == 'alibi':\n",
    "                assert attention_scores is not None, \"ALiBi requires attention_scores\"\n",
    "                return self.pos_encoder(attention_scores)\n",
    "    \n",
    "    # Test all methods\n",
    "    d_model = 128\n",
    "    seq_len = 20\n",
    "    batch_size = 2\n",
    "    \n",
    "    print(\"Testing Flexible Positional Encoding...\\n\")\n",
    "    \n",
    "    # Test sinusoidal\n",
    "    sin_pe = FlexiblePositionalEncoding(d_model, method='sinusoidal')\n",
    "    x = torch.randn(batch_size, seq_len, d_model)\n",
    "    x_sin = sin_pe(x)\n",
    "    print(f\"✓ Sinusoidal: {x.shape} -> {x_sin.shape}\")\n",
    "    \n",
    "    # Test learned\n",
    "    learned_pe = FlexiblePositionalEncoding(d_model, method='learned')\n",
    "    x_learned = learned_pe(x)\n",
    "    print(f\"✓ Learned: {x.shape} -> {x_learned.shape}\")\n",
    "    \n",
    "    # Test RoPE\n",
    "    rope_pe = FlexiblePositionalEncoding(d_model, method='rope', num_heads=8)\n",
    "    q = k = torch.randn(batch_size, 8, seq_len, d_model // 8)\n",
    "    q_rot, k_rot = rope_pe(x, q=q, k=k)\n",
    "    print(f\"✓ RoPE: {q.shape} -> {q_rot.shape}\")\n",
    "    \n",
    "    print(\"\\nAll methods working! Easy to experiment with different approaches.\")\n",
    "    print(\"\\nRecommended for 2025: RoPE (best performance, extrapolation)\")\n",
    "\n",
    "# Uncomment to see solution:\n",
    "# show_comprehensive_solution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### Core Concepts\n",
    "\n",
    "**1. Why positional encoding is critical**:\n",
    "- Attention is permutation-invariant (treats input as a set)\n",
    "- Without position info, \"dog bites man\" = \"man bites dog\"\n",
    "- Positional encoding injects sequence order into transformers\n",
    "\n",
    "**2. Three main approaches**:\n",
    "\n",
    "**Sinusoidal (Original Transformer)**:\n",
    "- Fixed mathematical function (sin/cos with multiple frequencies)\n",
    "- No learnable parameters\n",
    "- Can extrapolate to longer sequences\n",
    "- Good baseline, but absolute position\n",
    "\n",
    "**Learned (BERT/GPT-2)**:\n",
    "- Learnable embedding lookup table\n",
    "- Task-adaptive, simple implementation\n",
    "- Cannot extrapolate beyond training length\n",
    "- Good for fixed-length tasks\n",
    "\n",
    "**RoPE (Modern LLMs)**:\n",
    "- Rotates Q and K by position\n",
    "- Encodes **relative** position naturally\n",
    "- Can extrapolate, no extra parameters\n",
    "- **Industry standard for 2023-2025**\n",
    "\n",
    "**3. Why RoPE is the modern standard**:\n",
    "- Relative position is more natural for language\n",
    "- Excellent extrapolation (can extend context after training)\n",
    "- Used in LLaMA, PaLM, Mistral, and nearly all modern LLMs\n",
    "- Proven at scale (up to 100K+ token contexts)\n",
    "\n",
    "**4. Key properties to consider**:\n",
    "- **Absolute vs Relative**: Relative position is usually better\n",
    "- **Extrapolation**: Can the model handle longer sequences than training?\n",
    "- **Parameters**: Does it add learnable parameters?\n",
    "- **Performance**: Empirical results on your task\n",
    "\n",
    "### Connection to Modern AI\n",
    "\n",
    "**Transformers = Attention + Position + FFN**:\n",
    "- Positional encoding is as critical as attention\n",
    "- Choice affects model's ability to understand sequence\n",
    "\n",
    "**Modern trends**:\n",
    "- **RoPE dominance**: Nearly universal in new LLMs\n",
    "- **Context extension**: ALiBi, extended RoPE for longer contexts\n",
    "- **2D/3D extensions**: For images, video, 3D data\n",
    "\n",
    "**Why you cannot skip this**:\n",
    "- Every transformer needs positional encoding\n",
    "- Wrong choice limits model capability\n",
    "- Understanding position encoding is key to understanding transformers\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "You've mastered positional encoding! Next:\n",
    "- **Complete Transformer Architecture**: Putting attention + position + FFN together\n",
    "- **Advanced variants**: Flash Attention, GQA, Mixture of Experts\n",
    "- **Building LLMs**: From scratch implementations\n",
    "\n",
    "With attention and positional encoding under your belt, you're ready to build complete transformers!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "\n",
    "### Essential Papers\n",
    "1. **Vaswani et al. (2017)**: \"Attention is All You Need\" (Original sinusoidal PE)\n",
    "2. **Devlin et al. (2018)**: \"BERT\" (Learned positional embeddings)\n",
    "3. **Su et al. (2021)**: \"RoFormer: Enhanced Transformer with Rotary Position Embedding\" (RoPE)\n",
    "4. **Press et al. (2021)**: \"Train Short, Test Long: Attention with Linear Biases\" (ALiBi)\n",
    "\n",
    "### Modern Perspectives\n",
    "5. **Chen et al. (2023)**: \"Extending Context Length in LLMs\" (RoPE extensions)\n",
    "6. **Dosovitskiy et al. (2020)**: \"An Image is Worth 16x16 Words\" (2D positional encoding for ViT)\n",
    "\n",
    "### Tutorials and Analysis\n",
    "7. **Positional Encoding Analysis** (Kazemnejad): https://kazemnejad.com/blog/transformer_architecture_positional_encoding/\n",
    "8. **Understanding RoPE** (EleutherAI): https://blog.eleuther.ai/rotary-embeddings/\n",
    "\n",
    "### Implementation Resources\n",
    "- **LLaMA source code**: Reference RoPE implementation\n",
    "- **Hugging Face Transformers**: Multiple positional encoding implementations\n",
    "- **PyTorch examples**: Official transformer tutorials"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
