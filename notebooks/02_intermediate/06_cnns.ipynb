{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic 6: Convolutional Neural Networks (CNNs)\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Understand **why** CNNs were invented and what problem they solve\n",
    "- Learn how convolutions exploit spatial structure in images\n",
    "- Build CNNs from scratch using PyTorch\n",
    "- Understand pooling, stride, padding, and their purposes\n",
    "- Recognize CNN architectures and their evolution\n",
    "- Connect CNNs to the broader deep learning ecosystem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Big Picture: Why CNNs?\n",
    "\n",
    "### The Problem CNNs Solve\n",
    "\n",
    "Imagine you want to build a classifier for 224x224 RGB images using a fully-connected neural network:\n",
    "\n",
    "- **Input size**: 224 × 224 × 3 = 150,528 pixels\n",
    "- **First hidden layer** (1,000 neurons): 150,528 × 1,000 = **150 million parameters**\n",
    "- That's just the first layer!\n",
    "\n",
    "**Problems**:\n",
    "1. **Too many parameters**: Computationally expensive, prone to overfitting\n",
    "2. **Ignores spatial structure**: Treats pixels independently, doesn't understand that nearby pixels are related\n",
    "3. **Not translation invariant**: A cat in the top-left corner looks completely different from a cat in the bottom-right\n",
    "\n",
    "### The CNN Solution\n",
    "\n",
    "CNNs solve these problems by:\n",
    "1. **Local connectivity**: Each neuron only looks at a small region (receptive field)\n",
    "2. **Parameter sharing**: Same weights are reused across the entire image\n",
    "3. **Hierarchy of features**: Early layers detect edges, later layers detect complex patterns\n",
    "\n",
    "**Real-world impact**: CNNs power:\n",
    "- Computer vision (object detection, segmentation, face recognition)\n",
    "- Medical imaging (tumor detection, X-ray analysis)\n",
    "- Self-driving cars (lane detection, obstacle recognition)\n",
    "- Even parts of modern vision transformers (ViT uses CNN-like tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Convolutions: The Core Operation\n",
    "\n",
    "### What is a Convolution?\n",
    "\n",
    "A **convolution** is a mathematical operation that slides a small matrix (kernel/filter) over an image and computes element-wise products and sums.\n",
    "\n",
    "**Why this works**:\n",
    "- **Local patterns matter**: Edges, textures, and shapes are local phenomena\n",
    "- **Translation invariance**: The same kernel detects the same feature anywhere in the image\n",
    "- **Parameter efficiency**: A 3×3 kernel has only 9 parameters, but can process an entire image\n",
    "\n",
    "### Convolution Step-by-Step\n",
    "\n",
    "Let's see how a simple edge detection kernel works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple image (8x8) with a vertical edge\n",
    "image = torch.zeros(1, 1, 8, 8)  # batch=1, channels=1, height=8, width=8\n",
    "image[:, :, :, 4:] = 1.0  # Right half is white\n",
    "\n",
    "# Vertical edge detection kernel\n",
    "# Why this works: Detects transitions from dark to light (or vice versa)\n",
    "kernel = torch.tensor([[\n",
    "    [-1, 0, 1],\n",
    "    [-2, 0, 2],\n",
    "    [-1, 0, 1]\n",
    "]], dtype=torch.float32).view(1, 1, 3, 3)\n",
    "\n",
    "# Apply convolution\n",
    "output = F.conv2d(image, kernel, padding=1)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "axes[0].imshow(image[0, 0], cmap='gray')\n",
    "axes[0].set_title('Original Image')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(kernel[0, 0], cmap='gray')\n",
    "axes[1].set_title('Edge Detection Kernel')\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].imshow(output[0, 0].detach(), cmap='gray')\n",
    "axes[2].set_title('Detected Edges')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice how the edge is highlighted in the output!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution Parameters: Padding, Stride, and Dilation\n",
    "\n",
    "### 1. Padding\n",
    "\n",
    "**What**: Adding extra pixels around the image border\n",
    "\n",
    "**Why it's needed**:\n",
    "- **Preserve spatial dimensions**: Without padding, each conv layer shrinks the image\n",
    "- **Use edge information**: Edge pixels are processed fewer times without padding\n",
    "\n",
    "**Common choices**:\n",
    "- `padding=0`: Valid convolution (shrinks output)\n",
    "- `padding=kernel_size//2`: Same convolution (preserves dimensions)\n",
    "\n",
    "### 2. Stride\n",
    "\n",
    "**What**: Number of pixels to skip when sliding the kernel\n",
    "\n",
    "**Why it's used**:\n",
    "- **Reduce spatial dimensions**: Stride > 1 downsamples the image\n",
    "- **Computational efficiency**: Fewer operations than processing every pixel\n",
    "- **Alternative to pooling**: Modern architectures sometimes use strided convs instead of pooling\n",
    "\n",
    "### 3. Dilation\n",
    "\n",
    "**What**: Spacing between kernel elements (gaps in the filter)\n",
    "\n",
    "**Why it's used**:\n",
    "- **Larger receptive field**: See more context without adding parameters\n",
    "- **Multi-scale processing**: Capture patterns at different scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate different padding and stride effects\n",
    "input_tensor = torch.randn(1, 1, 8, 8)\n",
    "\n",
    "# Same padding (output size = input size)\n",
    "conv_same = nn.Conv2d(1, 1, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "out_same = conv_same(input_tensor)\n",
    "\n",
    "# Valid convolution (no padding)\n",
    "conv_valid = nn.Conv2d(1, 1, kernel_size=3, stride=1, padding=0, bias=False)\n",
    "out_valid = conv_valid(input_tensor)\n",
    "\n",
    "# Strided convolution (downsampling)\n",
    "conv_stride = nn.Conv2d(1, 1, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "out_stride = conv_stride(input_tensor)\n",
    "\n",
    "print(f\"Input shape: {input_tensor.shape}\")\n",
    "print(f\"Same padding (padding=1, stride=1): {out_same.shape}\")\n",
    "print(f\"Valid convolution (padding=0, stride=1): {out_valid.shape}\")\n",
    "print(f\"Strided convolution (padding=1, stride=2): {out_stride.shape}\")\n",
    "\n",
    "print(\"\\nNotice:\")\n",
    "print(\"- Same padding preserves dimensions: 8x8 -> 8x8\")\n",
    "print(\"- Valid conv shrinks: 8x8 -> 6x6 (lost 2 pixels per dimension)\")\n",
    "print(\"- Stride=2 downsamples: 8x8 -> 4x4 (halved dimensions)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling Layers: Why Downsample?\n",
    "\n",
    "### Purpose of Pooling\n",
    "\n",
    "Pooling reduces spatial dimensions while preserving important features.\n",
    "\n",
    "**Why pooling is needed**:\n",
    "1. **Reduce computation**: Fewer pixels = fewer operations in later layers\n",
    "2. **Increase receptive field**: Each neuron sees a larger portion of the original image\n",
    "3. **Translation invariance**: Small shifts in input don't change output\n",
    "4. **Prevent overfitting**: Acts as a form of regularization\n",
    "\n",
    "### Types of Pooling\n",
    "\n",
    "**Max Pooling**: Takes maximum value in each region\n",
    "- **Why**: Preserves strongest activations (most prominent features)\n",
    "- **Common choice**: 2×2 with stride 2 (halves dimensions)\n",
    "\n",
    "**Average Pooling**: Takes average of values in each region\n",
    "- **Why**: Smoother downsampling, often used in final layers\n",
    "\n",
    "**Global Average Pooling**: Averages each feature map to a single value\n",
    "- **Why**: Replaces fully-connected layers, reduces parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate pooling operations\n",
    "feature_map = torch.randn(1, 1, 8, 8)\n",
    "\n",
    "# Max pooling (2x2, stride 2)\n",
    "max_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "out_max = max_pool(feature_map)\n",
    "\n",
    "# Average pooling (2x2, stride 2)\n",
    "avg_pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "out_avg = avg_pool(feature_map)\n",
    "\n",
    "# Global average pooling\n",
    "gap = nn.AdaptiveAvgPool2d((1, 1))\n",
    "out_gap = gap(feature_map)\n",
    "\n",
    "print(f\"Original shape: {feature_map.shape}\")\n",
    "print(f\"After max pooling: {out_max.shape}\")\n",
    "print(f\"After average pooling: {out_avg.shape}\")\n",
    "print(f\"After global average pooling: {out_gap.shape}\")\n",
    "\n",
    "print(\"\\nGlobal Average Pooling (GAP) is especially useful:\")\n",
    "print(\"- Replaces flatten + fully-connected layers\")\n",
    "print(\"- Works with any input size\")\n",
    "print(\"- Reduces overfitting (fewer parameters)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Simple CNN from Scratch\n",
    "\n",
    "Let's build a classic CNN architecture for CIFAR-10 classification.\n",
    "\n",
    "### Architecture Design Principles\n",
    "\n",
    "**Why this structure**:\n",
    "1. **Progressive feature extraction**: Early layers detect simple patterns (edges), later layers detect complex patterns (objects)\n",
    "2. **Increase channels, decrease spatial dimensions**: Trade resolution for feature richness\n",
    "3. **Batch normalization**: Stabilizes training, allows higher learning rates\n",
    "4. **Dropout**: Prevents overfitting by randomly dropping neurons\n",
    "\n",
    "**Common pattern**:\n",
    "```\n",
    "Conv -> BatchNorm -> ReLU -> Pool -> \n",
    "Conv -> BatchNorm -> ReLU -> Pool -> \n",
    "... -> \n",
    "Global Average Pool -> Classifier\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple but effective CNN for CIFAR-10.\n",
    "    \n",
    "    Architecture:\n",
    "    - 3 convolutional blocks (conv -> bn -> relu -> pool)\n",
    "    - Each block doubles the channels and halves spatial dimensions\n",
    "    - Global average pooling to reduce parameters\n",
    "    - Single fully-connected layer for classification\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        \n",
    "        # Why these channel sizes? \n",
    "        # Start small (32), gradually increase (64, 128) to capture complexity\n",
    "        \n",
    "        # Block 1: 32x32x3 -> 16x16x32\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)  # Stabilizes training\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)  # Downsample by 2\n",
    "        \n",
    "        # Block 2: 16x16x32 -> 8x8x64\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Block 3: 8x8x64 -> 4x4x128\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Global average pooling: 4x4x128 -> 1x1x128\n",
    "        self.gap = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # Classifier\n",
    "        self.dropout = nn.Dropout(0.5)  # Regularization\n",
    "        self.fc = nn.Linear(128, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Block 1\n",
    "        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
    "        \n",
    "        # Block 2\n",
    "        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
    "        \n",
    "        # Block 3\n",
    "        x = self.pool3(F.relu(self.bn3(self.conv3(x))))\n",
    "        \n",
    "        # Global pooling and classification\n",
    "        x = self.gap(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Create model and inspect\n",
    "model = SimpleCNN(num_classes=10)\n",
    "print(model)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# Test forward pass\n",
    "dummy_input = torch.randn(4, 3, 32, 32)  # Batch of 4 CIFAR-10 images\n",
    "output = model(dummy_input)\n",
    "print(f\"\\nInput shape: {dummy_input.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(\"Output represents logits for 10 classes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the CNN on CIFAR-10\n",
    "\n",
    "Let's train our CNN on a real dataset to see it in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation\n",
    "# Why these transforms?\n",
    "# - Normalization: Helps optimization by centering data around 0\n",
    "# - Data augmentation (train only): Improves generalization\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),  # Augmentation\n",
    "    transforms.RandomCrop(32, padding=4),  # Augmentation\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Load CIFAR-10\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=128,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "print(f\"Training samples: {len(trainset)}\")\n",
    "print(f\"Test samples: {len(testset)}\")\n",
    "print(f\"Classes: {classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for inputs, labels in loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    return running_loss / len(loader), 100. * correct / total\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    return running_loss / len(loader), 100. * correct / total\n",
    "\n",
    "# Setup training\n",
    "device = torch.device('cpu')  # Use CPU for this demo\n",
    "model = SimpleCNN(num_classes=10).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train for a few epochs (use more for better results)\n",
    "num_epochs = 3\n",
    "\n",
    "print(\"Training started...\\n\")\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train_one_epoch(model, trainloader, criterion, optimizer, device)\n",
    "    test_loss, test_acc = evaluate(model, testloader, criterion, device)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"  Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.2f}%\")\n",
    "\n",
    "print(\"\\nTraining complete!\")\n",
    "print(f\"Final test accuracy: {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing What CNNs Learn\n",
    "\n",
    "Let's visualize the learned filters to understand what features the CNN detects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize first layer filters\n",
    "def visualize_filters(model):\n",
    "    # Get first conv layer weights\n",
    "    weights = model.conv1.weight.data.cpu()\n",
    "    \n",
    "    # Normalize to [0, 1] for visualization\n",
    "    weights = (weights - weights.min()) / (weights.max() - weights.min())\n",
    "    \n",
    "    # Plot first 16 filters\n",
    "    fig, axes = plt.subplots(4, 8, figsize=(12, 6))\n",
    "    for idx, ax in enumerate(axes.flat):\n",
    "        if idx < weights.shape[0]:\n",
    "            # Convert RGB channels to grayscale for visualization\n",
    "            filter_img = weights[idx].mean(0)\n",
    "            ax.imshow(filter_img, cmap='viridis')\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.suptitle('First Layer Filters (32 total, showing first 32)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_filters(model)\n",
    "\n",
    "print(\"These filters learned to detect basic patterns:\")\n",
    "print(\"- Edges at different orientations\")\n",
    "print(\"- Color gradients\")\n",
    "print(\"- Simple textures\")\n",
    "print(\"\\nDeeper layers combine these to detect more complex features!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modern CNN Architectures: Evolution and Design Patterns\n",
    "\n",
    "### Historical Evolution\n",
    "\n",
    "**1. LeNet-5 (1998)**: First successful CNN\n",
    "- **Why important**: Proved CNNs work for digit recognition\n",
    "- **Architecture**: Conv -> Pool -> Conv -> Pool -> FC\n",
    "\n",
    "**2. AlexNet (2012)**: ImageNet breakthrough\n",
    "- **Why revolutionary**: Won ImageNet by huge margin, revived deep learning\n",
    "- **Innovations**: ReLU, Dropout, GPU training, data augmentation\n",
    "\n",
    "**3. VGG (2014)**: Simplicity and depth\n",
    "- **Why influential**: Showed deeper is better (16-19 layers)\n",
    "- **Design principle**: Small 3×3 filters, consistent architecture\n",
    "\n",
    "**4. ResNet (2015)**: Residual connections\n",
    "- **Why critical**: Solved vanishing gradients, enabled 100+ layer networks\n",
    "- **Innovation**: Skip connections (x + F(x))\n",
    "\n",
    "**5. EfficientNet (2019)**: Compound scaling\n",
    "- **Why modern**: Balanced depth, width, and resolution\n",
    "- **Innovation**: Neural architecture search (NAS)\n",
    "\n",
    "### Key Design Patterns\n",
    "\n",
    "**Residual Blocks** (ResNet, ResNeXt):\n",
    "```python\n",
    "# Skip connection allows gradients to flow directly\n",
    "out = F.relu(x + conv_block(x))\n",
    "```\n",
    "\n",
    "**Inception Modules** (GoogLeNet):\n",
    "- Multiple filter sizes in parallel\n",
    "- Captures multi-scale features\n",
    "\n",
    "**Depthwise Separable Convolutions** (MobileNet):\n",
    "- Factorize convolution into depthwise + pointwise\n",
    "- Dramatically fewer parameters\n",
    "\n",
    "**Squeeze-and-Excitation** (SENet):\n",
    "- Channel-wise attention mechanism\n",
    "- Reweights feature maps by importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Simple Residual Block\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic residual block with skip connection.\n",
    "    \n",
    "    Why skip connections work:\n",
    "    - Gradient flows directly through skip connection\n",
    "    - Network can learn identity function (do nothing) if needed\n",
    "    - Easier to optimize very deep networks\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        # If dimensions change, use 1x1 conv to match\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1,\n",
    "                          stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        identity = self.shortcut(x)\n",
    "        \n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        \n",
    "        # Add skip connection BEFORE activation\n",
    "        out += identity\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# Test residual block\n",
    "res_block = ResidualBlock(64, 128, stride=2)\n",
    "test_input = torch.randn(1, 64, 32, 32)\n",
    "test_output = res_block(test_input)\n",
    "\n",
    "print(f\"Input shape: {test_input.shape}\")\n",
    "print(f\"Output shape: {test_output.shape}\")\n",
    "print(\"\\nNotice: Channels doubled (64->128) and spatial dimensions halved (32->16)\")\n",
    "print(\"The skip connection was automatically adjusted to match dimensions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connections to Modern Deep Learning\n",
    "\n",
    "### CNNs in the Age of Transformers\n",
    "\n",
    "**Why CNNs are still relevant**:\n",
    "\n",
    "1. **Vision Transformers (ViT)**: Use CNN-like patch embeddings\n",
    "   - Input image is divided into patches (like conv kernels)\n",
    "   - Some architectures combine CNNs with transformers\n",
    "\n",
    "2. **Hybrid architectures**: \n",
    "   - ConvNeXt (2022): Modernized ResNet matches ViT performance\n",
    "   - Combines CNN efficiency with transformer-inspired design\n",
    "\n",
    "3. **Efficiency**: CNNs are faster and more parameter-efficient for many vision tasks\n",
    "\n",
    "4. **Inductive biases**: CNNs have built-in assumptions (locality, translation invariance) that help on images\n",
    "\n",
    "### Key Concepts to Remember\n",
    "\n",
    "**Why CNNs work**:\n",
    "- Exploit spatial structure through local connectivity\n",
    "- Parameter sharing reduces overfitting\n",
    "- Hierarchical feature learning (edges -> textures -> objects)\n",
    "\n",
    "**When to use CNNs**:\n",
    "- Image classification, detection, segmentation\n",
    "- Any spatial data (medical images, satellite imagery)\n",
    "- When efficiency matters (mobile devices)\n",
    "\n",
    "**Connection to transformers**:\n",
    "- Both build hierarchical representations\n",
    "- Transformers replace local connectivity with attention\n",
    "- Hybrid approaches combine strengths of both"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini Exercises\n",
    "\n",
    "Test your understanding with these exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Calculate Output Dimensions\n",
    "\n",
    "Given an input of size 64×64 and a conv layer with:\n",
    "- kernel_size=5\n",
    "- stride=2\n",
    "- padding=2\n",
    "\n",
    "What is the output size?\n",
    "\n",
    "Formula: output_size = (input_size + 2×padding - kernel_size) / stride + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Calculate the output size\n",
    "\n",
    "\n",
    "# SOLUTION (run to reveal)\n",
    "def show_solution_1():\n",
    "    input_size = 64\n",
    "    kernel_size = 5\n",
    "    stride = 2\n",
    "    padding = 2\n",
    "    \n",
    "    output_size = (input_size + 2*padding - kernel_size) // stride + 1\n",
    "    print(f\"Output size: {output_size}\")\n",
    "    print(f\"\\nCalculation: (64 + 2*2 - 5) / 2 + 1 = 63/2 + 1 = 32\")\n",
    "    \n",
    "    # Verify with PyTorch\n",
    "    conv = nn.Conv2d(3, 16, kernel_size=5, stride=2, padding=2)\n",
    "    x = torch.randn(1, 3, 64, 64)\n",
    "    out = conv(x)\n",
    "    print(f\"PyTorch verification: {out.shape}\")\n",
    "\n",
    "# Uncomment to see solution:\n",
    "# show_solution_1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Design a Deeper CNN\n",
    "\n",
    "Modify `SimpleCNN` to have 4 convolutional blocks instead of 3. Make sure to:\n",
    "- Follow the pattern of doubling channels each block\n",
    "- Use batch normalization\n",
    "- Maintain the same final classifier structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "class DeeperCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(DeeperCNN, self).__init__()\n",
    "        # Add your code here\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Add your code here\n",
    "        pass\n",
    "\n",
    "\n",
    "# SOLUTION (run to reveal)\n",
    "def show_solution_2():\n",
    "    class DeeperCNN(nn.Module):\n",
    "        def __init__(self, num_classes=10):\n",
    "            super(DeeperCNN, self).__init__()\n",
    "            \n",
    "            # Block 1: 32x32 -> 16x16, channels: 3 -> 32\n",
    "            self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "            self.bn1 = nn.BatchNorm2d(32)\n",
    "            self.pool1 = nn.MaxPool2d(2, 2)\n",
    "            \n",
    "            # Block 2: 16x16 -> 8x8, channels: 32 -> 64\n",
    "            self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "            self.bn2 = nn.BatchNorm2d(64)\n",
    "            self.pool2 = nn.MaxPool2d(2, 2)\n",
    "            \n",
    "            # Block 3: 8x8 -> 4x4, channels: 64 -> 128\n",
    "            self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "            self.bn3 = nn.BatchNorm2d(128)\n",
    "            self.pool3 = nn.MaxPool2d(2, 2)\n",
    "            \n",
    "            # Block 4: 4x4 -> 2x2, channels: 128 -> 256\n",
    "            self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "            self.bn4 = nn.BatchNorm2d(256)\n",
    "            self.pool4 = nn.MaxPool2d(2, 2)\n",
    "            \n",
    "            # Global average pooling and classifier\n",
    "            self.gap = nn.AdaptiveAvgPool2d((1, 1))\n",
    "            self.dropout = nn.Dropout(0.5)\n",
    "            self.fc = nn.Linear(256, num_classes)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
    "            x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
    "            x = self.pool3(F.relu(self.bn3(self.conv3(x))))\n",
    "            x = self.pool4(F.relu(self.bn4(self.conv4(x))))\n",
    "            \n",
    "            x = self.gap(x)\n",
    "            x = x.view(x.size(0), -1)\n",
    "            x = self.dropout(x)\n",
    "            x = self.fc(x)\n",
    "            return x\n",
    "    \n",
    "    model = DeeperCNN()\n",
    "    print(model)\n",
    "    print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Uncomment to see solution:\n",
    "# show_solution_2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Implement Depthwise Separable Convolution\n",
    "\n",
    "Depthwise separable convolution is used in MobileNet for efficiency. It consists of:\n",
    "1. **Depthwise conv**: Apply separate filters to each input channel\n",
    "2. **Pointwise conv**: 1×1 conv to combine channels\n",
    "\n",
    "Why is this more efficient? Count the parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "class DepthwiseSeparableConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "        super(DepthwiseSeparableConv, self).__init__()\n",
    "        # Add your code here\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Add your code here\n",
    "        pass\n",
    "\n",
    "\n",
    "# SOLUTION (run to reveal)\n",
    "def show_solution_3():\n",
    "    class DepthwiseSeparableConv(nn.Module):\n",
    "        def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "            super(DepthwiseSeparableConv, self).__init__()\n",
    "            \n",
    "            # Depthwise: Apply 3x3 filter to EACH input channel separately\n",
    "            # groups=in_channels means each input channel gets its own filter\n",
    "            self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size=kernel_size,\n",
    "                                       stride=stride, padding=padding, groups=in_channels,\n",
    "                                       bias=False)\n",
    "            self.bn1 = nn.BatchNorm2d(in_channels)\n",
    "            \n",
    "            # Pointwise: 1x1 conv to combine channels\n",
    "            self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1,\n",
    "                                       bias=False)\n",
    "            self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            x = F.relu(self.bn1(self.depthwise(x)))\n",
    "            x = F.relu(self.bn2(self.pointwise(x)))\n",
    "            return x\n",
    "    \n",
    "    # Compare parameter count\n",
    "    in_ch, out_ch = 64, 128\n",
    "    \n",
    "    # Standard conv\n",
    "    standard_conv = nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1)\n",
    "    standard_params = sum(p.numel() for p in standard_conv.parameters())\n",
    "    \n",
    "    # Depthwise separable conv\n",
    "    dw_conv = DepthwiseSeparableConv(in_ch, out_ch)\n",
    "    dw_params = sum(p.numel() for p in dw_conv.parameters())\n",
    "    \n",
    "    print(f\"Standard Conv2d parameters: {standard_params:,}\")\n",
    "    print(f\"Depthwise Separable parameters: {dw_params:,}\")\n",
    "    print(f\"\\nReduction: {standard_params / dw_params:.2f}x fewer parameters!\")\n",
    "    print(\"\\nWhy this matters: Mobile devices have limited compute/memory\")\n",
    "\n",
    "# Uncomment to see solution:\n",
    "# show_solution_3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprehensive Exercise: Build a Modern CNN\n",
    "\n",
    "Build a CNN that combines modern techniques:\n",
    "1. Residual connections (like ResNet)\n",
    "2. Batch normalization\n",
    "3. Global average pooling\n",
    "4. Data augmentation\n",
    "\n",
    "Train it on CIFAR-10 and achieve >75% test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Build and train your modern CNN\n",
    "\n",
    "\n",
    "# SOLUTION (run to reveal)\n",
    "def show_comprehensive_solution():\n",
    "    class ModernCNN(nn.Module):\n",
    "        def __init__(self, num_classes=10):\n",
    "            super(ModernCNN, self).__init__()\n",
    "            \n",
    "            # Initial conv\n",
    "            self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1, bias=False)\n",
    "            self.bn1 = nn.BatchNorm2d(64)\n",
    "            \n",
    "            # Residual blocks\n",
    "            self.layer1 = self._make_layer(64, 64, num_blocks=2, stride=1)\n",
    "            self.layer2 = self._make_layer(64, 128, num_blocks=2, stride=2)\n",
    "            self.layer3 = self._make_layer(128, 256, num_blocks=2, stride=2)\n",
    "            \n",
    "            # Global average pooling and classifier\n",
    "            self.gap = nn.AdaptiveAvgPool2d((1, 1))\n",
    "            self.fc = nn.Linear(256, num_classes)\n",
    "        \n",
    "        def _make_layer(self, in_channels, out_channels, num_blocks, stride):\n",
    "            layers = []\n",
    "            # First block may downsample\n",
    "            layers.append(ResidualBlock(in_channels, out_channels, stride))\n",
    "            # Remaining blocks maintain dimensions\n",
    "            for _ in range(1, num_blocks):\n",
    "                layers.append(ResidualBlock(out_channels, out_channels, stride=1))\n",
    "            return nn.Sequential(*layers)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            x = F.relu(self.bn1(self.conv1(x)))\n",
    "            \n",
    "            x = self.layer1(x)\n",
    "            x = self.layer2(x)\n",
    "            x = self.layer3(x)\n",
    "            \n",
    "            x = self.gap(x)\n",
    "            x = x.view(x.size(0), -1)\n",
    "            x = self.fc(x)\n",
    "            return x\n",
    "    \n",
    "    # Training code\n",
    "    device = torch.device('cpu')\n",
    "    model = ModernCNN(num_classes=10).to(device)\n",
    "    \n",
    "    print(\"Modern CNN Architecture:\")\n",
    "    print(model)\n",
    "    print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    # Setup training (would need more epochs for >75% accuracy)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "    \n",
    "    print(\"\\nThis architecture combines:\")\n",
    "    print(\"✓ Residual connections for deep training\")\n",
    "    print(\"✓ Batch normalization for stable optimization\")\n",
    "    print(\"✓ Global average pooling for fewer parameters\")\n",
    "    print(\"✓ Cosine annealing scheduler for better convergence\")\n",
    "\n",
    "# Uncomment to see solution:\n",
    "# show_comprehensive_solution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### Core Concepts\n",
    "\n",
    "1. **CNNs exploit spatial structure**:\n",
    "   - Local connectivity reduces parameters\n",
    "   - Parameter sharing enables translation invariance\n",
    "   - Hierarchical features (edges → objects)\n",
    "\n",
    "2. **Key components and their purposes**:\n",
    "   - **Convolution**: Feature extraction with parameter sharing\n",
    "   - **Pooling**: Downsampling and translation invariance\n",
    "   - **Batch Norm**: Stable training and faster convergence\n",
    "   - **Residual connections**: Enable very deep networks\n",
    "\n",
    "3. **Design principles**:\n",
    "   - Increase channels while decreasing spatial dimensions\n",
    "   - Use small filters (3×3) with multiple layers\n",
    "   - Add skip connections for depth\n",
    "   - Use global average pooling to reduce overfitting\n",
    "\n",
    "4. **Modern relevance**:\n",
    "   - Still state-of-the-art for many vision tasks\n",
    "   - More efficient than transformers on smaller datasets\n",
    "   - Hybrid CNN-transformer architectures are emerging\n",
    "   - Principles apply to any spatial data\n",
    "\n",
    "### Connection to Broader Ecosystem\n",
    "\n",
    "**CNNs → Vision Transformers**:\n",
    "- ViT uses patch embeddings (similar to conv kernels)\n",
    "- Some architectures combine CNNs and attention\n",
    "\n",
    "**CNNs → Modern Architectures**:\n",
    "- Residual connections appear everywhere (Transformers use them too)\n",
    "- Batch normalization evolved into Layer Norm\n",
    "- Attention mechanisms can be seen as dynamic convolution\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "You've mastered CNNs! Next topics:\n",
    "- **Attention mechanisms**: The foundation of transformers\n",
    "- **Positional encodings**: How to inject sequence information\n",
    "- **Transformer architecture**: The revolution in deep learning\n",
    "\n",
    "CNNs taught us about hierarchical feature learning and parameter sharing—concepts that carry forward into transformers and beyond!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "\n",
    "### Classic Papers\n",
    "1. **LeCun et al. (1998)**: \"Gradient-Based Learning Applied to Document Recognition\" (LeNet)\n",
    "2. **Krizhevsky et al. (2012)**: \"ImageNet Classification with Deep Convolutional Neural Networks\" (AlexNet)\n",
    "3. **He et al. (2015)**: \"Deep Residual Learning for Image Recognition\" (ResNet)\n",
    "4. **Tan & Le (2019)**: \"EfficientNet: Rethinking Model Scaling for CNNs\"\n",
    "\n",
    "### Modern Perspectives\n",
    "5. **Liu et al. (2022)**: \"A ConvNet for the 2020s\" (ConvNeXt)\n",
    "6. **Dosovitskiy et al. (2020)**: \"An Image is Worth 16x16 Words\" (Vision Transformer)\n",
    "\n",
    "### Resources\n",
    "- **CS231n**: Stanford's CNN course (http://cs231n.stanford.edu/)\n",
    "- **PyTorch Vision Models**: torchvision.models for pre-trained architectures\n",
    "- **Papers With Code**: Compare architectures and benchmarks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
