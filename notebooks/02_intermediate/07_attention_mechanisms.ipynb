{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic 7: Attention Mechanisms - A Comprehensive Guide\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Understand **why** attention was invented and what problem it solves\n",
    "- Learn the intuition behind Queries, Keys, and Values\n",
    "- Build self-attention from scratch, step by step\n",
    "- Understand cross-attention and when to use it\n",
    "- Implement multi-head attention from first principles\n",
    "- Use PyTorch's optimized attention functions\n",
    "- Connect attention to transformers and modern LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Big Picture: Why Attention?\n",
    "\n",
    "### The Problem: Fixed-Length Bottleneck\n",
    "\n",
    "Before attention, sequence-to-sequence models (like machine translation) used an **encoder-decoder** architecture:\n",
    "\n",
    "```\n",
    "Input Sequence → Encoder → Fixed-Size Vector → Decoder → Output Sequence\n",
    "```\n",
    "\n",
    "**The critical flaw**: All input information must compress into a single fixed-size vector!\n",
    "\n",
    "**Real-world example** (English to French translation):\n",
    "```\n",
    "Input: \"The cat sat on the mat because it was comfortable\"\n",
    "Problem: When translating \"it\", the decoder has lost the context\n",
    "         that \"it\" refers to \"the mat\", not \"the cat\"\n",
    "```\n",
    "\n",
    "**Why this fails**:\n",
    "- **Information loss**: Long sequences can't fit in fixed vector\n",
    "- **No selectivity**: Decoder can't focus on relevant parts of input\n",
    "- **Gradient problems**: Backprop through long sequences is unstable\n",
    "\n",
    "### The Attention Solution\n",
    "\n",
    "**Key insight**: Instead of a fixed vector, let the decoder **attend to the entire input sequence** and dynamically focus on relevant parts!\n",
    "\n",
    "```\n",
    "When translating \"it\":\n",
    "↓\n",
    "Look at all input words with learned weights:\n",
    "  \"The\"    (0.02)\n",
    "  \"cat\"    (0.05)\n",
    "  \"sat\"    (0.03)\n",
    "  \"on\"     (0.04)\n",
    "  \"the\"    (0.08)\n",
    "  \"mat\"    (0.65)  ← High attention!\n",
    "  \"because\"(0.05)\n",
    "  \"it\"     (0.03)\n",
    "  \"was\"    (0.03)\n",
    "  \"comfortable\" (0.02)\n",
    "```\n",
    "\n",
    "**Impact**: Attention revolutionized NLP and enabled:\n",
    "- Transformers (GPT, BERT, LLaMA)\n",
    "- Vision transformers (ViT)\n",
    "- Multimodal models (CLIP, Flamingo)\n",
    "- State-of-the-art results across all of AI\n",
    "\n",
    "**Why it cannot be skipped**: Attention is the foundation of modern deep learning. Understanding it deeply is essential for working with any state-of-the-art model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Set up visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Queries, Keys, and Values\n",
    "\n",
    "### The Search Engine Analogy\n",
    "\n",
    "Think of attention like a search engine:\n",
    "\n",
    "**1. Query (Q)**: What you're searching for\n",
    "- \"Best Italian restaurants in Manhattan\"\n",
    "- Represents what information you need\n",
    "\n",
    "**2. Key (K)**: Indexed titles/tags of documents\n",
    "- \"Italian cuisine in NYC\", \"Manhattan dining guide\", \"Best pizza in Brooklyn\"\n",
    "- Represents what information is available\n",
    "\n",
    "**3. Value (V)**: The actual document content\n",
    "- Full restaurant reviews, menus, addresses\n",
    "- Represents the information to retrieve\n",
    "\n",
    "**The attention process**:\n",
    "1. **Compare Query with Keys**: Calculate relevance scores (which documents match your search?)\n",
    "2. **Softmax**: Convert scores to probabilities (how much to focus on each document?)\n",
    "3. **Weighted sum of Values**: Get a blend of most relevant content\n",
    "\n",
    "### Why This Design?\n",
    "\n",
    "**Separation of concerns**:\n",
    "- **Keys**: What can be attended to (searchable index)\n",
    "- **Values**: What information is retrieved (actual content)\n",
    "- **Queries**: What is being looked for (current context)\n",
    "\n",
    "**Flexibility**:\n",
    "- Same Keys/Values, different Queries → different focus\n",
    "- Learned projections allow the model to transform inputs into optimal Q, K, V representations\n",
    "\n",
    "**Why it's needed**: Without separate Q, K, V, the model can't learn what to search for vs. what to retrieve. This separation is crucial for flexible, context-dependent information flow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Attention: Step-by-Step from Scratch\n",
    "\n",
    "### What is Self-Attention?\n",
    "\n",
    "**Self-attention** means the input sequence attends to itself:\n",
    "- Queries, Keys, and Values all come from the same source\n",
    "- Each position looks at all positions (including itself)\n",
    "- Captures relationships within a single sequence\n",
    "\n",
    "**Example**: In the sentence \"The cat sat on the mat\"\n",
    "- When processing \"sat\", attend to \"cat\" (subject) and \"mat\" (location)\n",
    "- When processing \"it\", attend to \"cat\" or \"mat\" (pronoun resolution)\n",
    "\n",
    "### The Scaled Dot-Product Attention Formula\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "**Breaking it down**:\n",
    "1. $QK^T$: Compute similarity between queries and keys (dot product)\n",
    "2. $\\frac{1}{\\sqrt{d_k}}$: Scale by sqrt of dimension (prevents huge values)\n",
    "3. $\\text{softmax}$: Convert to probabilities (sums to 1)\n",
    "4. Multiply by $V$: Weighted combination of values\n",
    "\n",
    "**Why each step is needed**:\n",
    "- **Dot product**: Measures alignment between vectors (high value = similar)\n",
    "- **Scaling**: Without it, softmax saturates (all weight on one position)\n",
    "- **Softmax**: Normalizes scores, allows gradient flow to all positions\n",
    "- **Multiply V**: Actually retrieve and blend information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    Compute scaled dot-product attention from scratch.\n",
    "    \n",
    "    Args:\n",
    "        Q: Queries (batch_size, num_heads, seq_len, d_k)\n",
    "        K: Keys (batch_size, num_heads, seq_len, d_k)\n",
    "        V: Values (batch_size, num_heads, seq_len, d_v)\n",
    "        mask: Optional mask (batch_size, 1, seq_len, seq_len)\n",
    "    \n",
    "    Returns:\n",
    "        output: Attention output (batch_size, num_heads, seq_len, d_v)\n",
    "        attention_weights: Attention probabilities (batch_size, num_heads, seq_len, seq_len)\n",
    "    \"\"\"\n",
    "    # Step 1: Get dimension of keys (for scaling)\n",
    "    d_k = K.size(-1)\n",
    "    \n",
    "    # Step 2: Compute attention scores (Q * K^T)\n",
    "    # Why dot product? It measures similarity between query and key vectors\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1))  # (batch, heads, seq_len, seq_len)\n",
    "    \n",
    "    # Step 3: Scale by sqrt(d_k)\n",
    "    # Why? Prevents dot products from becoming too large (which makes softmax peak)\n",
    "    scores = scores / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
    "    \n",
    "    # Step 4: Apply mask (if provided)\n",
    "    # Why? To prevent attending to certain positions (e.g., future tokens in causal attention)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "    \n",
    "    # Step 5: Apply softmax to get attention probabilities\n",
    "    # Why? Converts scores to probabilities that sum to 1\n",
    "    attention_weights = F.softmax(scores, dim=-1)  # (batch, heads, seq_len, seq_len)\n",
    "    \n",
    "    # Step 6: Weighted sum of values\n",
    "    # Why? This is where we actually retrieve and blend information\n",
    "    output = torch.matmul(attention_weights, V)  # (batch, heads, seq_len, d_v)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "print(\"Scaled dot-product attention function defined!\")\n",
    "print(\"\\nThis is the CORE of all attention mechanisms.\")\n",
    "print(\"Every transformer, every LLM uses this exact computation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's See Attention in Action: A Simple Example\n",
    "\n",
    "We'll create a small sequence and visualize how attention weights are computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple example: 5-token sequence with 8-dimensional embeddings\n",
    "seq_len = 5\n",
    "d_model = 8\n",
    "\n",
    "# For this example, Q=K=V (self-attention)\n",
    "X = torch.randn(1, 1, seq_len, d_model)  # (batch=1, heads=1, seq_len=5, d_model=8)\n",
    "\n",
    "# In self-attention, Q, K, V are projections of the same input\n",
    "# For simplicity, we'll just use X directly\n",
    "Q = K = V = X\n",
    "\n",
    "# Compute attention\n",
    "output, attention_weights = scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "# Visualize attention weights\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(attention_weights[0, 0].detach().numpy(), \n",
    "            annot=True, fmt='.3f', cmap='YlOrRd',\n",
    "            xticklabels=[f'Token {i}' for i in range(seq_len)],\n",
    "            yticklabels=[f'Token {i}' for i in range(seq_len)],\n",
    "            cbar_kws={'label': 'Attention Weight'})\n",
    "plt.title('Self-Attention Weights\\n(Row i = where token i attends to)')\n",
    "plt.xlabel('Key positions')\n",
    "plt.ylabel('Query positions')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nHow to read this heatmap:\")\n",
    "print(\"- Each row shows where one token attends\")\n",
    "print(\"- Brighter colors = higher attention weight\")\n",
    "print(\"- Each row sums to 1.0 (probability distribution)\")\n",
    "print(\"\\nNotice:\")\n",
    "print(\"- Diagonal (self-attention) is often high\")\n",
    "print(\"- Each token can attend to all other tokens\")\n",
    "print(\"- Weights are learned during training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Complete Self-Attention Layer\n",
    "\n",
    "### Why Do We Need Learned Projections?\n",
    "\n",
    "In practice, we don't use input embeddings directly as Q, K, V. Instead:\n",
    "\n",
    "**1. We project them through learned weight matrices**:\n",
    "```python\n",
    "Q = X @ W_Q  # Learn what to search for\n",
    "K = X @ W_K  # Learn what to index\n",
    "V = X @ W_V  # Learn what to retrieve\n",
    "```\n",
    "\n",
    "**Why this is crucial**:\n",
    "- **Flexibility**: Different transformations for different purposes\n",
    "- **Learning**: Model learns optimal representations for attention\n",
    "- **Expressiveness**: Can attend to different aspects of inputs\n",
    "\n",
    "**2. We add an output projection**:\n",
    "```python\n",
    "output = attention_output @ W_O\n",
    "```\n",
    "\n",
    "**Why it's needed**:\n",
    "- Combines information from attention\n",
    "- Projects back to model dimension\n",
    "- Adds additional learnable transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Self-attention layer with learned Q, K, V projections.\n",
    "    \n",
    "    This is the building block of transformers!\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, d_k, d_v):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: Dimension of input embeddings\n",
    "            d_k: Dimension of queries and keys\n",
    "            d_v: Dimension of values\n",
    "        \"\"\"\n",
    "        super(SelfAttention, self).__init__()\n",
    "        \n",
    "        self.d_k = d_k\n",
    "        \n",
    "        # Learned projection matrices\n",
    "        # Why separate W_Q, W_K, W_V? Each has a different role!\n",
    "        self.W_Q = nn.Linear(d_model, d_k, bias=False)\n",
    "        self.W_K = nn.Linear(d_model, d_k, bias=False)\n",
    "        self.W_V = nn.Linear(d_model, d_v, bias=False)\n",
    "        \n",
    "        # Output projection\n",
    "        # Why needed? Combines attention output back to d_model dimension\n",
    "        self.W_O = nn.Linear(d_v, d_model, bias=False)\n",
    "    \n",
    "    def forward(self, X, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X: Input tensor (batch_size, seq_len, d_model)\n",
    "            mask: Optional attention mask\n",
    "        \n",
    "        Returns:\n",
    "            output: Attention output (batch_size, seq_len, d_model)\n",
    "            attention_weights: Attention probabilities\n",
    "        \"\"\"\n",
    "        # Step 1: Project to Q, K, V\n",
    "        # This is where the model learns WHAT to attend to\n",
    "        Q = self.W_Q(X)  # (batch, seq_len, d_k)\n",
    "        K = self.W_K(X)  # (batch, seq_len, d_k)\n",
    "        V = self.W_V(X)  # (batch, seq_len, d_v)\n",
    "        \n",
    "        # Add head dimension for compatibility with scaled_dot_product_attention\n",
    "        Q = Q.unsqueeze(1)  # (batch, 1, seq_len, d_k)\n",
    "        K = K.unsqueeze(1)\n",
    "        V = V.unsqueeze(1)\n",
    "        \n",
    "        # Step 2: Compute attention\n",
    "        attn_output, attention_weights = scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        # Remove head dimension\n",
    "        attn_output = attn_output.squeeze(1)  # (batch, seq_len, d_v)\n",
    "        \n",
    "        # Step 3: Project output back to d_model\n",
    "        output = self.W_O(attn_output)  # (batch, seq_len, d_model)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "# Create and test the self-attention layer\n",
    "d_model = 64\n",
    "d_k = d_v = 64\n",
    "seq_len = 10\n",
    "batch_size = 2\n",
    "\n",
    "self_attn = SelfAttention(d_model, d_k, d_v)\n",
    "X = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "output, attn_weights = self_attn(X)\n",
    "\n",
    "print(f\"Input shape: {X.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attn_weights.shape}\")\n",
    "print(\"\\nNotice: Output shape matches input shape!\")\n",
    "print(\"This allows stacking attention layers in deep networks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Attention: Attending Across Sequences\n",
    "\n",
    "### What is Cross-Attention?\n",
    "\n",
    "**Self-attention**: Q, K, V all from the same sequence\n",
    "**Cross-attention**: Q from one sequence, K and V from another\n",
    "\n",
    "**Use case**: Decoder attending to encoder outputs in sequence-to-sequence tasks\n",
    "\n",
    "```\n",
    "Machine Translation Example:\n",
    "Encoder input:  \"Le chat est noir\"     (French)\n",
    "Decoder:        \"The cat is ___\"       (English, being generated)\n",
    "\n",
    "Cross-attention:\n",
    "  Q: From decoder (\"The cat is\")\n",
    "  K, V: From encoder (\"Le chat est noir\")\n",
    "  \n",
    "The decoder queries what it needs from the encoder!\n",
    "```\n",
    "\n",
    "**Why it's different from self-attention**:\n",
    "- **Information flow**: One sequence queries information from another\n",
    "- **Asymmetric**: Query sequence can be different length than Key/Value sequence\n",
    "- **Purpose**: Allows decoder to focus on relevant encoder states\n",
    "\n",
    "**Real-world applications**:\n",
    "- **Translation**: Decoder attends to source language\n",
    "- **Image captioning**: Text decoder attends to image features\n",
    "- **CLIP**: Vision attends to text (and vice versa)\n",
    "- **Multimodal LLMs**: Text attends to image patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Cross-attention: Attend from one sequence to another.\n",
    "    \n",
    "    Used in encoder-decoder architectures!\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, d_k, d_v):\n",
    "        super(CrossAttention, self).__init__()\n",
    "        \n",
    "        self.d_k = d_k\n",
    "        \n",
    "        # Q from decoder (what we're looking for)\n",
    "        self.W_Q = nn.Linear(d_model, d_k, bias=False)\n",
    "        \n",
    "        # K, V from encoder (what information is available)\n",
    "        self.W_K = nn.Linear(d_model, d_k, bias=False)\n",
    "        self.W_V = nn.Linear(d_model, d_v, bias=False)\n",
    "        \n",
    "        self.W_O = nn.Linear(d_v, d_model, bias=False)\n",
    "    \n",
    "    def forward(self, decoder_hidden, encoder_outputs, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            decoder_hidden: Decoder states (batch, decoder_len, d_model)\n",
    "            encoder_outputs: Encoder states (batch, encoder_len, d_model)\n",
    "            mask: Optional mask (batch, 1, decoder_len, encoder_len)\n",
    "        \n",
    "        Returns:\n",
    "            output: Cross-attention output (batch, decoder_len, d_model)\n",
    "            attention_weights: Where decoder attended in encoder\n",
    "        \"\"\"\n",
    "        # Q from decoder (what does decoder need?)\n",
    "        Q = self.W_Q(decoder_hidden).unsqueeze(1)  # (batch, 1, decoder_len, d_k)\n",
    "        \n",
    "        # K, V from encoder (what information is available?)\n",
    "        K = self.W_K(encoder_outputs).unsqueeze(1)  # (batch, 1, encoder_len, d_k)\n",
    "        V = self.W_V(encoder_outputs).unsqueeze(1)  # (batch, 1, encoder_len, d_v)\n",
    "        \n",
    "        # Compute cross-attention\n",
    "        # This is where decoder \"looks at\" encoder!\n",
    "        attn_output, attention_weights = scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        attn_output = attn_output.squeeze(1)\n",
    "        output = self.W_O(attn_output)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "# Example: Translation scenario\n",
    "encoder_len = 7  # \"Le chat est noir\" (4 words + special tokens)\n",
    "decoder_len = 4  # \"The cat is\" (3 words generated so far)\n",
    "d_model = 64\n",
    "\n",
    "cross_attn = CrossAttention(d_model, d_k=64, d_v=64)\n",
    "\n",
    "# Encoder has processed the French sentence\n",
    "encoder_outputs = torch.randn(1, encoder_len, d_model)\n",
    "\n",
    "# Decoder is generating English\n",
    "decoder_hidden = torch.randn(1, decoder_len, d_model)\n",
    "\n",
    "# Cross-attention: Decoder attends to encoder\n",
    "output, attn_weights = cross_attn(decoder_hidden, encoder_outputs)\n",
    "\n",
    "print(f\"Encoder sequence length: {encoder_len}\")\n",
    "print(f\"Decoder sequence length: {decoder_len}\")\n",
    "print(f\"Cross-attention output: {output.shape}\")\n",
    "print(f\"Attention weights: {attn_weights.shape}\")\n",
    "\n",
    "# Visualize cross-attention\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.heatmap(attn_weights[0, 0].detach().numpy(),\n",
    "            annot=True, fmt='.3f', cmap='Blues',\n",
    "            xticklabels=[f'Enc{i}' for i in range(encoder_len)],\n",
    "            yticklabels=[f'Dec{i}' for i in range(decoder_len)],\n",
    "            cbar_kws={'label': 'Attention Weight'})\n",
    "plt.title('Cross-Attention Weights\\n(Decoder attending to Encoder)')\n",
    "plt.xlabel('Encoder positions (Keys/Values)')\n",
    "plt.ylabel('Decoder positions (Queries)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNotice: Decoder positions attend to ENCODER positions!\")\n",
    "print(\"This is fundamentally different from self-attention.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head Attention: Learning Different Relationships\n",
    "\n",
    "### Why Multiple Heads?\n",
    "\n",
    "**Problem with single-head attention**: One attention pattern might not capture all relationships!\n",
    "\n",
    "**Example sentence**: \"The bank by the river is steep\"\n",
    "\n",
    "Different types of relationships:\n",
    "- **Head 1**: Syntactic relationships (\"bank\" → \"is\")\n",
    "- **Head 2**: Semantic relationships (\"bank\" → \"river\")\n",
    "- **Head 3**: Long-range dependencies (\"steep\" → \"bank\")\n",
    "\n",
    "**Multi-head attention**: Run attention multiple times in parallel, each with different learned projections\n",
    "\n",
    "```python\n",
    "# Instead of one attention:\n",
    "output = Attention(Q, K, V)\n",
    "\n",
    "# Use multiple heads:\n",
    "head_1 = Attention(Q1, K1, V1)\n",
    "head_2 = Attention(Q2, K2, V2)\n",
    "...\n",
    "head_h = Attention(Qh, Kh, Vh)\n",
    "\n",
    "output = Concat(head_1, ..., head_h) @ W_O\n",
    "```\n",
    "\n",
    "**Why this works**:\n",
    "1. **Diversity**: Each head can specialize in different patterns\n",
    "2. **Ensemble effect**: Multiple perspectives are better than one\n",
    "3. **Efficiency**: Splitting dimensions doesn't increase compute much\n",
    "4. **Representation power**: Can represent complex attention patterns\n",
    "\n",
    "**Standard configuration**:\n",
    "- **8 heads** (small models) to **64 heads** (large LLMs)\n",
    "- Each head has dimension $d_k = d_{model} / h$ (split the dimensions)\n",
    "- Total parameters similar to single-head with full dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head attention: The secret sauce of transformers!\n",
    "    \n",
    "    Key insight: Multiple attention patterns in parallel.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: Model dimension (e.g., 512)\n",
    "            num_heads: Number of attention heads (e.g., 8)\n",
    "        \n",
    "        Note: d_model must be divisible by num_heads!\n",
    "        Why? We split d_model across heads: d_k = d_model / num_heads\n",
    "        \"\"\"\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads  # Dimension per head\n",
    "        \n",
    "        # Projections for all heads (done in one matrix multiply for efficiency)\n",
    "        # Why single matrices? More efficient than separate matrices per head\n",
    "        self.W_Q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_K = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_V = nn.Linear(d_model, d_model, bias=False)\n",
    "        \n",
    "        # Output projection\n",
    "        self.W_O = nn.Linear(d_model, d_model, bias=False)\n",
    "    \n",
    "    def split_heads(self, x):\n",
    "        \"\"\"\n",
    "        Split the last dimension into (num_heads, d_k).\n",
    "        \n",
    "        Why? Each head processes a different subspace of the embeddings.\n",
    "        \n",
    "        Input:  (batch, seq_len, d_model)\n",
    "        Output: (batch, num_heads, seq_len, d_k)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "        return x.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "    \n",
    "    def combine_heads(self, x):\n",
    "        \"\"\"\n",
    "        Inverse of split_heads: merge heads back together.\n",
    "        \n",
    "        Input:  (batch, num_heads, seq_len, d_k)\n",
    "        Output: (batch, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        batch_size, num_heads, seq_len, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "    \n",
    "    def forward(self, X, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X: Input (batch, seq_len, d_model)\n",
    "            mask: Optional mask (batch, 1, seq_len, seq_len)\n",
    "        \n",
    "        Returns:\n",
    "            output: (batch, seq_len, d_model)\n",
    "            attention_weights: (batch, num_heads, seq_len, seq_len)\n",
    "        \"\"\"\n",
    "        # Step 1: Linear projections for all heads at once\n",
    "        Q = self.W_Q(X)  # (batch, seq_len, d_model)\n",
    "        K = self.W_K(X)\n",
    "        V = self.W_V(X)\n",
    "        \n",
    "        # Step 2: Split into multiple heads\n",
    "        # This is where we create parallel attention patterns!\n",
    "        Q = self.split_heads(Q)  # (batch, num_heads, seq_len, d_k)\n",
    "        K = self.split_heads(K)\n",
    "        V = self.split_heads(V)\n",
    "        \n",
    "        # Step 3: Scaled dot-product attention for all heads in parallel\n",
    "        # Each head learns different attention patterns!\n",
    "        attn_output, attention_weights = scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        # Step 4: Concatenate heads\n",
    "        attn_output = self.combine_heads(attn_output)  # (batch, seq_len, d_model)\n",
    "        \n",
    "        # Step 5: Final linear projection\n",
    "        # Why? Combine information from all heads\n",
    "        output = self.W_O(attn_output)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "# Create multi-head attention\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "seq_len = 10\n",
    "batch_size = 2\n",
    "\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "X = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "output, attn_weights = mha(X)\n",
    "\n",
    "print(f\"Input shape: {X.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attn_weights.shape}\")\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  d_model: {d_model}\")\n",
    "print(f\"  num_heads: {num_heads}\")\n",
    "print(f\"  d_k per head: {d_model // num_heads}\")\n",
    "print(f\"\\nEach of the {num_heads} heads learned different attention patterns!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Multi-Head Attention\n",
    "\n",
    "Let's see how different heads learn different patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention patterns from different heads\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for head_idx in range(num_heads):\n",
    "    ax = axes[head_idx]\n",
    "    \n",
    "    # Get attention weights for this head\n",
    "    head_weights = attn_weights[0, head_idx].detach().numpy()\n",
    "    \n",
    "    sns.heatmap(head_weights, ax=ax, cmap='viridis',\n",
    "                xticklabels=False, yticklabels=False,\n",
    "                cbar=True, square=True)\n",
    "    ax.set_title(f'Head {head_idx + 1}')\n",
    "    ax.set_xlabel('Key positions')\n",
    "    ax.set_ylabel('Query positions')\n",
    "\n",
    "plt.suptitle('Multi-Head Attention: Each Head Learns Different Patterns', \n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNotice how different heads have different attention patterns!\")\n",
    "print(\"Some heads focus on nearby tokens, others on distant ones.\")\n",
    "print(\"Some are sparse, others spread attention more uniformly.\")\n",
    "print(\"\\nThis diversity is what makes multi-head attention so powerful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using PyTorch's Optimized Attention\n",
    "\n",
    "### Why Use PyTorch's Built-in Functions?\n",
    "\n",
    "While it's important to understand attention from scratch, PyTorch provides **highly optimized** implementations:\n",
    "\n",
    "**`F.scaled_dot_product_attention`** (PyTorch 2.0+):\n",
    "- **Flash Attention**: Memory-efficient attention algorithm\n",
    "- **Kernel fusion**: Optimized CUDA kernels\n",
    "- **Automatic selection**: Chooses best implementation for your hardware\n",
    "- **Up to 10x faster** than naive implementations\n",
    "\n",
    "**When to use what**:\n",
    "- **Learning**: Use from-scratch implementation\n",
    "- **Production**: Use PyTorch's optimized functions\n",
    "- **Research**: Understand both!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch 2.0+ has optimized attention\n",
    "class OptimizedMultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head attention using PyTorch's optimized implementation.\n",
    "    \n",
    "    This is what you should use in production!\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super(OptimizedMultiHeadAttention, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.W_Q = nn.Linear(d_model, d_model)\n",
    "        self.W_K = nn.Linear(d_model, d_model)\n",
    "        self.W_V = nn.Linear(d_model, d_model)\n",
    "        self.W_O = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = dropout\n",
    "    \n",
    "    def forward(self, X, mask=None):\n",
    "        batch_size, seq_len, d_model = X.size()\n",
    "        \n",
    "        # Project and reshape\n",
    "        Q = self.W_Q(X).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_K(X).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_V(X).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Use PyTorch's optimized attention\n",
    "        # This automatically uses Flash Attention when available!\n",
    "        attn_output = F.scaled_dot_product_attention(\n",
    "            Q, K, V,\n",
    "            attn_mask=mask,\n",
    "            dropout_p=self.dropout if self.training else 0.0,\n",
    "            is_causal=False  # Set to True for causal masking (GPT-style)\n",
    "        )\n",
    "        \n",
    "        # Reshape and project\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)\n",
    "        output = self.W_O(attn_output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Compare implementations\n",
    "optimized_mha = OptimizedMultiHeadAttention(d_model=512, num_heads=8)\n",
    "X_test = torch.randn(2, 10, 512)\n",
    "\n",
    "output_optimized = optimized_mha(X_test)\n",
    "\n",
    "print(\"Optimized Multi-Head Attention:\")\n",
    "print(f\"Output shape: {output_optimized.shape}\")\n",
    "print(\"\\nBenefits of PyTorch's implementation:\")\n",
    "print(\"✓ Flash Attention: O(N) memory instead of O(N²)\")\n",
    "print(\"✓ Fused kernels: Fewer memory transfers\")\n",
    "print(\"✓ Hardware-specific optimizations\")\n",
    "print(\"✓ Automatic backward pass optimization\")\n",
    "print(\"\\nIn practice: Use this for training large models!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causal (Masked) Attention for Language Modeling\n",
    "\n",
    "### Why Do We Need Masking?\n",
    "\n",
    "In language modeling (e.g., GPT), we want to predict the next token. **Problem**: We can't let the model cheat by looking at future tokens!\n",
    "\n",
    "**Example**:\n",
    "```\n",
    "Sentence: \"The cat sat on the mat\"\n",
    "\n",
    "When predicting \"sat\":\n",
    "  Can attend to: \"The\", \"cat\"\n",
    "  CANNOT attend to: \"on\", \"the\", \"mat\" (these are in the future!)\n",
    "```\n",
    "\n",
    "**Solution: Causal masking**\n",
    "- Mask out future positions by setting their attention scores to -∞\n",
    "- After softmax, -∞ becomes 0 (no attention)\n",
    "- Creates a lower-triangular attention pattern\n",
    "\n",
    "**Why this is critical**:\n",
    "- **Training**: Prevents information leakage from future tokens\n",
    "- **Autoregressive generation**: Enables next-token prediction\n",
    "- **Foundation of GPT**: This masking is what makes GPT-style models work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_causal_mask(seq_len):\n",
    "    \"\"\"\n",
    "    Create a causal (lower-triangular) mask.\n",
    "    \n",
    "    Why lower-triangular? Token i can only attend to tokens 0...i\n",
    "    \n",
    "    Returns:\n",
    "        mask: (seq_len, seq_len) with 1s in lower triangle, 0s above\n",
    "    \"\"\"\n",
    "    mask = torch.tril(torch.ones(seq_len, seq_len))\n",
    "    return mask\n",
    "\n",
    "# Visualize causal mask\n",
    "seq_len = 8\n",
    "causal_mask = create_causal_mask(seq_len)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.heatmap(causal_mask.numpy(), annot=True, fmt='.0f',\n",
    "            cmap='RdYlGn', cbar=False, square=True,\n",
    "            xticklabels=[f'T{i}' for i in range(seq_len)],\n",
    "            yticklabels=[f'T{i}' for i in range(seq_len)])\n",
    "plt.title('Causal Mask for GPT-style Models\\n(1 = can attend, 0 = masked)', fontsize=14)\n",
    "plt.xlabel('Key positions (what can be attended to)')\n",
    "plt.ylabel('Query positions (current token)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"How to read this:\")\n",
    "print(\"- Row 0 (token 0): Can only attend to itself\")\n",
    "print(\"- Row 3 (token 3): Can attend to tokens 0, 1, 2, 3\")\n",
    "print(\"- Upper triangle is masked: Can't see the future!\")\n",
    "print(\"\\nThis is exactly how GPT generates text:\")\n",
    "print(\"Each token prediction can only use past context.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply causal attention\n",
    "seq_len = 8\n",
    "d_model = 64\n",
    "\n",
    "# Create input\n",
    "X = torch.randn(1, seq_len, d_model)\n",
    "\n",
    "# Create causal mask (1, 1, seq_len, seq_len)\n",
    "causal_mask = create_causal_mask(seq_len).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "# Apply multi-head attention with causal mask\n",
    "mha = MultiHeadAttention(d_model, num_heads=4)\n",
    "output, attn_weights = mha(X, mask=causal_mask)\n",
    "\n",
    "# Visualize one head's causal attention\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(attn_weights[0, 0].detach().numpy(),\n",
    "            annot=True, fmt='.3f', cmap='Blues',\n",
    "            xticklabels=[f'T{i}' for i in range(seq_len)],\n",
    "            yticklabels=[f'T{i}' for i in range(seq_len)],\n",
    "            cbar_kws={'label': 'Attention Weight'})\n",
    "plt.title('Causal Attention Weights (Head 1)\\nUpper triangle is zero!', fontsize=14)\n",
    "plt.xlabel('Key positions')\n",
    "plt.ylabel('Query positions')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice:\")\n",
    "print(\"- Upper triangle is all zeros (masked future)\")\n",
    "print(\"- Each row still sums to 1.0 (valid probability distribution)\")\n",
    "print(\"- This pattern enables autoregressive generation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini Exercises\n",
    "\n",
    "Test your understanding with these exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Attention Score Calculation\n",
    "\n",
    "Given:\n",
    "```python\n",
    "Q = torch.tensor([[1.0, 0.0, 0.0]])\n",
    "K = torch.tensor([[1.0, 0.0, 0.0],\n",
    "                  [0.0, 1.0, 0.0],\n",
    "                  [0.0, 0.0, 1.0]])\n",
    "```\n",
    "\n",
    "Calculate the attention weights (before softmax) with scaling. What does this tell you about attention?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "# SOLUTION\n",
    "def show_solution_1():\n",
    "    Q = torch.tensor([[1.0, 0.0, 0.0]])\n",
    "    K = torch.tensor([[1.0, 0.0, 0.0],\n",
    "                      [0.0, 1.0, 0.0],\n",
    "                      [0.0, 0.0, 1.0]])\n",
    "    \n",
    "    d_k = K.size(-1)\n",
    "    \n",
    "    # Compute scores\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
    "    \n",
    "    print(\"Attention scores (before softmax):\")\n",
    "    print(scores)\n",
    "    \n",
    "    # Apply softmax\n",
    "    weights = F.softmax(scores, dim=-1)\n",
    "    print(\"\\nAttention weights (after softmax):\")\n",
    "    print(weights)\n",
    "    \n",
    "    print(\"\\nInterpretation:\")\n",
    "    print(\"- Q is [1,0,0] and K[0] is [1,0,0]: Perfect alignment → highest score\")\n",
    "    print(\"- Q is [1,0,0] but K[1] is [0,1,0]: Orthogonal → score of 0\")\n",
    "    print(\"- Dot product measures similarity/alignment of vectors\")\n",
    "    print(\"- Softmax converts scores to probabilities\")\n",
    "\n",
    "# Uncomment to see solution:\n",
    "# show_solution_1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Implement Additive Attention\n",
    "\n",
    "The original attention mechanism (Bahdanau et al., 2015) used **additive attention** instead of dot-product:\n",
    "\n",
    "$$\\text{score}(Q, K) = v^T \\tanh(W_Q Q + W_K K)$$\n",
    "\n",
    "Implement additive attention and compare with dot-product attention. Why did dot-product win?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "class AdditiveAttention(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(AdditiveAttention, self).__init__()\n",
    "        # Define W_Q, W_K, and v\n",
    "        pass\n",
    "    \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        # Implement additive attention\n",
    "        pass\n",
    "\n",
    "\n",
    "# SOLUTION\n",
    "def show_solution_2():\n",
    "    class AdditiveAttention(nn.Module):\n",
    "        def __init__(self, d_model):\n",
    "            super(AdditiveAttention, self).__init__()\n",
    "            self.W_Q = nn.Linear(d_model, d_model, bias=False)\n",
    "            self.W_K = nn.Linear(d_model, d_model, bias=False)\n",
    "            self.v = nn.Linear(d_model, 1, bias=False)\n",
    "        \n",
    "        def forward(self, Q, K, V, mask=None):\n",
    "            # Q: (batch, seq_len_q, d_model)\n",
    "            # K: (batch, seq_len_k, d_model)\n",
    "            \n",
    "            # Project Q and K\n",
    "            Q_proj = self.W_Q(Q).unsqueeze(2)  # (batch, seq_len_q, 1, d_model)\n",
    "            K_proj = self.W_K(K).unsqueeze(1)  # (batch, 1, seq_len_k, d_model)\n",
    "            \n",
    "            # Additive combination\n",
    "            combined = torch.tanh(Q_proj + K_proj)  # (batch, seq_len_q, seq_len_k, d_model)\n",
    "            \n",
    "            # Score\n",
    "            scores = self.v(combined).squeeze(-1)  # (batch, seq_len_q, seq_len_k)\n",
    "            \n",
    "            if mask is not None:\n",
    "                scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "            \n",
    "            weights = F.softmax(scores, dim=-1)\n",
    "            output = torch.matmul(weights, V)\n",
    "            \n",
    "            return output, weights\n",
    "    \n",
    "    print(\"Additive vs Dot-Product Attention:\")\n",
    "    print(\"\\nAdditive Attention:\")\n",
    "    print(\"  Pros: More expressive (learned non-linearity)\")\n",
    "    print(\"  Cons: Slower (more parameters, complex computation)\")\n",
    "    print(\"\\nDot-Product Attention:\")\n",
    "    print(\"  Pros: Faster (matrix multiply is highly optimized)\")\n",
    "    print(\"  Pros: Fewer parameters\")\n",
    "    print(\"  Cons: Less flexible (just dot product)\")\n",
    "    print(\"\\nWhy dot-product won:\")\n",
    "    print(\"  - Speed matters for large models (billions of parameters)\")\n",
    "    print(\"  - Multi-head attention adds expressiveness\")\n",
    "    print(\"  - Hardware optimization (GPUs excel at matrix multiply)\")\n",
    "\n",
    "# Uncomment to see solution:\n",
    "# show_solution_2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Attention for Sequence Classification\n",
    "\n",
    "Build a simple sequence classifier that uses self-attention to aggregate token representations. Use attention pooling instead of taking the last token or averaging all tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "class AttentionClassifier(nn.Module):\n",
    "    def __init__(self, d_model, num_classes):\n",
    "        super(AttentionClassifier, self).__init__()\n",
    "        # Add your code\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Add your code\n",
    "        pass\n",
    "\n",
    "\n",
    "# SOLUTION\n",
    "def show_solution_3():\n",
    "    class AttentionPooling(nn.Module):\n",
    "        \"\"\"\n",
    "        Attention-based pooling: Learn to attend to important tokens.\n",
    "        \"\"\"\n",
    "        def __init__(self, d_model):\n",
    "            super(AttentionPooling, self).__init__()\n",
    "            # Learnable query vector\n",
    "            self.query = nn.Parameter(torch.randn(1, 1, d_model))\n",
    "            self.W_K = nn.Linear(d_model, d_model)\n",
    "            self.W_V = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            # x: (batch, seq_len, d_model)\n",
    "            batch_size = x.size(0)\n",
    "            \n",
    "            # Expand query for batch\n",
    "            Q = self.query.expand(batch_size, -1, -1)  # (batch, 1, d_model)\n",
    "            K = self.W_K(x)  # (batch, seq_len, d_model)\n",
    "            V = self.W_V(x)\n",
    "            \n",
    "            # Attention scores\n",
    "            scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(K.size(-1))\n",
    "            weights = F.softmax(scores, dim=-1)  # (batch, 1, seq_len)\n",
    "            \n",
    "            # Weighted sum\n",
    "            pooled = torch.matmul(weights, V).squeeze(1)  # (batch, d_model)\n",
    "            \n",
    "            return pooled, weights\n",
    "    \n",
    "    class AttentionClassifier(nn.Module):\n",
    "        def __init__(self, d_model, num_classes):\n",
    "            super(AttentionClassifier, self).__init__()\n",
    "            self.attention_pool = AttentionPooling(d_model)\n",
    "            self.classifier = nn.Linear(d_model, num_classes)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            # Pool with attention\n",
    "            pooled, weights = self.attention_pool(x)\n",
    "            # Classify\n",
    "            logits = self.classifier(pooled)\n",
    "            return logits, weights\n",
    "    \n",
    "    # Test\n",
    "    model = AttentionClassifier(d_model=64, num_classes=5)\n",
    "    x = torch.randn(2, 10, 64)  # (batch=2, seq_len=10, d_model=64)\n",
    "    logits, weights = model(x)\n",
    "    \n",
    "    print(f\"Input shape: {x.shape}\")\n",
    "    print(f\"Output logits: {logits.shape}\")\n",
    "    print(f\"Attention weights: {weights.shape}\")\n",
    "    print(\"\\nWhy attention pooling?\")\n",
    "    print(\"- Learns which tokens are important for classification\")\n",
    "    print(\"- More flexible than averaging or taking [CLS] token\")\n",
    "    print(\"- Provides interpretability (can visualize which tokens matter)\")\n",
    "\n",
    "# Uncomment to see solution:\n",
    "# show_solution_3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprehensive Exercise: Build a Complete Attention Module\n",
    "\n",
    "Build a production-ready attention module that supports:\n",
    "1. Multi-head self-attention\n",
    "2. Optional causal masking\n",
    "3. Dropout for regularization\n",
    "4. Residual connection and layer normalization\n",
    "\n",
    "This is what you'd actually use in a transformer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "# SOLUTION\n",
    "def show_comprehensive_solution():\n",
    "    class TransformerAttentionBlock(nn.Module):\n",
    "        \"\"\"\n",
    "        Complete attention block as used in transformers.\n",
    "        \n",
    "        Includes:\n",
    "        - Multi-head attention\n",
    "        - Dropout\n",
    "        - Residual connection\n",
    "        - Layer normalization\n",
    "        \"\"\"\n",
    "        def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "            super(TransformerAttentionBlock, self).__init__()\n",
    "            \n",
    "            self.attention = MultiHeadAttention(d_model, num_heads)\n",
    "            self.norm = nn.LayerNorm(d_model)\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        def forward(self, x, mask=None):\n",
    "            \"\"\"\n",
    "            Args:\n",
    "                x: (batch, seq_len, d_model)\n",
    "                mask: Optional attention mask\n",
    "            \n",
    "            Returns:\n",
    "                output: (batch, seq_len, d_model)\n",
    "            \"\"\"\n",
    "            # Multi-head attention\n",
    "            attn_output, _ = self.attention(x, mask)\n",
    "            \n",
    "            # Dropout\n",
    "            attn_output = self.dropout(attn_output)\n",
    "            \n",
    "            # Residual connection + Layer norm (Post-LN)\n",
    "            # Why residual? Helps gradients flow, enables deep networks\n",
    "            # Why layer norm? Stabilizes training\n",
    "            output = self.norm(x + attn_output)\n",
    "            \n",
    "            return output\n",
    "    \n",
    "    # Test the complete block\n",
    "    block = TransformerAttentionBlock(d_model=512, num_heads=8, dropout=0.1)\n",
    "    x = torch.randn(2, 10, 512)\n",
    "    \n",
    "    output = block(x)\n",
    "    \n",
    "    print(\"Complete Transformer Attention Block:\")\n",
    "    print(f\"Input shape: {x.shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(\"\\nThis block includes everything needed for transformers:\")\n",
    "    print(\"✓ Multi-head attention (parallel attention patterns)\")\n",
    "    print(\"✓ Dropout (regularization)\")\n",
    "    print(\"✓ Residual connection (gradient flow)\")\n",
    "    print(\"✓ Layer normalization (training stability)\")\n",
    "    print(\"\\nStack multiple blocks → You have a transformer!\")\n",
    "\n",
    "# Uncomment to see solution:\n",
    "# show_comprehensive_solution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### Core Concepts\n",
    "\n",
    "**1. Why attention was invented**:\n",
    "- Solves the fixed-length bottleneck in sequence-to-sequence models\n",
    "- Allows dynamic focus on relevant parts of input\n",
    "- Enables long-range dependencies without vanishing gradients\n",
    "\n",
    "**2. The Query-Key-Value paradigm**:\n",
    "- **Query**: What information to look for\n",
    "- **Key**: What information is available (searchable index)\n",
    "- **Value**: The actual information to retrieve\n",
    "- Separation enables flexible, learned information retrieval\n",
    "\n",
    "**3. Scaled dot-product attention**:\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "- Dot product measures similarity\n",
    "- Scaling prevents saturation\n",
    "- Softmax normalizes to probabilities\n",
    "- Multiply by V retrieves information\n",
    "\n",
    "**4. Self-attention vs Cross-attention**:\n",
    "- **Self-attention**: Sequence attends to itself (Q, K, V from same source)\n",
    "- **Cross-attention**: One sequence attends to another (Q from one, K/V from another)\n",
    "\n",
    "**5. Multi-head attention**:\n",
    "- Multiple attention patterns in parallel\n",
    "- Each head can specialize (syntax, semantics, long-range)\n",
    "- Ensemble effect improves representation\n",
    "\n",
    "**6. Causal masking**:\n",
    "- Prevents attending to future tokens\n",
    "- Essential for autoregressive models (GPT)\n",
    "- Creates lower-triangular attention pattern\n",
    "\n",
    "### Connection to Modern AI\n",
    "\n",
    "**Transformers = Attention + FFN + Norm**:\n",
    "- Attention is the core building block\n",
    "- Next topic: Complete transformer architecture\n",
    "\n",
    "**Used everywhere in 2025**:\n",
    "- **Language models**: GPT, LLaMA, Claude\n",
    "- **Vision**: Vision transformers (ViT)\n",
    "- **Multimodal**: CLIP, Flamingo, GPT-4V\n",
    "- **Audio**: Whisper, MusicGen\n",
    "- **Protein folding**: AlphaFold\n",
    "\n",
    "**Why attention cannot be skipped**:\n",
    "- Foundation of all modern deep learning\n",
    "- Enables transfer learning at scale\n",
    "- Key to understanding any state-of-the-art model\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "You've mastered attention! Next:\n",
    "- **Positional encoding**: How to inject position information\n",
    "- **Transformer architecture**: Putting it all together\n",
    "- **Advanced attention**: Flash attention, GQA, MoE\n",
    "\n",
    "Understanding attention deeply is your ticket to understanding modern AI!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "\n",
    "### Essential Papers\n",
    "1. **Bahdanau et al. (2015)**: \"Neural Machine Translation by Jointly Learning to Align and Translate\" (Original attention)\n",
    "2. **Vaswani et al. (2017)**: \"Attention is All You Need\" (Transformers, multi-head attention)\n",
    "3. **Dao et al. (2022)**: \"FlashAttention: Fast and Memory-Efficient Exact Attention\" (Modern optimization)\n",
    "\n",
    "### Tutorials and Visualizations\n",
    "4. **The Illustrated Transformer** (Jay Alammar): http://jalammar.github.io/illustrated-transformer/\n",
    "5. **Attention? Attention!** (Lilian Weng): https://lilianweng.github.io/posts/2018-06-24-attention/\n",
    "\n",
    "### Advanced Topics\n",
    "6. **Grouped Query Attention** (GQA): Efficient attention for large models\n",
    "7. **Flash Attention**: Memory-efficient attention computation\n",
    "8. **Sparse Attention**: Reducing O(N²) complexity\n",
    "\n",
    "### Implementation Resources\n",
    "- PyTorch documentation: `torch.nn.MultiheadAttention`\n",
    "- Hugging Face Transformers library\n",
    "- Annotated Transformer (Harvard NLP): http://nlp.seas.harvard.edu/annotated-transformer/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
