{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Topic 14: torch.compile & Performance Optimization\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this notebook, you will:\n",
        "- Understand how torch.compile works (TorchDynamo + TorchInductor)\n",
        "- Master regional compilation for transformer layers (PyTorch 2.5+)\n",
        "- Learn mixed precision training (AMP, float16, bfloat16)\n",
        "- Profile bottlenecks using torch.profiler\n",
        "- Measure real speedups with comprehensive benchmarks\n",
        "- Know when and how to use torch.compile effectively\n",
        "- Optimize memory usage and throughput\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. The Big Picture: Why torch.compile?\n",
        "\n",
        "### The Problem with Eager Execution\n",
        "\n",
        "**Traditional PyTorch (eager mode)**:\n",
        "- Executes operations one at a time\n",
        "- Each operation: Python ‚Üí C++ ‚Üí CUDA kernel\n",
        "- Massive Python overhead for small operations\n",
        "- No cross-operation optimization\n",
        "\n",
        "**Example overhead**:\n",
        "```python\n",
        "x = x + 1      # Launch kernel 1\n",
        "x = x * 2      # Launch kernel 2\n",
        "x = x.relu()   # Launch kernel 3\n",
        "```\n",
        "- 3 separate kernel launches\n",
        "- 3x Python interpreter overhead\n",
        "- 3x memory reads/writes\n",
        "\n",
        "### The Compilation Revolution (PyTorch 2.0+)\n",
        "\n",
        "**torch.compile** transforms your model:\n",
        "1. **Capture graph**: TorchDynamo traces Python execution\n",
        "2. **Optimize graph**: Fuse operations, eliminate redundancy\n",
        "3. **Generate kernels**: TorchInductor creates optimized CUDA code\n",
        "4. **Execute fast**: Compiled kernels run 30-200% faster\n",
        "\n",
        "**Same example, compiled**:\n",
        "```python\n",
        "x = (x + 1) * 2\n",
        "x = x.relu()\n",
        "# ‚Üí Single fused kernel!\n",
        "```\n",
        "- 1 kernel launch\n",
        "- 1 memory read, 1 write\n",
        "- No Python overhead\n",
        "\n",
        "### Why This Matters\n",
        "\n",
        "**Real-world speedups**:\n",
        "- üöÄ **Training**: 30-50% faster on transformers\n",
        "- üöÄ **Inference**: 50-200% faster (especially small batches)\n",
        "- üöÄ **Memory**: Reduced memory traffic ‚Üí higher throughput\n",
        "\n",
        "**Modern LLM usage**:\n",
        "- ‚úÖ **LLaMA 2/3**: torch.compile for training\n",
        "- ‚úÖ **Mistral**: Compiled inference pipelines\n",
        "- ‚úÖ **GPT-4**: Likely uses similar compilation\n",
        "- ‚úÖ **Your models**: Free speedup with one line!\n",
        "\n",
        "### How It Works: The Stack\n",
        "\n",
        "```\n",
        "Your PyTorch Code\n",
        "        ‚Üì\n",
        "  TorchDynamo (captures Python execution as graph)\n",
        "        ‚Üì\n",
        "  AOTAutograd (ahead-of-time gradients)\n",
        "        ‚Üì\n",
        "  TorchInductor (generates optimized kernels)\n",
        "        ‚Üì\n",
        "  Triton/CUDA (low-level execution)\n",
        "```\n",
        "\n",
        "**You just write**: `model = torch.compile(model)`\n",
        "\n",
        "**PyTorch handles**: Everything else!\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "from typing import Optional\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Check if torch.compile is available\n",
        "compile_available = hasattr(torch, 'compile')\n",
        "print(f\"\\ntorch.compile available: {compile_available}\")\n",
        "if not compile_available:\n",
        "    print(\"‚ö†Ô∏è torch.compile requires PyTorch 2.0+\")\n",
        "    print(\"   Install with: pip install torch>=2.0.0\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 2. Basic torch.compile Usage\n",
        "\n",
        "### Compilation Modes\n",
        "\n",
        "PyTorch offers different compilation modes for different use cases:\n",
        "\n",
        "1. **`default`**: Balanced speed and compilation time\n",
        "2. **`reduce-overhead`**: Minimize Python overhead (best for small ops)\n",
        "3. **`max-autotune`**: Maximum optimization (slow compile, fast runtime)\n",
        "\n",
        "### Simple Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SimpleModel(nn.Module):\n",
        "    \"\"\"Simple model to demonstrate torch.compile\"\"\"\n",
        "    \n",
        "    def __init__(self, d_model: int):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_model)\n",
        "        self.linear2 = nn.Linear(d_model, d_model)\n",
        "        self.linear3 = nn.Linear(d_model, d_model)\n",
        "    \n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # Many small operations - perfect for compilation\n",
        "        x = self.linear1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.linear2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.linear3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "if compile_available:\n",
        "    # Create two versions: eager and compiled\n",
        "    d_model = 512\n",
        "    model_eager = SimpleModel(d_model).to(device)\n",
        "    model_compiled = torch.compile(SimpleModel(d_model).to(device))\n",
        "    \n",
        "    # Copy weights so they're identical\n",
        "    model_compiled.load_state_dict(model_eager.state_dict())\n",
        "    \n",
        "    # Test forward pass\n",
        "    x = torch.randn(32, 128, d_model, device=device)\n",
        "    \n",
        "    # Eager mode\n",
        "    out_eager = model_eager(x)\n",
        "    \n",
        "    # Compiled mode (first run compiles)\n",
        "    print(\"Compiling model... (first run)\")\n",
        "    out_compiled = model_compiled(x)\n",
        "    \n",
        "    # Check outputs match\n",
        "    print(f\"\\nOutputs match: {torch.allclose(out_eager, out_compiled, atol=1e-5)}\")\n",
        "    print(f\"Max difference: {(out_eager - out_compiled).abs().max().item():.2e}\")\n",
        "    print(\"\\n‚úÖ torch.compile produces identical results!\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Skipping compile demo - PyTorch 2.0+ required\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Benchmark: Eager vs Compiled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def benchmark_model(model, x, num_iters=100, warmup=10):\n",
        "    \"\"\"Benchmark model throughput\"\"\"\n",
        "    \n",
        "    # Warmup\n",
        "    for _ in range(warmup):\n",
        "        _ = model(x)\n",
        "    \n",
        "    if device.type == 'cuda':\n",
        "        torch.cuda.synchronize()\n",
        "    \n",
        "    # Benchmark\n",
        "    start = time.time()\n",
        "    for _ in range(num_iters):\n",
        "        _ = model(x)\n",
        "    \n",
        "    if device.type == 'cuda':\n",
        "        torch.cuda.synchronize()\n",
        "    \n",
        "    elapsed = time.time() - start\n",
        "    return elapsed / num_iters\n",
        "\n",
        "\n",
        "if compile_available and device.type == 'cuda':\n",
        "    print(\"Benchmarking Eager vs Compiled\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    batch_sizes = [8, 16, 32, 64]\n",
        "    eager_times = []\n",
        "    compiled_times = []\n",
        "    \n",
        "    for batch_size in batch_sizes:\n",
        "        x = torch.randn(batch_size, 128, d_model, device=device)\n",
        "        \n",
        "        eager_time = benchmark_model(model_eager, x)\n",
        "        compiled_time = benchmark_model(model_compiled, x)\n",
        "        \n",
        "        eager_times.append(eager_time * 1000)  # Convert to ms\n",
        "        compiled_times.append(compiled_time * 1000)\n",
        "        \n",
        "        speedup = eager_time / compiled_time\n",
        "        print(f\"Batch {batch_size:2d}: Eager={eager_time*1000:.2f}ms, \"\n",
        "              f\"Compiled={compiled_time*1000:.2f}ms, Speedup={speedup:.2f}x\")\n",
        "    \n",
        "    # Visualize\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    \n",
        "    # Plot 1: Time comparison\n",
        "    x_pos = np.arange(len(batch_sizes))\n",
        "    width = 0.35\n",
        "    \n",
        "    axes[0].bar(x_pos - width/2, eager_times, width, label='Eager', color='orange', alpha=0.7)\n",
        "    axes[0].bar(x_pos + width/2, compiled_times, width, label='Compiled', color='blue', alpha=0.7)\n",
        "    axes[0].set_xlabel('Batch Size', fontsize=12)\n",
        "    axes[0].set_ylabel('Time (ms)', fontsize=12)\n",
        "    axes[0].set_title('Eager vs Compiled Execution Time', fontsize=14)\n",
        "    axes[0].set_xticks(x_pos)\n",
        "    axes[0].set_xticklabels(batch_sizes)\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(True, alpha=0.3, axis='y')\n",
        "    \n",
        "    # Plot 2: Speedup\n",
        "    speedups = [e/c for e, c in zip(eager_times, compiled_times)]\n",
        "    axes[1].bar(x_pos, speedups, color='green', alpha=0.7)\n",
        "    axes[1].axhline(y=1.0, color='r', linestyle='--', label='Baseline')\n",
        "    axes[1].set_xlabel('Batch Size', fontsize=12)\n",
        "    axes[1].set_ylabel('Speedup (x)', fontsize=12)\n",
        "    axes[1].set_title('torch.compile Speedup', fontsize=14)\n",
        "    axes[1].set_xticks(x_pos)\n",
        "    axes[1].set_xticklabels(batch_sizes)\n",
        "    axes[1].legend()\n",
        "    axes[1].grid(True, alpha=0.3, axis='y')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    avg_speedup = np.mean(speedups)\n",
        "    print(f\"\\nüí° Average speedup: {avg_speedup:.2f}x\")\n",
        "    print(f\"   Compiled models are {(avg_speedup-1)*100:.0f}% faster!\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Skipping benchmark - requires CUDA and PyTorch 2.0+\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 3. Compilation Modes Deep Dive\n",
        "\n",
        "### Understanding the Modes\n",
        "\n",
        "Different modes optimize for different scenarios:\n",
        "\n",
        "| Mode | Compile Time | Runtime Speed | Use Case |\n",
        "|------|--------------|---------------|----------|\n",
        "| **default** | Fast | Good | General purpose, development |\n",
        "| **reduce-overhead** | Medium | Better | Small ops, low latency |\n",
        "| **max-autotune** | Slow | Best | Production, throughput-critical |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if compile_available and device.type == 'cuda':\n",
        "    print(\"Comparing Compilation Modes\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    # Create models with different modes\n",
        "    model_default = torch.compile(SimpleModel(d_model).to(device), mode=\"default\")\n",
        "    model_reduce = torch.compile(SimpleModel(d_model).to(device), mode=\"reduce-overhead\")\n",
        "    \n",
        "    # max-autotune can be very slow to compile, so we'll skip it in this demo\n",
        "    # model_max = torch.compile(SimpleModel(d_model).to(device), mode=\"max-autotune\")\n",
        "    \n",
        "    x = torch.randn(32, 128, d_model, device=device)\n",
        "    \n",
        "    # Compile (first run)\n",
        "    print(\"\\nCompiling models...\")\n",
        "    \n",
        "    start = time.time()\n",
        "    _ = model_default(x)\n",
        "    default_compile_time = time.time() - start\n",
        "    print(f\"  default mode: {default_compile_time:.2f}s\")\n",
        "    \n",
        "    start = time.time()\n",
        "    _ = model_reduce(x)\n",
        "    reduce_compile_time = time.time() - start\n",
        "    print(f\"  reduce-overhead mode: {reduce_compile_time:.2f}s\")\n",
        "    \n",
        "    # Benchmark runtime\n",
        "    print(\"\\nBenchmarking runtime...\")\n",
        "    default_time = benchmark_model(model_default, x) * 1000\n",
        "    reduce_time = benchmark_model(model_reduce, x) * 1000\n",
        "    \n",
        "    print(f\"\\nResults:\")\n",
        "    print(f\"  default: {default_time:.2f}ms\")\n",
        "    print(f\"  reduce-overhead: {reduce_time:.2f}ms\")\n",
        "    print(f\"\\nüí° reduce-overhead is {default_time/reduce_time:.2f}x faster for this model\")\n",
        "    print(f\"   Use reduce-overhead for inference with many small operations\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Skipping mode comparison - requires CUDA and PyTorch 2.0+\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 4. Regional Compilation (PyTorch 2.5+)\n",
        "\n",
        "### The Problem with Full Model Compilation\n",
        "\n",
        "**Issue**: Some operations don't compile well\n",
        "- Dynamic control flow (if/else based on tensor values)\n",
        "- Python data structures modified during forward\n",
        "- External function calls\n",
        "\n",
        "**Solution**: Compile only specific regions!\n",
        "\n",
        "### Using torch.compiler.region\n",
        "\n",
        "Mark specific code blocks for compilation:\n",
        "```python\n",
        "with torch.compiler.region():\n",
        "    # This code will be compiled\n",
        "    x = self.layer1(x)\n",
        "    x = self.layer2(x)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"Transformer block with regional compilation\"\"\"\n",
        "    \n",
        "    def __init__(self, d_model: int, num_heads: int, d_ff: int):\n",
        "        super().__init__()\n",
        "        self.attention = nn.MultiheadAttention(d_model, num_heads, batch_first=True)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        \n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_ff, d_model)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # Attention block\n",
        "        residual = x\n",
        "        x = self.norm1(x)\n",
        "        x, _ = self.attention(x, x, x, need_weights=False)\n",
        "        x = residual + x\n",
        "        \n",
        "        # FFN block (good candidate for compilation)\n",
        "        residual = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.ffn(x)\n",
        "        x = residual + x\n",
        "        \n",
        "        return x\n",
        "\n",
        "\n",
        "# Demo transformer block\n",
        "print(\"Transformer Block Compilation\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "d_model = 512\n",
        "num_heads = 8\n",
        "d_ff = 2048\n",
        "\n",
        "block = TransformerBlock(d_model, num_heads, d_ff).to(device)\n",
        "\n",
        "if compile_available:\n",
        "    # Compile the entire block\n",
        "    block_compiled = torch.compile(block)\n",
        "    \n",
        "    x = torch.randn(16, 128, d_model, device=device)\n",
        "    \n",
        "    print(\"Compiling transformer block...\")\n",
        "    out = block_compiled(x)\n",
        "    print(f\"Output shape: {out.shape}\")\n",
        "    print(\"\\n‚úÖ Entire transformer block compiled successfully!\")\n",
        "    print(\"   torch.compile automatically handles attention and FFN layers.\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Skipping transformer compilation - PyTorch 2.0+ required\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 5. Mixed Precision Training\n",
        "\n",
        "### Why Mixed Precision?\n",
        "\n",
        "**float32 (full precision)**:\n",
        "- ‚úÖ High accuracy\n",
        "- ‚ùå 4 bytes per number\n",
        "- ‚ùå Slower compute\n",
        "\n",
        "**float16 (half precision)**:\n",
        "- ‚úÖ 2 bytes per number (2x memory savings)\n",
        "- ‚úÖ 2-3x faster on modern GPUs\n",
        "- ‚ùå Reduced range (overflow risk)\n",
        "- ‚ùå Reduced precision (underflow risk)\n",
        "\n",
        "**bfloat16 (brain float)**:\n",
        "- ‚úÖ 2 bytes per number\n",
        "- ‚úÖ Same range as float32 (no overflow)\n",
        "- ‚úÖ 2-3x faster\n",
        "- ‚úÖ Better for training stability\n",
        "- ‚ùå Requires newer GPUs (A100, H100)\n",
        "\n",
        "### Automatic Mixed Precision (AMP)\n",
        "\n",
        "PyTorch automatically:\n",
        "1. Uses float16/bfloat16 where safe\n",
        "2. Uses float32 where needed (loss computation)\n",
        "3. Scales gradients to prevent underflow\n",
        "\n",
        "**Result**: Fast training with stable convergence!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_with_amp(model, optimizer, x, target, use_amp=False, dtype=torch.float16):\n",
        "    \"\"\"Training step with optional AMP\"\"\"\n",
        "    \n",
        "    if use_amp:\n",
        "        # Create GradScaler for numerical stability\n",
        "        scaler = torch.amp.GradScaler('cuda')\n",
        "        \n",
        "        # Forward pass in mixed precision\n",
        "        with torch.amp.autocast('cuda', dtype=dtype):\n",
        "            output = model(x)\n",
        "            loss = F.mse_loss(output, target)\n",
        "        \n",
        "        # Backward pass with gradient scaling\n",
        "        optimizer.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "    else:\n",
        "        # Standard float32 training\n",
        "        output = model(x)\n",
        "        loss = F.mse_loss(output, target)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "if device.type == 'cuda':\n",
        "    print(\"Mixed Precision Training Benchmark\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    # Create model and data\n",
        "    model_fp32 = TransformerBlock(d_model, num_heads, d_ff).to(device)\n",
        "    model_fp16 = TransformerBlock(d_model, num_heads, d_ff).to(device)\n",
        "    model_fp16.load_state_dict(model_fp32.state_dict())\n",
        "    \n",
        "    optimizer_fp32 = torch.optim.Adam(model_fp32.parameters(), lr=1e-4)\n",
        "    optimizer_fp16 = torch.optim.Adam(model_fp16.parameters(), lr=1e-4)\n",
        "    \n",
        "    batch_size = 32\n",
        "    seq_len = 128\n",
        "    x = torch.randn(batch_size, seq_len, d_model, device=device)\n",
        "    target = torch.randn(batch_size, seq_len, d_model, device=device)\n",
        "    \n",
        "    # Warmup\n",
        "    for _ in range(5):\n",
        "        _ = train_with_amp(model_fp32, optimizer_fp32, x, target, use_amp=False)\n",
        "        _ = train_with_amp(model_fp16, optimizer_fp16, x, target, use_amp=True)\n",
        "    \n",
        "    torch.cuda.synchronize()\n",
        "    \n",
        "    # Benchmark float32\n",
        "    start = time.time()\n",
        "    for _ in range(50):\n",
        "        _ = train_with_amp(model_fp32, optimizer_fp32, x, target, use_amp=False)\n",
        "    torch.cuda.synchronize()\n",
        "    fp32_time = (time.time() - start) / 50 * 1000\n",
        "    \n",
        "    # Benchmark float16\n",
        "    start = time.time()\n",
        "    for _ in range(50):\n",
        "        _ = train_with_amp(model_fp16, optimizer_fp16, x, target, use_amp=True)\n",
        "    torch.cuda.synchronize()\n",
        "    fp16_time = (time.time() - start) / 50 * 1000\n",
        "    \n",
        "    speedup = fp32_time / fp16_time\n",
        "    \n",
        "    print(f\"Results:\")\n",
        "    print(f\"  float32: {fp32_time:.2f} ms/iter\")\n",
        "    print(f\"  float16 (AMP): {fp16_time:.2f} ms/iter\")\n",
        "    print(f\"  Speedup: {speedup:.2f}x\")\n",
        "    \n",
        "    # Memory comparison\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    _ = train_with_amp(model_fp32, optimizer_fp32, x, target, use_amp=False)\n",
        "    fp32_mem = torch.cuda.max_memory_allocated() / 1024**2\n",
        "    \n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    _ = train_with_amp(model_fp16, optimizer_fp16, x, target, use_amp=True)\n",
        "    fp16_mem = torch.cuda.max_memory_allocated() / 1024**2\n",
        "    \n",
        "    print(f\"\\nMemory usage:\")\n",
        "    print(f\"  float32: {fp32_mem:.1f} MB\")\n",
        "    print(f\"  float16 (AMP): {fp16_mem:.1f} MB\")\n",
        "    print(f\"  Memory saved: {(1 - fp16_mem/fp32_mem)*100:.1f}%\")\n",
        "    \n",
        "    print(f\"\\nüí° AMP provides {speedup:.1f}x speedup and {(1-fp16_mem/fp32_mem)*100:.0f}% memory savings!\")\n",
        "    print(\"   Use AMP for all training - it's nearly free performance.\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Skipping AMP benchmark - requires CUDA\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 6. Profiling with torch.profiler\n",
        "\n",
        "### Finding Bottlenecks\n",
        "\n",
        "**torch.profiler** helps you:\n",
        "- Identify slow operations\n",
        "- Find memory bottlenecks\n",
        "- See GPU utilization\n",
        "- Optimize CUDA kernel usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if device.type == 'cuda':\n",
        "    from torch.profiler import profile, ProfilerActivity, record_function\n",
        "    \n",
        "    print(\"Profiling Transformer Block\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    model = TransformerBlock(d_model, num_heads, d_ff).to(device)\n",
        "    x = torch.randn(32, 128, d_model, device=device)\n",
        "    \n",
        "    # Profile the model\n",
        "    with profile(\n",
        "        activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
        "        record_shapes=True,\n",
        "        profile_memory=True,\n",
        "        with_stack=True\n",
        "    ) as prof:\n",
        "        with record_function(\"model_forward\"):\n",
        "            output = model(x)\n",
        "    \n",
        "    # Print profiling results\n",
        "    print(\"\\nTop 10 operations by CUDA time:\")\n",
        "    print(prof.key_averages().table(\n",
        "        sort_by=\"cuda_time_total\",\n",
        "        row_limit=10\n",
        "    ))\n",
        "    \n",
        "    print(\"\\nüí° Profiling insights:\")\n",
        "    print(\"   - Look for operations with high 'CUDA time'\")\n",
        "    print(\"   - Focus optimization on top 3-5 operations\")\n",
        "    print(\"   - Check 'Self CUDA %' to find actual bottlenecks\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Skipping profiling - requires CUDA\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 7. Complete Optimization Stack\n",
        "\n",
        "### Combining All Techniques\n",
        "\n",
        "Let's build a fully optimized training loop using:\n",
        "1. torch.compile\n",
        "2. Mixed precision (AMP)\n",
        "3. Gradient accumulation\n",
        "4. Efficient data loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class OptimizedTransformer(nn.Module):\n",
        "    \"\"\"Fully optimized transformer for training\"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model: int,\n",
        "        num_heads: int,\n",
        "        d_ff: int,\n",
        "        num_layers: int\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([\n",
        "            TransformerBlock(d_model, num_heads, d_ff)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "    \n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return self.norm(x)\n",
        "\n",
        "\n",
        "def optimized_training_loop(\n",
        "    model: nn.Module,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    num_steps: int = 100,\n",
        "    batch_size: int = 32,\n",
        "    seq_len: int = 128,\n",
        "    use_compile: bool = True,\n",
        "    use_amp: bool = True,\n",
        "    gradient_accumulation_steps: int = 1\n",
        "):\n",
        "    \"\"\"Fully optimized training loop\"\"\"\n",
        "    \n",
        "    # Compile model if requested\n",
        "    if use_compile and compile_available:\n",
        "        print(\"Compiling model...\")\n",
        "        model = torch.compile(model, mode=\"reduce-overhead\")\n",
        "    \n",
        "    # Create GradScaler for AMP\n",
        "    scaler = torch.amp.GradScaler('cuda') if use_amp and device.type == 'cuda' else None\n",
        "    \n",
        "    model.train()\n",
        "    total_time = 0\n",
        "    \n",
        "    for step in range(num_steps):\n",
        "        step_start = time.time()\n",
        "        \n",
        "        # Simulate data loading\n",
        "        x = torch.randn(batch_size, seq_len, d_model, device=device)\n",
        "        target = torch.randn(batch_size, seq_len, d_model, device=device)\n",
        "        \n",
        "        # Forward pass with AMP\n",
        "        if use_amp and device.type == 'cuda':\n",
        "            with torch.amp.autocast('cuda', dtype=torch.float16):\n",
        "                output = model(x)\n",
        "                loss = F.mse_loss(output, target)\n",
        "                loss = loss / gradient_accumulation_steps\n",
        "            \n",
        "            # Backward with gradient scaling\n",
        "            scaler.scale(loss).backward()\n",
        "            \n",
        "            # Update weights every N steps\n",
        "            if (step + 1) % gradient_accumulation_steps == 0:\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "                optimizer.zero_grad()\n",
        "        else:\n",
        "            # Standard training\n",
        "            output = model(x)\n",
        "            loss = F.mse_loss(output, target)\n",
        "            loss = loss / gradient_accumulation_steps\n",
        "            \n",
        "            loss.backward()\n",
        "            \n",
        "            if (step + 1) % gradient_accumulation_steps == 0:\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "        \n",
        "        if device.type == 'cuda':\n",
        "            torch.cuda.synchronize()\n",
        "        \n",
        "        step_time = time.time() - step_start\n",
        "        if step >= 10:  # Skip warmup\n",
        "            total_time += step_time\n",
        "        \n",
        "        if (step + 1) % 25 == 0:\n",
        "            avg_time = total_time / (step - 9) * 1000 if step > 9 else 0\n",
        "            print(f\"Step {step+1}/{num_steps}: Loss={loss.item():.4f}, \"\n",
        "                  f\"Time={step_time*1000:.2f}ms, Avg={avg_time:.2f}ms\")\n",
        "    \n",
        "    avg_time = total_time / (num_steps - 10)\n",
        "    return avg_time\n",
        "\n",
        "\n",
        "if device.type == 'cuda' and compile_available:\n",
        "    print(\"\\nComprehensive Optimization Benchmark\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    # Create model\n",
        "    d_model = 512\n",
        "    num_heads = 8\n",
        "    d_ff = 2048\n",
        "    num_layers = 4\n",
        "    \n",
        "    configs = [\n",
        "        (\"Baseline (no opt)\", False, False),\n",
        "        (\"AMP only\", False, True),\n",
        "        (\"Compile only\", True, False),\n",
        "        (\"AMP + Compile\", True, True),\n",
        "    ]\n",
        "    \n",
        "    results = []\n",
        "    \n",
        "    for name, use_compile, use_amp in configs:\n",
        "        print(f\"\\n{name}:\")\n",
        "        print(\"-\" * 70)\n",
        "        \n",
        "        model = OptimizedTransformer(d_model, num_heads, d_ff, num_layers).to(device)\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
        "        \n",
        "        avg_time = optimized_training_loop(\n",
        "            model, optimizer,\n",
        "            num_steps=50,\n",
        "            use_compile=use_compile,\n",
        "            use_amp=use_amp\n",
        "        )\n",
        "        \n",
        "        results.append((name, avg_time * 1000))\n",
        "        print(f\"\\n‚úì Average time: {avg_time*1000:.2f}ms/step\")\n",
        "    \n",
        "    # Visualize results\n",
        "    names = [r[0] for r in results]\n",
        "    times = [r[1] for r in results]\n",
        "    baseline = times[0]\n",
        "    speedups = [baseline / t for t in times]\n",
        "    \n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    \n",
        "    # Plot 1: Absolute times\n",
        "    colors = ['red', 'orange', 'blue', 'green']\n",
        "    axes[0].barh(names, times, color=colors, alpha=0.7)\n",
        "    axes[0].set_xlabel('Time (ms/step)', fontsize=12)\n",
        "    axes[0].set_title('Training Time by Configuration', fontsize=14)\n",
        "    axes[0].grid(True, alpha=0.3, axis='x')\n",
        "    \n",
        "    # Plot 2: Speedups\n",
        "    axes[1].barh(names, speedups, color=colors, alpha=0.7)\n",
        "    axes[1].axvline(x=1.0, color='r', linestyle='--', label='Baseline')\n",
        "    axes[1].set_xlabel('Speedup (x)', fontsize=12)\n",
        "    axes[1].set_title('Speedup vs Baseline', fontsize=14)\n",
        "    axes[1].legend()\n",
        "    axes[1].grid(True, alpha=0.3, axis='x')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"Summary:\")\n",
        "    for i, (name, time) in enumerate(results):\n",
        "        print(f\"  {name}: {time:.2f}ms ({speedups[i]:.2f}x)\")\n",
        "    \n",
        "    best_speedup = max(speedups)\n",
        "    print(f\"\\nüí° Best configuration: {best_speedup:.2f}x faster than baseline!\")\n",
        "    print(\"   Always use AMP + compile for training.\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Skipping comprehensive benchmark - requires CUDA and PyTorch 2.0+\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Mini Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 1: Benchmark Custom Model\n",
        "\n",
        "Create your own model and benchmark eager vs compiled performance.\n",
        "Include at least 5 layers with different operations (Linear, Conv, ReLU, etc.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your code here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Solution\n",
        "class MyCustomModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv1d(512, 512, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv1d(512, 512, kernel_size=3, padding=1)\n",
        "        self.linear1 = nn.Linear(512, 1024)\n",
        "        self.linear2 = nn.Linear(1024, 512)\n",
        "        self.norm = nn.LayerNorm(512)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # x: (batch, seq_len, 512)\n",
        "        x_t = x.transpose(1, 2)  # (batch, 512, seq_len)\n",
        "        x_t = F.relu(self.conv1(x_t))\n",
        "        x_t = F.relu(self.conv2(x_t))\n",
        "        x = x_t.transpose(1, 2)  # (batch, seq_len, 512)\n",
        "        x = F.relu(self.linear1(x))\n",
        "        x = self.linear2(x)\n",
        "        x = self.norm(x)\n",
        "        return x\n",
        "\n",
        "if compile_available and device.type == 'cuda':\n",
        "    model_eager = MyCustomModel().to(device)\n",
        "    model_compiled = torch.compile(MyCustomModel().to(device))\n",
        "    \n",
        "    x = torch.randn(32, 128, 512, device=device)\n",
        "    \n",
        "    eager_time = benchmark_model(model_eager, x)\n",
        "    compiled_time = benchmark_model(model_compiled, x)\n",
        "    \n",
        "    print(f\"Custom Model Benchmark:\")\n",
        "    print(f\"  Eager: {eager_time*1000:.2f}ms\")\n",
        "    print(f\"  Compiled: {compiled_time*1000:.2f}ms\")\n",
        "    print(f\"  Speedup: {eager_time/compiled_time:.2f}x\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Requires CUDA and PyTorch 2.0+\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 2: Profile a Bottleneck\n",
        "\n",
        "Create a model with an intentional bottleneck and use torch.profiler to find it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your code here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Solution\n",
        "class BottleneckModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fast = nn.Linear(512, 512)\n",
        "        # Intentional bottleneck: very large intermediate\n",
        "        self.slow = nn.Linear(512, 8192)\n",
        "        self.output = nn.Linear(8192, 512)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.fast(x)  # Fast operation\n",
        "        x = self.slow(x)  # Bottleneck!\n",
        "        x = F.relu(x)\n",
        "        x = self.output(x)  # Bottleneck!\n",
        "        return x\n",
        "\n",
        "if device.type == 'cuda':\n",
        "    from torch.profiler import profile, ProfilerActivity\n",
        "    \n",
        "    model = BottleneckModel().to(device)\n",
        "    x = torch.randn(32, 128, 512, device=device)\n",
        "    \n",
        "    with profile(activities=[ProfilerActivity.CUDA]) as prof:\n",
        "        output = model(x)\n",
        "    \n",
        "    print(\"Top operations by CUDA time:\")\n",
        "    print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=5))\n",
        "    print(\"\\nüí° The 8192-dimensional layers are the bottleneck!\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Requires CUDA\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 3: Compare Data Types\n",
        "\n",
        "Benchmark float32, float16, and bfloat16 (if available) for the same model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your code here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Solution\n",
        "if device.type == 'cuda':\n",
        "    model = TransformerBlock(512, 8, 2048).to(device)\n",
        "    x = torch.randn(32, 128, 512, device=device)\n",
        "    target = torch.randn(32, 128, 512, device=device)\n",
        "    optimizer = torch.optim.Adam(model.parameters())\n",
        "    \n",
        "    dtypes = []\n",
        "    if torch.cuda.is_available():\n",
        "        dtypes.append((\"float32\", None))\n",
        "        dtypes.append((\"float16\", torch.float16))\n",
        "        # Check if bfloat16 is supported\n",
        "        if torch.cuda.is_bf16_supported():\n",
        "            dtypes.append((\"bfloat16\", torch.bfloat16))\n",
        "    \n",
        "    print(\"Data Type Comparison:\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    results = []\n",
        "    for name, dtype in dtypes:\n",
        "        # Warmup\n",
        "        for _ in range(5):\n",
        "            if dtype:\n",
        "                with torch.amp.autocast('cuda', dtype=dtype):\n",
        "                    out = model(x)\n",
        "            else:\n",
        "                out = model(x)\n",
        "        \n",
        "        # Benchmark\n",
        "        torch.cuda.synchronize()\n",
        "        start = time.time()\n",
        "        for _ in range(50):\n",
        "            if dtype:\n",
        "                with torch.amp.autocast('cuda', dtype=dtype):\n",
        "                    out = model(x)\n",
        "            else:\n",
        "                out = model(x)\n",
        "        torch.cuda.synchronize()\n",
        "        elapsed = (time.time() - start) / 50 * 1000\n",
        "        \n",
        "        results.append((name, elapsed))\n",
        "        print(f\"  {name}: {elapsed:.2f}ms\")\n",
        "    \n",
        "    baseline = results[0][1]\n",
        "    print(f\"\\nSpeedups vs float32:\")\n",
        "    for name, time in results:\n",
        "        speedup = baseline / time\n",
        "        print(f\"  {name}: {speedup:.2f}x\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Requires CUDA\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Key Takeaways\n",
        "\n",
        "1. **torch.compile is free speedup**: 30-50% faster with one line\n",
        "2. **Use reduce-overhead mode**: Best for inference and small operations\n",
        "3. **Mixed precision (AMP) is essential**: 2-3x faster, half the memory\n",
        "4. **Combine techniques**: AMP + compile = maximum performance\n",
        "5. **Profile to find bottlenecks**: Don't guess, measure\n",
        "6. **bfloat16 > float16**: Better numerical stability for training\n",
        "7. **Compilation takes time**: First run is slow, subsequent runs are fast\n",
        "\n",
        "## When to Use Each Technique\n",
        "\n",
        "**torch.compile**:\n",
        "- ‚úÖ Training large models\n",
        "- ‚úÖ Inference (especially small batches)\n",
        "- ‚úÖ Models with many small ops\n",
        "- ‚ùå Highly dynamic models (lots of control flow)\n",
        "\n",
        "**Mixed Precision (AMP)**:\n",
        "- ‚úÖ Always for training (unless debugging)\n",
        "- ‚úÖ Inference on GPU\n",
        "- ‚úÖ Models that fit in memory\n",
        "- ‚ùå CPU inference\n",
        "\n",
        "**Profiling**:\n",
        "- ‚úÖ When optimizing bottlenecks\n",
        "- ‚úÖ Before major refactoring\n",
        "- ‚úÖ Comparing implementations\n",
        "- ‚ùå Every training run (overhead)\n",
        "\n",
        "## Modern LLM Usage (2025)\n",
        "\n",
        "**LLaMA 2/3**:\n",
        "- torch.compile for training\n",
        "- bfloat16 mixed precision\n",
        "- Regional compilation for custom kernels\n",
        "\n",
        "**Mistral**:\n",
        "- Compiled inference pipelines\n",
        "- float16 for deployment\n",
        "- Custom Triton kernels for attention\n",
        "\n",
        "**Your Production Models**:\n",
        "- Always compile for inference\n",
        "- Always use AMP for training\n",
        "- Profile before optimizing\n",
        "- Use bfloat16 on A100/H100\n",
        "\n",
        "---\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "Continue to: [Topic 15: Production PyTorch Best Practices](15_production_pytorch.ipynb)\n",
        "\n",
        "---\n",
        "\n",
        "## Further Reading\n",
        "\n",
        "- [PyTorch 2.0 Release Notes](https://pytorch.org/get-started/pytorch-2.0/)\n",
        "- [torch.compile Tutorial](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html)\n",
        "- [Automatic Mixed Precision Guide](https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html)\n",
        "- [torch.profiler Documentation](https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html)\n",
        "- [TorchInductor Deep Dive](https://dev-discuss.pytorch.org/t/torchinductor-a-pytorch-native-compiler-with-define-by-run-ir-and-symbolic-shapes/747)\n",
        "- [bfloat16 for Deep Learning](https://arxiv.org/abs/1905.12322)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
