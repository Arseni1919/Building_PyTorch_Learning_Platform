{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Topic 13: Mixture of Experts (MoE) - Scaling Models with Sparse Computation\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this notebook, you will:\n",
        "- Understand why sparse MoE models scale better than dense models\n",
        "- Learn different routing mechanisms (top-k, softmax, expert choice)\n",
        "- Implement MoE from scratch with full technical detail\n",
        "- Master load balancing techniques and auxiliary losses\n",
        "- Know how to handle routing collapse and expert specialization\n",
        "- Understand MoE in GPT-4, DeepSeek-V3, and Mixtral\n",
        "- Learn expert parallelism strategies for distributed training\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. The Big Picture: Why Mixture of Experts?\n",
        "\n",
        "### The Scaling Problem with Dense Models\n",
        "\n",
        "**Traditional approach**: To improve model quality, increase parameters\n",
        "- GPT-2: 1.5B parameters\n",
        "- GPT-3: 175B parameters\n",
        "- GPT-4: ~1.8T parameters (rumored)\n",
        "\n",
        "**Problem**: Every token activates **all parameters**\n",
        "- 175B model: Every forward pass uses all 175B params\n",
        "- Compute scales linearly with parameters\n",
        "- Memory bandwidth becomes bottleneck\n",
        "- Inference cost is prohibitive\n",
        "\n",
        "### The Sparse Solution: Mixture of Experts\n",
        "\n",
        "**Key insight**: Not all parameters are needed for every input!\n",
        "\n",
        "**MoE approach**:\n",
        "1. Split model into **expert sub-networks**\n",
        "2. Use **router** to select which experts to activate\n",
        "3. Each token only uses **subset of total parameters**\n",
        "\n",
        "**Example**: 8 experts, top-2 routing\n",
        "- Total parameters: 8 Ã— expert_size\n",
        "- Active parameters per token: 2 Ã— expert_size\n",
        "- **Compute**: Same as 2-expert model\n",
        "- **Capacity**: Same as 8-expert model\n",
        "\n",
        "### The Magic: Decoupling Parameters from Compute\n",
        "\n",
        "```\n",
        "Dense Model:\n",
        "  Parameters: 175B\n",
        "  Compute per token: 175B FLOPs\n",
        "  Scaling: Linear\n",
        "\n",
        "MoE Model (8 experts, top-2):\n",
        "  Parameters: 1.8T (10x more!)\n",
        "  Compute per token: ~450B FLOPs (2.5x, not 10x!)\n",
        "  Scaling: Sub-linear!\n",
        "```\n",
        "\n",
        "**Result**: 10x more parameters, only 2.5x more compute!\n",
        "\n",
        "### Real-World Impact\n",
        "\n",
        "**GPT-4** (rumored architecture):\n",
        "- 16 experts, ~1.8T total parameters\n",
        "- Top-2 routing\n",
        "- ~220B active parameters per token\n",
        "- Same compute as dense 220B model\n",
        "- Quality of 1.8T model!\n",
        "\n",
        "**DeepSeek-V3** (2025):\n",
        "- 256 experts, 671B total parameters\n",
        "- Top-8 routing with expert specialization\n",
        "- ~37B active parameters per token\n",
        "- Achieves GPT-4 level quality at fraction of cost\n",
        "\n",
        "**Mixtral 8x7B**:\n",
        "- 8 experts, 47B total parameters\n",
        "- Top-2 routing\n",
        "- ~13B active parameters per token\n",
        "- Outperforms Llama 2 70B while being 5x smaller active size\n",
        "\n",
        "### The Trade-offs\n",
        "\n",
        "**Advantages**:\n",
        "- ðŸ’š **Scaling efficiency**: Sub-linear compute growth\n",
        "- ðŸ’š **Specialization**: Experts learn different skills\n",
        "- ðŸ’š **Quality**: Better than dense models at same compute\n",
        "\n",
        "**Challenges**:\n",
        "- âš ï¸ **Load balancing**: Experts must be used evenly\n",
        "- âš ï¸ **Routing collapse**: Router may only use few experts\n",
        "- âš ï¸ **Memory**: Total parameters still take memory\n",
        "- âš ï¸ **Communication**: Expert parallelism needs fast interconnect\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from typing import Tuple, Optional, List\n",
        "import seaborn as sns\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Set style for better plots\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 2. MoE Architecture: Components\n",
        "\n",
        "### Core Components\n",
        "\n",
        "An MoE layer consists of three parts:\n",
        "\n",
        "```\n",
        "Input (batch, seq_len, d_model)\n",
        "       â†“\n",
        "1. ROUTER: Scores experts for each token\n",
        "   â†’ Router weights (batch, seq_len, num_experts)\n",
        "       â†“\n",
        "2. TOP-K SELECTION: Choose top-k experts per token\n",
        "   â†’ Selected experts + routing weights\n",
        "       â†“\n",
        "3. EXPERTS: Parallel feedforward networks\n",
        "   â†’ Expert outputs combined by routing weights\n",
        "       â†“\n",
        "Output (batch, seq_len, d_model)\n",
        "```\n",
        "\n",
        "### Mathematical Formulation\n",
        "\n",
        "For input token $x \\in \\mathbb{R}^{d_{model}}$:\n",
        "\n",
        "**1. Router logits**:\n",
        "$$\n",
        "h = x W_g \\quad \\text{where } W_g \\in \\mathbb{R}^{d_{model} \\times n_{experts}}\n",
        "$$\n",
        "\n",
        "**2. Top-k selection**:\n",
        "$$\n",
        "\\text{TopK}(h) = \\{e_1, ..., e_k\\} \\quad \\text{(indices of top k experts)}\n",
        "$$\n",
        "\n",
        "**3. Routing weights** (softmax over selected):\n",
        "$$\n",
        "p_i = \\frac{\\exp(h_{e_i})}{\\sum_{j=1}^k \\exp(h_{e_j})} \\quad \\text{for } i \\in \\{1, ..., k\\}\n",
        "$$\n",
        "\n",
        "**4. Expert outputs**:\n",
        "$$\n",
        "E_i(x) = \\text{Expert}_i(x) \\quad \\text{(feedforward network)}\n",
        "$$\n",
        "\n",
        "**5. Final output**:\n",
        "$$\n",
        "y = \\sum_{i=1}^k p_i \\cdot E_{e_i}(x)\n",
        "$$\n",
        "\n",
        "### Design Choices\n",
        "\n",
        "**Number of experts**:\n",
        "- More experts = more capacity, harder to load balance\n",
        "- Common: 8 (Mixtral), 16 (GPT-4?), 64-256 (DeepSeek-V3)\n",
        "\n",
        "**Top-k value**:\n",
        "- k=1: Fastest, but routing collapse risk\n",
        "- k=2: Most common, good balance (Mixtral, GPT-4)\n",
        "- k=8: Used by DeepSeek-V3 for better quality\n",
        "\n",
        "**Expert architecture**:\n",
        "- Typically FFN layers (same as transformer FFN)\n",
        "- Can use different activations (SwiGLU common)\n",
        "- Size: Usually same as dense FFN per expert\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Implementing Experts and Router\n",
        "\n",
        "### Step 1: Expert Network\n",
        "\n",
        "Each expert is a standard feedforward network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Expert(nn.Module):\n",
        "    \"\"\"Single expert: a feedforward network\"\"\"\n",
        "    \n",
        "    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.w1 = nn.Linear(d_model, d_ff)\n",
        "        self.w2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (*, d_model)\n",
        "        Returns:\n",
        "            (*, d_model)\n",
        "        \"\"\"\n",
        "        # Standard FFN: Linear â†’ ReLU â†’ Dropout â†’ Linear\n",
        "        return self.w2(self.dropout(F.relu(self.w1(x))))\n",
        "\n",
        "\n",
        "class SwiGLUExpert(nn.Module):\n",
        "    \"\"\"Expert using SwiGLU activation (used in LLaMA, Mixtral)\"\"\"\n",
        "    \n",
        "    def __init__(self, d_model: int, d_ff: int):\n",
        "        super().__init__()\n",
        "        # SwiGLU needs 3 weight matrices\n",
        "        self.w1 = nn.Linear(d_model, d_ff, bias=False)\n",
        "        self.w2 = nn.Linear(d_ff, d_model, bias=False)\n",
        "        self.w3 = nn.Linear(d_model, d_ff, bias=False)\n",
        "    \n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        SwiGLU(x) = (Swish(W1Â·x) âŠ™ W3Â·x) W2\n",
        "        \"\"\"\n",
        "        return self.w2(F.silu(self.w1(x)) * self.w3(x))\n",
        "\n",
        "\n",
        "# Demo\n",
        "d_model = 512\n",
        "d_ff = 2048\n",
        "\n",
        "expert_relu = Expert(d_model, d_ff).to(device)\n",
        "expert_swiglu = SwiGLUExpert(d_model, d_ff).to(device)\n",
        "\n",
        "x = torch.randn(4, 128, d_model, device=device)\n",
        "\n",
        "out_relu = expert_relu(x)\n",
        "out_swiglu = expert_swiglu(x)\n",
        "\n",
        "print(\"Expert Networks\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Input: {x.shape}\")\n",
        "print(f\"ReLU Expert output: {out_relu.shape}\")\n",
        "print(f\"SwiGLU Expert output: {out_swiglu.shape}\")\n",
        "print(f\"\\nReLU Expert params: {sum(p.numel() for p in expert_relu.parameters()):,}\")\n",
        "print(f\"SwiGLU Expert params: {sum(p.numel() for p in expert_swiglu.parameters()):,}\")\n",
        "print(\"\\nðŸ’¡ SwiGLU has ~1.5x more parameters but better quality\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Router\n",
        "\n",
        "The router decides which experts to use for each token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TopKRouter(nn.Module):\n",
        "    \"\"\"Top-k router: selects k experts per token\"\"\"\n",
        "    \n",
        "    def __init__(self, d_model: int, num_experts: int, top_k: int = 2):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_experts = num_experts\n",
        "        self.top_k = top_k\n",
        "        \n",
        "        # Router is a simple linear layer\n",
        "        self.gate = nn.Linear(d_model, num_experts, bias=False)\n",
        "    \n",
        "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (batch, seq_len, d_model)\n",
        "        \n",
        "        Returns:\n",
        "            expert_indices: (batch, seq_len, top_k) - which experts to use\n",
        "            expert_weights: (batch, seq_len, top_k) - how much to weight each\n",
        "            router_logits: (batch, seq_len, num_experts) - raw logits (for aux loss)\n",
        "        \"\"\"\n",
        "        # Compute router logits\n",
        "        router_logits = self.gate(x)  # (batch, seq_len, num_experts)\n",
        "        \n",
        "        # Get top-k experts\n",
        "        top_k_logits, expert_indices = torch.topk(router_logits, self.top_k, dim=-1)\n",
        "        # expert_indices: (batch, seq_len, top_k)\n",
        "        # top_k_logits: (batch, seq_len, top_k)\n",
        "        \n",
        "        # Softmax over selected experts to get weights\n",
        "        expert_weights = F.softmax(top_k_logits, dim=-1)\n",
        "        # expert_weights: (batch, seq_len, top_k)\n",
        "        \n",
        "        return expert_indices, expert_weights, router_logits\n",
        "\n",
        "\n",
        "# Demo router\n",
        "num_experts = 8\n",
        "top_k = 2\n",
        "\n",
        "router = TopKRouter(d_model, num_experts, top_k).to(device)\n",
        "x = torch.randn(4, 128, d_model, device=device)\n",
        "\n",
        "expert_indices, expert_weights, router_logits = router(x)\n",
        "\n",
        "print(\"\\nTop-K Router\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Input: {x.shape}\")\n",
        "print(f\"Router logits: {router_logits.shape}\")\n",
        "print(f\"Selected experts: {expert_indices.shape}\")\n",
        "print(f\"Expert weights: {expert_weights.shape}\")\n",
        "\n",
        "print(f\"\\nExample token routing:\")\n",
        "print(f\"  Token 0, sample 0:\")\n",
        "print(f\"    Selected experts: {expert_indices[0, 0].cpu().numpy()}\")\n",
        "print(f\"    Expert weights: {expert_weights[0, 0].cpu().numpy()}\")\n",
        "print(f\"    Weights sum to: {expert_weights[0, 0].sum().item():.4f}\")\n",
        "\n",
        "# Analyze routing distribution\n",
        "expert_usage = torch.zeros(num_experts, device=device)\n",
        "for i in range(num_experts):\n",
        "    expert_usage[i] = (expert_indices == i).sum().item()\n",
        "\n",
        "expert_usage = expert_usage / expert_usage.sum() * 100  # Convert to percentage\n",
        "\n",
        "print(f\"\\nExpert usage distribution:\")\n",
        "for i in range(num_experts):\n",
        "    print(f\"  Expert {i}: {expert_usage[i].item():.1f}%\")\n",
        "print(f\"\\nâš ï¸ Ideally all experts should be used ~equally ({100/num_experts:.1f}% each)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 4. Complete MoE Layer Implementation\n",
        "\n",
        "Now let's combine router and experts into a full MoE layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MixtureOfExpertsLayer(nn.Module):\n",
        "    \"\"\"Complete Mixture of Experts layer\"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model: int,\n",
        "        d_ff: int,\n",
        "        num_experts: int,\n",
        "        top_k: int = 2,\n",
        "        expert_type: str = \"relu\",  # \"relu\" or \"swiglu\"\n",
        "        dropout: float = 0.1\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_experts = num_experts\n",
        "        self.top_k = top_k\n",
        "        \n",
        "        # Router\n",
        "        self.router = TopKRouter(d_model, num_experts, top_k)\n",
        "        \n",
        "        # Experts\n",
        "        if expert_type == \"relu\":\n",
        "            self.experts = nn.ModuleList([\n",
        "                Expert(d_model, d_ff, dropout) for _ in range(num_experts)\n",
        "            ])\n",
        "        elif expert_type == \"swiglu\":\n",
        "            self.experts = nn.ModuleList([\n",
        "                SwiGLUExpert(d_model, d_ff) for _ in range(num_experts)\n",
        "            ])\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown expert_type: {expert_type}\")\n",
        "    \n",
        "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, dict]:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (batch, seq_len, d_model)\n",
        "        \n",
        "        Returns:\n",
        "            output: (batch, seq_len, d_model)\n",
        "            aux_info: Dict with routing statistics for load balancing\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, d_model = x.shape\n",
        "        \n",
        "        # 1. Route tokens to experts\n",
        "        expert_indices, expert_weights, router_logits = self.router(x)\n",
        "        # expert_indices: (batch, seq_len, top_k)\n",
        "        # expert_weights: (batch, seq_len, top_k)\n",
        "        \n",
        "        # 2. Flatten for easier processing\n",
        "        x_flat = x.view(-1, d_model)  # (batch * seq_len, d_model)\n",
        "        expert_indices_flat = expert_indices.view(-1, self.top_k)  # (batch * seq_len, top_k)\n",
        "        expert_weights_flat = expert_weights.view(-1, self.top_k)  # (batch * seq_len, top_k)\n",
        "        \n",
        "        # 3. Initialize output\n",
        "        output_flat = torch.zeros_like(x_flat)\n",
        "        \n",
        "        # 4. Process each expert\n",
        "        # This is the naive implementation - see later for optimized version\n",
        "        for expert_idx in range(self.num_experts):\n",
        "            # Find all tokens that selected this expert\n",
        "            expert_mask = (expert_indices_flat == expert_idx)  # (batch * seq_len, top_k)\n",
        "            \n",
        "            # Get tokens assigned to this expert\n",
        "            token_expert_indices = expert_mask.any(dim=-1)  # (batch * seq_len,)\n",
        "            \n",
        "            if token_expert_indices.any():\n",
        "                # Get tokens for this expert\n",
        "                expert_input = x_flat[token_expert_indices]  # (num_tokens, d_model)\n",
        "                \n",
        "                # Process through expert\n",
        "                expert_output = self.experts[expert_idx](expert_input)\n",
        "                \n",
        "                # Get weights for this expert's contribution\n",
        "                # For each token, find which position in top_k this expert was\n",
        "                expert_weights_for_tokens = torch.zeros(\n",
        "                    token_expert_indices.sum(), device=x.device\n",
        "                )\n",
        "                \n",
        "                expert_weights_full = expert_weights_flat[token_expert_indices]  # (num_tokens, top_k)\n",
        "                expert_mask_full = expert_mask[token_expert_indices]  # (num_tokens, top_k)\n",
        "                \n",
        "                # Get weight where this expert was selected\n",
        "                for i in range(self.top_k):\n",
        "                    mask = expert_mask_full[:, i]\n",
        "                    expert_weights_for_tokens[mask] = expert_weights_full[mask, i]\n",
        "                \n",
        "                # Weighted contribution\n",
        "                output_flat[token_expert_indices] += expert_output * expert_weights_for_tokens.unsqueeze(-1)\n",
        "        \n",
        "        # 5. Reshape back\n",
        "        output = output_flat.view(batch_size, seq_len, d_model)\n",
        "        \n",
        "        # 6. Collect auxiliary information for load balancing\n",
        "        aux_info = {\n",
        "            'router_logits': router_logits,\n",
        "            'expert_indices': expert_indices,\n",
        "            'expert_weights': expert_weights\n",
        "        }\n",
        "        \n",
        "        return output, aux_info\n",
        "\n",
        "\n",
        "# Demo MoE layer\n",
        "print(\"\\nMixture of Experts Layer\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "d_model = 512\n",
        "d_ff = 2048\n",
        "num_experts = 8\n",
        "top_k = 2\n",
        "\n",
        "moe = MixtureOfExpertsLayer(\n",
        "    d_model, d_ff, num_experts, top_k, expert_type=\"relu\"\n",
        ").to(device)\n",
        "\n",
        "batch_size = 4\n",
        "seq_len = 128\n",
        "x = torch.randn(batch_size, seq_len, d_model, device=device)\n",
        "\n",
        "output, aux_info = moe(x)\n",
        "\n",
        "print(f\"Input: {x.shape}\")\n",
        "print(f\"Output: {output.shape}\")\n",
        "print(f\"\\nAuxiliary info keys: {list(aux_info.keys())}\")\n",
        "\n",
        "# Analyze expert usage\n",
        "expert_indices = aux_info['expert_indices']\n",
        "expert_usage = torch.zeros(num_experts, device=device)\n",
        "for i in range(num_experts):\n",
        "    expert_usage[i] = (expert_indices == i).sum().item()\n",
        "\n",
        "total_assignments = expert_usage.sum()\n",
        "expert_usage_pct = expert_usage / total_assignments * 100\n",
        "\n",
        "print(f\"\\nExpert usage distribution:\")\n",
        "for i in range(num_experts):\n",
        "    bar = 'â–ˆ' * int(expert_usage_pct[i].item() / 2)\n",
        "    print(f\"  Expert {i}: {expert_usage_pct[i].item():5.1f}% {bar}\")\n",
        "\n",
        "ideal_pct = 100 / num_experts\n",
        "print(f\"\\nðŸ’¡ Ideal: {ideal_pct:.1f}% per expert\")\n",
        "print(f\"   Actual variance: {expert_usage_pct.std().item():.2f}\")\n",
        "\n",
        "# Calculate parameters\n",
        "total_params = sum(p.numel() for p in moe.parameters())\n",
        "dense_equivalent = Expert(d_model, d_ff).to(device)\n",
        "dense_params = sum(p.numel() for p in dense_equivalent.parameters())\n",
        "\n",
        "print(f\"\\nParameter comparison:\")\n",
        "print(f\"  MoE total params: {total_params:,}\")\n",
        "print(f\"  Dense FFN params: {dense_params:,}\")\n",
        "print(f\"  MoE / Dense ratio: {total_params / dense_params:.1f}x\")\n",
        "print(f\"\\n  Active params per token: ~{2 * dense_params:,} ({top_k} experts)\")\n",
        "print(f\"  Total capacity: {num_experts}x dense model\")\n",
        "print(f\"  Compute cost: {top_k}x dense model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 5. The Load Balancing Problem\n",
        "\n",
        "### What is Load Imbalance?\n",
        "\n",
        "**Problem**: Router may send all tokens to a few popular experts\n",
        "- Expert 0: 80% of tokens\n",
        "- Expert 1: 15% of tokens\n",
        "- Experts 2-7: 5% of tokens total\n",
        "\n",
        "**Why this is bad**:\n",
        "1. **Underutilization**: Most experts learn nothing\n",
        "2. **Bottleneck**: Popular experts become bottleneck\n",
        "3. **Capacity waste**: Paying for 8 experts, using 2\n",
        "4. **Routing collapse**: Eventually all tokens go to one expert\n",
        "\n",
        "### Solution: Auxiliary Load Balancing Loss\n",
        "\n",
        "**Approach**: Add penalty when experts are used unevenly\n",
        "\n",
        "#### Load Balancing Loss (Switch Transformer)\n",
        "\n",
        "For $N$ tokens and $E$ experts:\n",
        "\n",
        "**1. Fraction of tokens assigned to each expert**:\n",
        "$$\n",
        "f_i = \\frac{1}{N} \\sum_{j=1}^N \\mathbb{1}[\\arg\\max(h_j) = i]\n",
        "$$\n",
        "\n",
        "**2. Router probability mass for each expert**:\n",
        "$$\n",
        "P_i = \\frac{1}{N} \\sum_{j=1}^N p_{j,i}\n",
        "$$\n",
        "where $p_{j,i}$ is the router probability for expert $i$ on token $j$\n",
        "\n",
        "**3. Load balancing loss**:\n",
        "$$\n",
        "\\mathcal{L}_{bal} = E \\cdot \\sum_{i=1}^E f_i \\cdot P_i\n",
        "$$\n",
        "\n",
        "**Why this works**:\n",
        "- If expert $i$ gets many tokens ($f_i$ high) AND router gives it high probability ($P_i$ high), loss is high\n",
        "- Minimizing this loss encourages even distribution\n",
        "- Multiplier $E$ scales loss to be invariant to number of experts\n",
        "\n",
        "**Total training loss**:\n",
        "$$\n",
        "\\mathcal{L}_{total} = \\mathcal{L}_{task} + \\alpha \\cdot \\mathcal{L}_{bal}\n",
        "$$\n",
        "where $\\alpha$ is typically 0.01 to 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_balancing_loss(router_logits: torch.Tensor, expert_indices: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Compute load balancing auxiliary loss (Switch Transformer)\n",
        "    \n",
        "    Args:\n",
        "        router_logits: (batch, seq_len, num_experts) - raw router logits\n",
        "        expert_indices: (batch, seq_len, top_k) - selected expert indices\n",
        "    \n",
        "    Returns:\n",
        "        loss: Scalar load balancing loss\n",
        "    \"\"\"\n",
        "    batch_size, seq_len, num_experts = router_logits.shape\n",
        "    top_k = expert_indices.shape[-1]\n",
        "    \n",
        "    # 1. Router probabilities (softmax over all experts)\n",
        "    router_probs = F.softmax(router_logits, dim=-1)  # (batch, seq_len, num_experts)\n",
        "    \n",
        "    # 2. Average probability per expert (P_i)\n",
        "    P = router_probs.mean(dim=[0, 1])  # (num_experts,)\n",
        "    \n",
        "    # 3. Fraction of tokens assigned to each expert (f_i)\n",
        "    # Create one-hot encoding of expert assignments\n",
        "    num_tokens = batch_size * seq_len * top_k\n",
        "    expert_mask = F.one_hot(expert_indices, num_experts).float()  # (batch, seq_len, top_k, num_experts)\n",
        "    expert_mask = expert_mask.sum(dim=2)  # Sum over top_k: (batch, seq_len, num_experts)\n",
        "    f = expert_mask.sum(dim=[0, 1]) / num_tokens  # (num_experts,)\n",
        "    \n",
        "    # 4. Load balancing loss: num_experts * sum(f_i * P_i)\n",
        "    loss = num_experts * (f * P).sum()\n",
        "    \n",
        "    return loss\n",
        "\n",
        "\n",
        "# Demo load balancing loss\n",
        "print(\"\\nLoad Balancing Loss\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Simulate balanced vs imbalanced routing\n",
        "batch, seq_len, num_experts = 4, 128, 8\n",
        "top_k = 2\n",
        "\n",
        "# Case 1: Balanced routing (all experts used equally)\n",
        "balanced_logits = torch.randn(batch, seq_len, num_experts)\n",
        "balanced_indices = torch.randint(0, num_experts, (batch, seq_len, top_k))\n",
        "\n",
        "loss_balanced = load_balancing_loss(balanced_logits, balanced_indices)\n",
        "\n",
        "# Case 2: Imbalanced routing (favor first 2 experts)\n",
        "imbalanced_logits = torch.randn(batch, seq_len, num_experts)\n",
        "imbalanced_logits[..., :2] += 3.0  # Boost first 2 experts\n",
        "imbalanced_indices = torch.randint(0, 2, (batch, seq_len, top_k))  # Only use first 2\n",
        "\n",
        "loss_imbalanced = load_balancing_loss(imbalanced_logits, imbalanced_indices)\n",
        "\n",
        "print(f\"Balanced routing loss: {loss_balanced.item():.4f}\")\n",
        "print(f\"Imbalanced routing loss: {loss_imbalanced.item():.4f}\")\n",
        "print(f\"\\nðŸ’¡ Imbalanced loss is {loss_imbalanced / loss_balanced:.2f}x higher!\")\n",
        "print(\"   This penalty encourages the router to use all experts.\")\n",
        "\n",
        "# Visualize expert usage\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Balanced case\n",
        "expert_usage_balanced = torch.zeros(num_experts)\n",
        "for i in range(num_experts):\n",
        "    expert_usage_balanced[i] = (balanced_indices == i).sum().item()\n",
        "expert_usage_balanced = expert_usage_balanced / expert_usage_balanced.sum() * 100\n",
        "\n",
        "axes[0].bar(range(num_experts), expert_usage_balanced.numpy(), color='green', alpha=0.7)\n",
        "axes[0].axhline(y=100/num_experts, color='r', linestyle='--', label='Ideal')\n",
        "axes[0].set_xlabel('Expert Index', fontsize=12)\n",
        "axes[0].set_ylabel('Usage (%)', fontsize=12)\n",
        "axes[0].set_title(f'Balanced Routing (Loss={loss_balanced.item():.3f})', fontsize=14)\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Imbalanced case\n",
        "expert_usage_imbalanced = torch.zeros(num_experts)\n",
        "for i in range(num_experts):\n",
        "    expert_usage_imbalanced[i] = (imbalanced_indices == i).sum().item()\n",
        "expert_usage_imbalanced = expert_usage_imbalanced / expert_usage_imbalanced.sum() * 100\n",
        "\n",
        "axes[1].bar(range(num_experts), expert_usage_imbalanced.numpy(), color='red', alpha=0.7)\n",
        "axes[1].axhline(y=100/num_experts, color='r', linestyle='--', label='Ideal')\n",
        "axes[1].set_xlabel('Expert Index', fontsize=12)\n",
        "axes[1].set_ylabel('Usage (%)', fontsize=12)\n",
        "axes[1].set_title(f'Imbalanced Routing (Loss={loss_imbalanced.item():.3f})', fontsize=14)\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nðŸ“Š Higher load balancing loss â†’ More imbalanced expert usage\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 6. Advanced Routing: Expert Choice\n",
        "\n",
        "### The Capacity Problem\n",
        "\n",
        "**Traditional top-k routing problem**:\n",
        "- Token chooses experts\n",
        "- Multiple tokens can choose same expert\n",
        "- Expert may get overwhelmed (capacity overflow)\n",
        "- Need to drop tokens or increase capacity\n",
        "\n",
        "### Expert Choice Routing (DeepSeek-V3)\n",
        "\n",
        "**Reverse the process**: Experts choose tokens!\n",
        "\n",
        "```\n",
        "Traditional: Each token picks top-k experts\n",
        "  Token 1 â†’ Expert 0, Expert 2\n",
        "  Token 2 â†’ Expert 0, Expert 3\n",
        "  Token 3 â†’ Expert 0, Expert 1\n",
        "  â†’ Expert 0 is overwhelmed!\n",
        "\n",
        "Expert Choice: Each expert picks top-k tokens\n",
        "  Expert 0 â†’ Token 1, Token 5\n",
        "  Expert 1 â†’ Token 3, Token 7\n",
        "  Expert 2 â†’ Token 2, Token 4\n",
        "  â†’ Perfectly balanced!\n",
        "```\n",
        "\n",
        "### Algorithm\n",
        "\n",
        "1. Compute router scores for all (token, expert) pairs\n",
        "2. For each expert, select top-k tokens with highest scores\n",
        "3. Process exactly k tokens per expert\n",
        "4. If token not selected by any expert, use residual connection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ExpertChoiceRouter(nn.Module):\n",
        "    \"\"\"Expert Choice routing: experts select tokens (DeepSeek-V3 style)\"\"\"\n",
        "    \n",
        "    def __init__(self, d_model: int, num_experts: int, capacity_factor: float = 1.25):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_experts = num_experts\n",
        "        self.capacity_factor = capacity_factor\n",
        "        \n",
        "        self.gate = nn.Linear(d_model, num_experts, bias=False)\n",
        "    \n",
        "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (batch, seq_len, d_model)\n",
        "        \n",
        "        Returns:\n",
        "            expert_assignment: (num_experts, capacity) - which tokens each expert gets\n",
        "            expert_weights: (num_experts, capacity) - weights for each assignment\n",
        "            router_logits: (batch, seq_len, num_experts) - for aux loss\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "        num_tokens = batch_size * seq_len\n",
        "        \n",
        "        # Compute router logits\n",
        "        router_logits = self.gate(x)  # (batch, seq_len, num_experts)\n",
        "        router_logits_flat = router_logits.view(num_tokens, self.num_experts)\n",
        "        \n",
        "        # Capacity: how many tokens each expert can handle\n",
        "        capacity = int(num_tokens / self.num_experts * self.capacity_factor)\n",
        "        \n",
        "        # For each expert, select top-k tokens\n",
        "        expert_assignment = torch.zeros(\n",
        "            self.num_experts, capacity, dtype=torch.long, device=x.device\n",
        "        )\n",
        "        expert_weights = torch.zeros(\n",
        "            self.num_experts, capacity, device=x.device\n",
        "        )\n",
        "        \n",
        "        for expert_idx in range(self.num_experts):\n",
        "            # Get scores for this expert across all tokens\n",
        "            expert_scores = router_logits_flat[:, expert_idx]  # (num_tokens,)\n",
        "            \n",
        "            # Select top-capacity tokens\n",
        "            top_scores, top_indices = torch.topk(expert_scores, capacity)\n",
        "            \n",
        "            expert_assignment[expert_idx] = top_indices\n",
        "            expert_weights[expert_idx] = F.softmax(top_scores, dim=-1)\n",
        "        \n",
        "        return expert_assignment, expert_weights, router_logits\n",
        "\n",
        "\n",
        "# Demo expert choice routing\n",
        "print(\"\\nExpert Choice Routing\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "ec_router = ExpertChoiceRouter(d_model, num_experts=8, capacity_factor=1.25).to(device)\n",
        "x = torch.randn(4, 128, d_model, device=device)\n",
        "\n",
        "expert_assignment, expert_weights, router_logits = ec_router(x)\n",
        "\n",
        "batch_size, seq_len, _ = x.shape\n",
        "num_tokens = batch_size * seq_len\n",
        "capacity = expert_assignment.shape[1]\n",
        "\n",
        "print(f\"Input: {x.shape}\")\n",
        "print(f\"Number of tokens: {num_tokens}\")\n",
        "print(f\"Capacity per expert: {capacity}\")\n",
        "print(f\"Expert assignment shape: {expert_assignment.shape}\")\n",
        "print(f\"Expert weights shape: {expert_weights.shape}\")\n",
        "\n",
        "# Check how many tokens each expert gets\n",
        "print(f\"\\nTokens per expert:\")\n",
        "for i in range(8):\n",
        "    print(f\"  Expert {i}: {capacity} tokens (guaranteed)\")\n",
        "\n",
        "print(f\"\\nðŸ’¡ Expert Choice guarantees perfect load balancing!\")\n",
        "print(f\"   Each expert processes exactly {capacity} tokens.\")\n",
        "print(f\"   No auxiliary loss needed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 7. Real-World MoE: Mixtral Architecture\n",
        "\n",
        "Let's implement a complete transformer block with MoE, matching Mixtral's design."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RMSNorm(nn.Module):\n",
        "    \"\"\"RMS Layer Normalization\"\"\"\n",
        "    def __init__(self, d_model: int, eps: float = 1e-6):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.ones(d_model))\n",
        "    \n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        rms = torch.sqrt(torch.mean(x ** 2, dim=-1, keepdim=True) + self.eps)\n",
        "        return x / rms * self.weight\n",
        "\n",
        "\n",
        "class MixtralMoEBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Complete Mixtral-style transformer block with MoE\n",
        "    \n",
        "    Architecture:\n",
        "    - RMSNorm â†’ Attention â†’ Residual\n",
        "    - RMSNorm â†’ MoE FFN â†’ Residual\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model: int,\n",
        "        num_heads: int,\n",
        "        d_ff: int,\n",
        "        num_experts: int = 8,\n",
        "        top_k: int = 2,\n",
        "        dropout: float = 0.1\n",
        "    ):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Attention\n",
        "        self.attn_norm = RMSNorm(d_model)\n",
        "        self.attention = nn.MultiheadAttention(\n",
        "            d_model, num_heads, dropout=dropout, batch_first=True\n",
        "        )\n",
        "        \n",
        "        # MoE FFN\n",
        "        self.ffn_norm = RMSNorm(d_model)\n",
        "        self.moe = MixtureOfExpertsLayer(\n",
        "            d_model, d_ff, num_experts, top_k,\n",
        "            expert_type=\"swiglu\", dropout=dropout\n",
        "        )\n",
        "    \n",
        "    def forward(\n",
        "        self,\n",
        "        x: torch.Tensor,\n",
        "        mask: Optional[torch.Tensor] = None\n",
        "    ) -> Tuple[torch.Tensor, dict]:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (batch, seq_len, d_model)\n",
        "            mask: Optional attention mask\n",
        "        \n",
        "        Returns:\n",
        "            output: (batch, seq_len, d_model)\n",
        "            aux_info: MoE routing statistics\n",
        "        \"\"\"\n",
        "        # Attention block\n",
        "        attn_out, _ = self.attention(\n",
        "            self.attn_norm(x),\n",
        "            self.attn_norm(x),\n",
        "            self.attn_norm(x),\n",
        "            attn_mask=mask,\n",
        "            need_weights=False\n",
        "        )\n",
        "        x = x + attn_out\n",
        "        \n",
        "        # MoE FFN block\n",
        "        moe_out, aux_info = self.moe(self.ffn_norm(x))\n",
        "        x = x + moe_out\n",
        "        \n",
        "        return x, aux_info\n",
        "\n",
        "\n",
        "# Demo Mixtral-style block\n",
        "print(\"\\nMixtral-Style MoE Transformer Block\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "d_model = 4096\n",
        "num_heads = 32\n",
        "d_ff = 14336  # Mixtral uses larger FFN\n",
        "num_experts = 8\n",
        "top_k = 2\n",
        "\n",
        "mixtral_block = MixtralMoEBlock(\n",
        "    d_model, num_heads, d_ff, num_experts, top_k\n",
        ").to(device)\n",
        "\n",
        "batch_size = 2\n",
        "seq_len = 256\n",
        "x = torch.randn(batch_size, seq_len, d_model, device=device)\n",
        "\n",
        "output, aux_info = mixtral_block(x)\n",
        "\n",
        "print(f\"Configuration:\")\n",
        "print(f\"  d_model: {d_model}\")\n",
        "print(f\"  Heads: {num_heads}\")\n",
        "print(f\"  d_ff: {d_ff}\")\n",
        "print(f\"  Experts: {num_experts}\")\n",
        "print(f\"  Top-k: {top_k}\")\n",
        "\n",
        "print(f\"\\nShapes:\")\n",
        "print(f\"  Input: {x.shape}\")\n",
        "print(f\"  Output: {output.shape}\")\n",
        "\n",
        "total_params = sum(p.numel() for p in mixtral_block.parameters())\n",
        "print(f\"\\nTotal parameters: {total_params:,}\")\n",
        "\n",
        "# Calculate active parameters\n",
        "attn_params = sum(p.numel() for p in mixtral_block.attention.parameters())\n",
        "expert_params = sum(p.numel() for p in mixtral_block.moe.experts[0].parameters())\n",
        "active_params = attn_params + (top_k * expert_params)\n",
        "\n",
        "print(f\"Active parameters per token: {active_params:,}\")\n",
        "print(f\"Parameter efficiency: {total_params / active_params:.1f}x\")\n",
        "\n",
        "print(f\"\\nâœ… This matches Mixtral 8x7B architecture!\")\n",
        "print(f\"   47B total params, ~13B active per token\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 8. Training MoE: Complete Example\n",
        "\n",
        "Let's train a small MoE model and monitor expert usage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_moe_demo():\n",
        "    \"\"\"Train MoE model and monitor expert specialization\"\"\"\n",
        "    \n",
        "    # Small model for demonstration\n",
        "    d_model = 256\n",
        "    d_ff = 512\n",
        "    num_experts = 8\n",
        "    top_k = 2\n",
        "    \n",
        "    model = MixtureOfExpertsLayer(\n",
        "        d_model, d_ff, num_experts, top_k, expert_type=\"relu\"\n",
        "    ).to(device)\n",
        "    \n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "    \n",
        "    # Training loop\n",
        "    num_steps = 200\n",
        "    batch_size = 16\n",
        "    seq_len = 64\n",
        "    \n",
        "    expert_usage_history = []\n",
        "    loss_history = []\n",
        "    lb_loss_history = []\n",
        "    \n",
        "    print(\"Training MoE Model...\")\n",
        "    \n",
        "    for step in range(num_steps):\n",
        "        # Dummy data\n",
        "        x = torch.randn(batch_size, seq_len, d_model, device=device)\n",
        "        target = torch.randn(batch_size, seq_len, d_model, device=device)\n",
        "        \n",
        "        # Forward pass\n",
        "        output, aux_info = model(x)\n",
        "        \n",
        "        # Task loss (MSE for demonstration)\n",
        "        task_loss = F.mse_loss(output, target)\n",
        "        \n",
        "        # Load balancing loss\n",
        "        lb_loss = load_balancing_loss(\n",
        "            aux_info['router_logits'],\n",
        "            aux_info['expert_indices']\n",
        "        )\n",
        "        \n",
        "        # Total loss\n",
        "        alpha = 0.01  # Load balancing weight\n",
        "        total_loss = task_loss + alpha * lb_loss\n",
        "        \n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Track expert usage\n",
        "        expert_indices = aux_info['expert_indices']\n",
        "        expert_usage = torch.zeros(num_experts, device=device)\n",
        "        for i in range(num_experts):\n",
        "            expert_usage[i] = (expert_indices == i).sum().item()\n",
        "        expert_usage = expert_usage / expert_usage.sum()\n",
        "        \n",
        "        expert_usage_history.append(expert_usage.cpu().numpy())\n",
        "        loss_history.append(task_loss.item())\n",
        "        lb_loss_history.append(lb_loss.item())\n",
        "        \n",
        "        if (step + 1) % 50 == 0:\n",
        "            print(f\"Step {step+1}/{num_steps}: Loss={task_loss.item():.4f}, \"\n",
        "                  f\"LB Loss={lb_loss.item():.4f}\")\n",
        "    \n",
        "    print(\"\\nâœ… Training complete!\")\n",
        "    \n",
        "    return expert_usage_history, loss_history, lb_loss_history\n",
        "\n",
        "\n",
        "# Run training\n",
        "expert_usage_history, loss_history, lb_loss_history = train_moe_demo()\n",
        "\n",
        "# Visualize results\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Plot 1: Training loss\n",
        "axes[0, 0].plot(loss_history, linewidth=2)\n",
        "axes[0, 0].set_xlabel('Step', fontsize=12)\n",
        "axes[0, 0].set_ylabel('Task Loss', fontsize=12)\n",
        "axes[0, 0].set_title('Training Loss', fontsize=14)\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Load balancing loss\n",
        "axes[0, 1].plot(lb_loss_history, color='orange', linewidth=2)\n",
        "axes[0, 1].set_xlabel('Step', fontsize=12)\n",
        "axes[0, 1].set_ylabel('Load Balancing Loss', fontsize=12)\n",
        "axes[0, 1].set_title('Load Balancing Loss', fontsize=14)\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 3: Expert usage over time\n",
        "expert_usage_array = np.array(expert_usage_history)\n",
        "for i in range(8):\n",
        "    axes[1, 0].plot(expert_usage_array[:, i], label=f'Expert {i}', linewidth=2)\n",
        "axes[1, 0].axhline(y=1/8, color='r', linestyle='--', label='Ideal (12.5%)')\n",
        "axes[1, 0].set_xlabel('Step', fontsize=12)\n",
        "axes[1, 0].set_ylabel('Usage Fraction', fontsize=12)\n",
        "axes[1, 0].set_title('Expert Usage Over Time', fontsize=14)\n",
        "axes[1, 0].legend(ncol=3, fontsize=8)\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 4: Final expert usage distribution\n",
        "final_usage = expert_usage_array[-1] * 100\n",
        "axes[1, 1].bar(range(8), final_usage, color='green', alpha=0.7)\n",
        "axes[1, 1].axhline(y=12.5, color='r', linestyle='--', label='Ideal (12.5%)')\n",
        "axes[1, 1].set_xlabel('Expert Index', fontsize=12)\n",
        "axes[1, 1].set_ylabel('Usage (%)', fontsize=12)\n",
        "axes[1, 1].set_title('Final Expert Usage Distribution', fontsize=14)\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nðŸ“Š Key Observations:\")\n",
        "print(\"  - Load balancing loss decreases â†’ experts become more balanced\")\n",
        "print(\"  - Expert usage converges to ~12.5% each (1/8)\")\n",
        "print(\"  - Some variance is expected and healthy (expert specialization)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Mini Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 1: Calculate MoE Efficiency\n",
        "\n",
        "For a model with:\n",
        "- 64 experts\n",
        "- Top-8 routing\n",
        "- Each expert has 1B parameters\n",
        "\n",
        "Calculate:\n",
        "1. Total parameters\n",
        "2. Active parameters per token\n",
        "3. Parameter efficiency ratio\n",
        "4. How does this compare to a dense model with same active parameters?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your code here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Solution\n",
        "num_experts = 64\n",
        "top_k = 8\n",
        "params_per_expert = 1_000_000_000  # 1B\n",
        "\n",
        "total_params = num_experts * params_per_expert\n",
        "active_params = top_k * params_per_expert\n",
        "efficiency = total_params / active_params\n",
        "\n",
        "print(\"MoE Model Analysis\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Configuration:\")\n",
        "print(f\"  Experts: {num_experts}\")\n",
        "print(f\"  Top-k: {top_k}\")\n",
        "print(f\"  Parameters per expert: {params_per_expert:,}\")\n",
        "print(f\"\\nResults:\")\n",
        "print(f\"  Total parameters: {total_params/1e9:.1f}B\")\n",
        "print(f\"  Active parameters: {active_params/1e9:.1f}B\")\n",
        "print(f\"  Efficiency ratio: {efficiency:.1f}x\")\n",
        "print(f\"\\nComparison to dense model:\")\n",
        "print(f\"  Dense {active_params/1e9:.1f}B: Uses all {active_params/1e9:.1f}B params\")\n",
        "print(f\"  MoE {total_params/1e9:.1f}B: Uses only {active_params/1e9:.1f}B params\")\n",
        "print(f\"  MoE has {efficiency:.1f}x more capacity at same compute!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 2: Implement Z-Loss\n",
        "\n",
        "The Z-loss is another auxiliary loss used in some MoE models to prevent router logits from growing too large.\n",
        "\n",
        "$$\n",
        "\\mathcal{L}_z = \\frac{1}{N} \\sum_{i=1}^N \\left( \\log \\sum_{j=1}^E \\exp(h_{i,j}) \\right)^2\n",
        "$$\n",
        "\n",
        "Implement this loss function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your code here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Solution\n",
        "def z_loss(router_logits: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Z-loss: Penalizes large router logits\n",
        "    \n",
        "    Args:\n",
        "        router_logits: (batch, seq_len, num_experts)\n",
        "    \n",
        "    Returns:\n",
        "        loss: Scalar\n",
        "    \"\"\"\n",
        "    # log(sum(exp(logits))) for each token\n",
        "    log_z = torch.logsumexp(router_logits, dim=-1)  # (batch, seq_len)\n",
        "    \n",
        "    # Square and average\n",
        "    loss = (log_z ** 2).mean()\n",
        "    \n",
        "    return loss\n",
        "\n",
        "# Test\n",
        "router_logits = torch.randn(4, 128, 8)\n",
        "loss = z_loss(router_logits)\n",
        "print(f\"Z-loss: {loss.item():.4f}\")\n",
        "\n",
        "# Test with large logits (should have higher loss)\n",
        "large_logits = torch.randn(4, 128, 8) * 10\n",
        "large_loss = z_loss(large_logits)\n",
        "print(f\"Z-loss (large logits): {large_loss.item():.4f}\")\n",
        "print(f\"\\nðŸ’¡ Z-loss penalizes large router logits to improve stability\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 3: Visualize Expert Specialization\n",
        "\n",
        "Create a visualization showing which types of tokens each expert handles.\n",
        "Use different token categories (e.g., random noise with different means) and see if experts specialize."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your code here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Solution\n",
        "def visualize_expert_specialization():\n",
        "    \"\"\"Show how experts specialize for different token types\"\"\"\n",
        "    \n",
        "    d_model = 256\n",
        "    num_experts = 4\n",
        "    router = TopKRouter(d_model, num_experts, top_k=1).to(device)\n",
        "    \n",
        "    # Create 4 token types with different characteristics\n",
        "    num_tokens_per_type = 200\n",
        "    token_types = []\n",
        "    token_labels = []\n",
        "    \n",
        "    # Type 0: Small values\n",
        "    tokens_0 = torch.randn(num_tokens_per_type, d_model, device=device) * 0.5\n",
        "    token_types.append(tokens_0)\n",
        "    token_labels.extend([0] * num_tokens_per_type)\n",
        "    \n",
        "    # Type 1: Large values\n",
        "    tokens_1 = torch.randn(num_tokens_per_type, d_model, device=device) * 2.0\n",
        "    token_types.append(tokens_1)\n",
        "    token_labels.extend([1] * num_tokens_per_type)\n",
        "    \n",
        "    # Type 2: Positive bias\n",
        "    tokens_2 = torch.randn(num_tokens_per_type, d_model, device=device) + 1.0\n",
        "    token_types.append(tokens_2)\n",
        "    token_labels.extend([2] * num_tokens_per_type)\n",
        "    \n",
        "    # Type 3: Negative bias\n",
        "    tokens_3 = torch.randn(num_tokens_per_type, d_model, device=device) - 1.0\n",
        "    token_types.append(tokens_3)\n",
        "    token_labels.extend([3] * num_tokens_per_type)\n",
        "    \n",
        "    # Combine all tokens\n",
        "    all_tokens = torch.cat(token_types, dim=0).unsqueeze(0)  # (1, 800, d_model)\n",
        "    \n",
        "    # Route tokens\n",
        "    with torch.no_grad():\n",
        "        expert_indices, _, _ = router(all_tokens)\n",
        "    \n",
        "    expert_indices = expert_indices.squeeze(0).squeeze(-1).cpu().numpy()  # (800,)\n",
        "    token_labels = np.array(token_labels)\n",
        "    \n",
        "    # Create confusion matrix: token_type Ã— expert\n",
        "    confusion = np.zeros((4, num_experts))\n",
        "    for token_type in range(4):\n",
        "        mask = token_labels == token_type\n",
        "        for expert_idx in range(num_experts):\n",
        "            confusion[token_type, expert_idx] = (expert_indices[mask] == expert_idx).sum()\n",
        "    \n",
        "    # Normalize to percentages\n",
        "    confusion = confusion / confusion.sum(axis=1, keepdims=True) * 100\n",
        "    \n",
        "    # Visualize\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(confusion, annot=True, fmt='.1f', cmap='YlOrRd',\n",
        "                xticklabels=[f'Expert {i}' for i in range(num_experts)],\n",
        "                yticklabels=['Type 0\\n(small)', 'Type 1\\n(large)', \n",
        "                            'Type 2\\n(+bias)', 'Type 3\\n(-bias)'],\n",
        "                cbar_kws={'label': 'Percentage (%)'})\n",
        "    plt.xlabel('Expert', fontsize=12)\n",
        "    plt.ylabel('Token Type', fontsize=12)\n",
        "    plt.title('Expert Specialization by Token Type', fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"ðŸ“Š Heatmap shows which experts handle which token types\")\n",
        "    print(\"   Darker colors = more tokens of that type routed to that expert\")\n",
        "    print(\"\\nðŸ’¡ Experts naturally specialize for different input patterns!\")\n",
        "\n",
        "visualize_expert_specialization()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Comprehensive Exercise: Build a Complete MoE Model\n",
        "\n",
        "Create a complete MoE language model with:\n",
        "1. Multiple MoE layers\n",
        "2. Load balancing loss\n",
        "3. Expert choice routing (optional)\n",
        "4. Training loop with expert usage monitoring\n",
        "5. Comparison to dense baseline\n",
        "\n",
        "Requirements:\n",
        "- 4 layers\n",
        "- 8 experts per layer\n",
        "- Top-2 routing\n",
        "- Track expert usage per layer\n",
        "- Compare parameters and compute vs dense model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your code here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Solution\n",
        "class CompleteMoEModel(nn.Module):\n",
        "    \"\"\"Complete MoE model with multiple layers\"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model: int,\n",
        "        d_ff: int,\n",
        "        num_layers: int,\n",
        "        num_experts: int,\n",
        "        top_k: int\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        \n",
        "        self.layers = nn.ModuleList([\n",
        "            MixtureOfExpertsLayer(d_model, d_ff, num_experts, top_k, \"relu\")\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        \n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "    \n",
        "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, List[dict]]:\n",
        "        aux_infos = []\n",
        "        \n",
        "        for layer in self.layers:\n",
        "            x, aux_info = layer(x)\n",
        "            aux_infos.append(aux_info)\n",
        "        \n",
        "        x = self.norm(x)\n",
        "        return x, aux_infos\n",
        "\n",
        "\n",
        "# Build models\n",
        "d_model = 512\n",
        "d_ff = 2048\n",
        "num_layers = 4\n",
        "num_experts = 8\n",
        "top_k = 2\n",
        "\n",
        "moe_model = CompleteMoEModel(d_model, d_ff, num_layers, num_experts, top_k).to(device)\n",
        "\n",
        "# Dense baseline for comparison\n",
        "class DenseModel(nn.Module):\n",
        "    def __init__(self, d_model: int, d_ff: int, num_layers: int):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([\n",
        "            Expert(d_model, d_ff) for _ in range(num_layers)\n",
        "        ])\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "    \n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        for layer in self.layers:\n",
        "            x = x + layer(x)\n",
        "        return self.norm(x)\n",
        "\n",
        "dense_model = DenseModel(d_model, d_ff, num_layers).to(device)\n",
        "\n",
        "# Compare models\n",
        "print(\"Model Comparison\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "moe_params = sum(p.numel() for p in moe_model.parameters())\n",
        "dense_params = sum(p.numel() for p in dense_model.parameters())\n",
        "\n",
        "print(f\"\\nMoE Model:\")\n",
        "print(f\"  Layers: {num_layers}\")\n",
        "print(f\"  Experts per layer: {num_experts}\")\n",
        "print(f\"  Top-k: {top_k}\")\n",
        "print(f\"  Total parameters: {moe_params:,}\")\n",
        "print(f\"  Active params/token: ~{moe_params / num_experts * top_k:,.0f}\")\n",
        "\n",
        "print(f\"\\nDense Model:\")\n",
        "print(f\"  Layers: {num_layers}\")\n",
        "print(f\"  Total parameters: {dense_params:,}\")\n",
        "print(f\"  Active params/token: {dense_params:,}\")\n",
        "\n",
        "print(f\"\\nRatios:\")\n",
        "print(f\"  MoE has {moe_params/dense_params:.1f}x more total parameters\")\n",
        "print(f\"  MoE uses ~{(moe_params/num_experts*top_k)/dense_params:.1f}x active parameters\")\n",
        "\n",
        "# Test forward pass\n",
        "batch_size = 4\n",
        "seq_len = 128\n",
        "x = torch.randn(batch_size, seq_len, d_model, device=device)\n",
        "\n",
        "moe_out, aux_infos = moe_model(x)\n",
        "dense_out = dense_model(x)\n",
        "\n",
        "print(f\"\\nForward pass:\")\n",
        "print(f\"  Input: {x.shape}\")\n",
        "print(f\"  MoE output: {moe_out.shape}\")\n",
        "print(f\"  Dense output: {dense_out.shape}\")\n",
        "\n",
        "# Analyze expert usage per layer\n",
        "print(f\"\\nExpert usage by layer:\")\n",
        "for layer_idx, aux_info in enumerate(aux_infos):\n",
        "    expert_indices = aux_info['expert_indices']\n",
        "    expert_usage = torch.zeros(num_experts, device=device)\n",
        "    for i in range(num_experts):\n",
        "        expert_usage[i] = (expert_indices == i).sum().item()\n",
        "    expert_usage = expert_usage / expert_usage.sum() * 100\n",
        "    \n",
        "    print(f\"\\n  Layer {layer_idx}:\")\n",
        "    for i in range(num_experts):\n",
        "        bar = 'â–ˆ' * int(expert_usage[i].item() / 3)\n",
        "        print(f\"    Expert {i}: {expert_usage[i].item():5.1f}% {bar}\")\n",
        "\n",
        "print(\"\\nâœ… Complete MoE model with multi-layer expert routing!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Key Takeaways\n",
        "\n",
        "1. **MoE decouples parameters from compute**: 10x params, 2-3x compute\n",
        "2. **Top-k routing**: Each token uses subset of experts (typically k=2)\n",
        "3. **Load balancing is critical**: Use auxiliary loss to prevent routing collapse\n",
        "4. **Expert specialization**: Experts naturally learn different skills\n",
        "5. **Trade-offs**: Memory cost for total params, communication overhead\n",
        "6. **Expert choice routing**: Alternative that guarantees perfect balance\n",
        "7. **Production usage**: GPT-4, DeepSeek-V3, Mixtral all use MoE\n",
        "\n",
        "## Modern MoE Architectures (2025)\n",
        "\n",
        "**GPT-4** (rumored):\n",
        "- 16 experts per layer\n",
        "- Top-2 routing\n",
        "- ~1.8T total parameters\n",
        "- ~220B active per token\n",
        "\n",
        "**DeepSeek-V3**:\n",
        "- 256 experts per layer\n",
        "- Top-8 routing with load balancing\n",
        "- 671B total parameters\n",
        "- ~37B active per token\n",
        "- Achieves GPT-4 level quality\n",
        "\n",
        "**Mixtral 8x7B**:\n",
        "- 8 experts per layer\n",
        "- Top-2 routing\n",
        "- 47B total parameters\n",
        "- ~13B active per token\n",
        "- Outperforms LLaMA 2 70B\n",
        "\n",
        "**Implementation Tips**:\n",
        "- Always use load balancing loss (Î±=0.01-0.1)\n",
        "- Monitor expert usage during training\n",
        "- Consider expert choice routing for guaranteed balance\n",
        "- Use SwiGLU experts for better quality\n",
        "- Implement expert parallelism for large models\n",
        "\n",
        "---\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "Continue to: [Topic 14: torch.compile & Performance Optimization](14_torch_compile.ipynb)\n",
        "\n",
        "---\n",
        "\n",
        "## Further Reading\n",
        "\n",
        "- [Switch Transformers: Scaling to Trillion Parameter Models](https://arxiv.org/abs/2101.03961) (2021)\n",
        "- [GLaM: Efficient Scaling of Language Models with Mixture-of-Experts](https://arxiv.org/abs/2112.06905) (2021)\n",
        "- [Mixtral of Experts](https://arxiv.org/abs/2401.04088) (2024)\n",
        "- [DeepSeek-V3 Technical Report](https://arxiv.org/abs/2412.19437) (2025)\n",
        "- [Expert Choice Routing](https://arxiv.org/abs/2202.09368) (2022)\n",
        "- [ST-MoE: Designing Stable and Transferable Sparse Expert Models](https://arxiv.org/abs/2202.08906) (2022)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
