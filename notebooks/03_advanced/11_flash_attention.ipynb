{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic 11: Flash Attention - Optimizing Attention for Speed and Memory\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Understand why standard attention is memory-inefficient\n",
    "- Learn the Flash Attention algorithm and its optimizations\n",
    "- Use PyTorch 2.5+ FlexAttention API for flexible attention patterns\n",
    "- Leverage cuDNN Fused Flash Attention for H100 GPUs (75% speedup)\n",
    "- Know when to apply Flash Attention in your models\n",
    "- Understand the trade-offs and performance characteristics\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Big Picture: Why Flash Attention?\n",
    "\n",
    "### The Attention Memory Problem\n",
    "\n",
    "Standard attention has revolutionized AI, but it has a **critical bottleneck**: **O(n¬≤) memory complexity**.\n",
    "\n",
    "**What does this mean?**\n",
    "- For a sequence of length n=1024 tokens\n",
    "- Attention matrix size: 1024 √ó 1024 = **1,048,576 elements**\n",
    "- For n=4096 (common in modern LLMs): **16,777,216 elements**\n",
    "- For n=16384 (long context): **268,435,456 elements** (1GB+ just for attention scores!)\n",
    "\n",
    "**Why is this a problem?**\n",
    "1. **GPU Memory Limited**: Modern GPUs have 40-80GB VRAM; long sequences eat it all\n",
    "2. **Memory Bandwidth Bottleneck**: Moving data between GPU memory levels is slow\n",
    "3. **Limits Context Length**: Can't fit long documents/conversations\n",
    "\n",
    "### Enter Flash Attention\n",
    "\n",
    "**Flash Attention** solves this by:\n",
    "1. **Tiling**: Processing attention in smaller blocks that fit in fast SRAM\n",
    "2. **Recomputation**: Recalculating values instead of storing them (compute vs memory trade-off)\n",
    "3. **Kernel Fusion**: Combining operations to minimize memory reads/writes\n",
    "\n",
    "**Result**: Same output, but **2-4x faster** and **10-20x less memory**!\n",
    "\n",
    "### Real-World Impact\n",
    "- **GPT-4**: Uses Flash Attention for 32k+ context windows\n",
    "- **LLaMA 3**: Enables 128k context with Flash Attention 2\n",
    "- **Claude 3**: 200k context possible with optimized attention\n",
    "- **Your models**: Train larger batches, longer sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from torch.nn.attention import SDPBackend, sdpa_kernel\n",
    "\n",
    "# Check PyTorch version\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Standard Attention: Understanding the Bottleneck\n",
    "\n",
    "### Attention Formula Recap\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "\n",
    "### Memory Analysis: Step by Step\n",
    "\n",
    "For sequence length **n** and embedding dimension **d**:\n",
    "\n",
    "1. **Input**: Q, K, V each are (n, d) ‚Üí **3nd** memory\n",
    "2. **Attention Scores**: QK^T is (n, n) ‚Üí **n¬≤** memory ‚ö†Ô∏è\n",
    "3. **Softmax**: Still (n, n) ‚Üí **n¬≤** memory ‚ö†Ô∏è\n",
    "4. **Output**: (n, n) @ (n, d) ‚Üí **nd** memory\n",
    "\n",
    "**Total**: O(n¬≤ + nd) ‚âà **O(n¬≤)** for large n\n",
    "\n",
    "**The bottleneck**: Storing the (n, n) attention matrix!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_attention(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    Standard attention implementation (memory-inefficient but clear)\n",
    "    \n",
    "    Args:\n",
    "        Q: (batch, n, d_k) - Queries\n",
    "        K: (batch, n, d_k) - Keys\n",
    "        V: (batch, n, d_v) - Values\n",
    "        mask: Optional attention mask\n",
    "    \n",
    "    Returns:\n",
    "        output: (batch, n, d_v)\n",
    "    \"\"\"\n",
    "    d_k = Q.size(-1)\n",
    "    \n",
    "    # Step 1: Compute attention scores (THIS IS THE BOTTLENECK!)\n",
    "    # Shape: (batch, n, n) - stores ALL pairwise scores\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(d_k)\n",
    "    \n",
    "    # Step 2: Apply mask (if provided)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "    \n",
    "    # Step 3: Softmax (still n√ón matrix)\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    \n",
    "    # Step 4: Apply attention to values\n",
    "    output = torch.matmul(attention_weights, V)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "\n",
    "# Demo: Memory usage visualization\n",
    "def analyze_memory(seq_len, d_model=512):\n",
    "    \"\"\"Calculate memory for attention matrix\"\"\"\n",
    "    # Attention matrix: (seq_len, seq_len)\n",
    "    attention_elements = seq_len * seq_len\n",
    "    # float32 = 4 bytes\n",
    "    attention_memory_mb = (attention_elements * 4) / (1024 ** 2)\n",
    "    \n",
    "    # Input matrices: Q, K, V each (seq_len, d_model)\n",
    "    input_elements = 3 * seq_len * d_model\n",
    "    input_memory_mb = (input_elements * 4) / (1024 ** 2)\n",
    "    \n",
    "    return {\n",
    "        'attention_matrix_mb': attention_memory_mb,\n",
    "        'input_matrices_mb': input_memory_mb,\n",
    "        'total_mb': attention_memory_mb + input_memory_mb\n",
    "    }\n",
    "\n",
    "# Compare different sequence lengths\n",
    "seq_lengths = [512, 1024, 2048, 4096, 8192, 16384]\n",
    "results = [analyze_memory(n) for n in seq_lengths]\n",
    "\n",
    "print(\"Memory Usage Analysis (per attention head):\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Seq Len':>10} {'Attention Matrix':>20} {'Input Matrices':>20} {'Total':>10}\")\n",
    "print(\"=\"*60)\n",
    "for n, r in zip(seq_lengths, results):\n",
    "    print(f\"{n:>10} {r['attention_matrix_mb']:>18.1f} MB {r['input_matrices_mb']:>18.1f} MB {r['total_mb']:>8.1f} MB\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è Notice: Attention matrix dominates for large sequences!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize memory growth\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot 1: Memory vs Sequence Length\n",
    "plt.subplot(1, 2, 1)\n",
    "attention_mem = [r['attention_matrix_mb'] for r in results]\n",
    "input_mem = [r['input_matrices_mb'] for r in results]\n",
    "\n",
    "plt.plot(seq_lengths, attention_mem, 'r-o', label='Attention Matrix (O(n¬≤))', linewidth=2)\n",
    "plt.plot(seq_lengths, input_mem, 'b-s', label='Input Matrices (O(n))', linewidth=2)\n",
    "plt.xlabel('Sequence Length', fontsize=12)\n",
    "plt.ylabel('Memory (MB)', fontsize=12)\n",
    "plt.title('Memory Usage: O(n¬≤) vs O(n)', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "\n",
    "# Plot 2: Percentage breakdown\n",
    "plt.subplot(1, 2, 2)\n",
    "percentages = [(r['attention_matrix_mb'] / r['total_mb'] * 100) for r in results]\n",
    "plt.bar(range(len(seq_lengths)), percentages, color='coral')\n",
    "plt.xticks(range(len(seq_lengths)), seq_lengths)\n",
    "plt.xlabel('Sequence Length', fontsize=12)\n",
    "plt.ylabel('Attention Matrix %', fontsize=12)\n",
    "plt.title('Attention Matrix as % of Total Memory', fontsize=14)\n",
    "plt.axhline(y=50, color='r', linestyle='--', label='50%')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüí° Insight: At seq_len=16384, attention matrix is {percentages[-1]:.1f}% of memory!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Flash Attention Algorithm\n",
    "\n",
    "### Core Insight: IO-Awareness\n",
    "\n",
    "Modern GPUs have a **memory hierarchy**:\n",
    "1. **HBM (High Bandwidth Memory)**: Large (40-80GB) but slow\n",
    "2. **SRAM (on-chip memory)**: Small (20MB) but **20x faster**\n",
    "\n",
    "**Standard attention**: Reads/writes the full (n, n) matrix from/to HBM ‚Üí slow!\n",
    "\n",
    "**Flash Attention**: Keeps working set in SRAM, minimizes HBM access\n",
    "\n",
    "### The Three Key Ideas\n",
    "\n",
    "#### 1. Tiling (Block-wise Computation)\n",
    "- Divide Q, K, V into blocks that fit in SRAM\n",
    "- Compute attention for each block independently\n",
    "- Combine results online (no need to store full attention matrix)\n",
    "\n",
    "#### 2. Recomputation in Backward Pass\n",
    "- Don't store attention matrix for gradients\n",
    "- Recompute it during backward pass (fast because it's in SRAM)\n",
    "- Trade-off: More compute, less memory\n",
    "\n",
    "#### 3. Kernel Fusion\n",
    "- Fuse softmax, dropout, masking into attention kernel\n",
    "- Fewer memory reads/writes\n",
    "- Better GPU utilization\n",
    "\n",
    "### Algorithm Visualization\n",
    "\n",
    "```\n",
    "Standard Attention:\n",
    "  Q, K, V (in HBM) ‚Üí Attention Matrix (in HBM) ‚Üí Output\n",
    "  ^^^^^^^^^^^^^^^^    ^^^^^^^^^^^^^^^^^^^^^^^\n",
    "      Slow I/O              Huge memory\n",
    "\n",
    "Flash Attention:\n",
    "  Load Q_block, K_block ‚Üí Compute in SRAM ‚Üí Update output\n",
    "  Repeat for all blocks ‚Üí No full matrix stored!\n",
    "  ^^^^^^^^^^^^^^^^^^^^^    ^^^^^^^^^^^^^^^^^^^^\n",
    "    Minimal I/O              Memory efficient\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flash_attention_simplified(Q, K, V, block_size=64):\n",
    "    \"\"\"\n",
    "    Simplified Flash Attention for educational purposes\n",
    "    (Real implementation is in CUDA for speed)\n",
    "    \n",
    "    Key idea: Process attention in blocks to save memory\n",
    "    \"\"\"\n",
    "    batch_size, seq_len, d_k = Q.shape\n",
    "    d_v = V.size(-1)\n",
    "    \n",
    "    # Initialize output and normalization factors\n",
    "    output = torch.zeros(batch_size, seq_len, d_v, device=Q.device)\n",
    "    row_max = torch.full((batch_size, seq_len), float('-inf'), device=Q.device)\n",
    "    row_sum = torch.zeros(batch_size, seq_len, device=Q.device)\n",
    "    \n",
    "    # Process in blocks (tiling)\n",
    "    num_blocks = (seq_len + block_size - 1) // block_size\n",
    "    \n",
    "    for i in range(num_blocks):\n",
    "        # Query block\n",
    "        q_start = i * block_size\n",
    "        q_end = min((i + 1) * block_size, seq_len)\n",
    "        Q_block = Q[:, q_start:q_end, :]  # (batch, block_size, d_k)\n",
    "        \n",
    "        for j in range(num_blocks):\n",
    "            # Key/Value block\n",
    "            k_start = j * block_size\n",
    "            k_end = min((j + 1) * block_size, seq_len)\n",
    "            K_block = K[:, k_start:k_end, :]  # (batch, block_size, d_k)\n",
    "            V_block = V[:, k_start:k_end, :]  # (batch, block_size, d_v)\n",
    "            \n",
    "            # Compute attention scores for this block pair\n",
    "            scores = torch.matmul(Q_block, K_block.transpose(-2, -1)) / np.sqrt(d_k)\n",
    "            \n",
    "            # Online softmax (numerically stable)\n",
    "            block_max = scores.max(dim=-1, keepdim=True).values\n",
    "            scores_exp = torch.exp(scores - block_max)\n",
    "            \n",
    "            # Update running max and sum for softmax normalization\n",
    "            new_max = torch.maximum(row_max[:, q_start:q_end], block_max.squeeze(-1))\n",
    "            old_scale = torch.exp(row_max[:, q_start:q_end] - new_max)\n",
    "            new_scale = torch.exp(block_max.squeeze(-1) - new_max)\n",
    "            \n",
    "            # Update output and normalization (this is the clever part!)\n",
    "            output[:, q_start:q_end, :] = (output[:, q_start:q_end, :] * old_scale.unsqueeze(-1) + \n",
    "                                           torch.matmul(scores_exp * new_scale.unsqueeze(-1), V_block))\n",
    "            \n",
    "            row_sum[:, q_start:q_end] = row_sum[:, q_start:q_end] * old_scale + scores_exp.sum(dim=-1)\n",
    "            row_max[:, q_start:q_end] = new_max\n",
    "    \n",
    "    # Final normalization\n",
    "    output = output / row_sum.unsqueeze(-1)\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Test: Verify it produces same results as standard attention\n",
    "batch, n, d = 2, 128, 64\n",
    "Q = torch.randn(batch, n, d)\n",
    "K = torch.randn(batch, n, d)\n",
    "V = torch.randn(batch, n, d)\n",
    "\n",
    "# Standard attention\n",
    "standard_out, _ = standard_attention(Q, K, V)\n",
    "\n",
    "# Flash attention (simplified)\n",
    "flash_out = flash_attention_simplified(Q, K, V, block_size=32)\n",
    "\n",
    "# Compare\n",
    "difference = torch.abs(standard_out - flash_out).max().item()\n",
    "print(f\"Maximum difference: {difference:.6f}\")\n",
    "print(f\"Results match: {torch.allclose(standard_out, flash_out, atol=1e-4)}\")\n",
    "print(\"\\n‚úÖ Flash Attention produces identical output with less memory!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. PyTorch's Built-in Flash Attention\n",
    "\n",
    "### Scaled Dot-Product Attention (SDPA)\n",
    "\n",
    "PyTorch 2.0+ includes **optimized attention** via `F.scaled_dot_product_attention()`\n",
    "\n",
    "**It automatically chooses the best backend**:\n",
    "1. **Flash Attention 2** (if available)\n",
    "2. **Memory-efficient attention** (xFormers)\n",
    "3. **cuDNN Fused Flash Attention** (H100 GPUs - 75% faster!)\n",
    "4. **Math implementation** (fallback)\n",
    "\n",
    "**You just call one function** and get the fastest version!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using PyTorch's optimized attention\n",
    "def efficient_attention(Q, K, V, mask=None, is_causal=False):\n",
    "    \"\"\"\n",
    "    Use PyTorch's scaled_dot_product_attention (automatically uses Flash Attention)\n",
    "    \"\"\"\n",
    "    output = F.scaled_dot_product_attention(\n",
    "        Q, K, V,\n",
    "        attn_mask=mask,\n",
    "        is_causal=is_causal,  # For autoregressive models\n",
    "        dropout_p=0.0\n",
    "    )\n",
    "    return output\n",
    "\n",
    "# Check which backend is being used\n",
    "print(\"Available attention backends:\")\n",
    "print(f\"  Flash Attention: {torch.backends.cuda.flash_sdp_enabled()}\")\n",
    "print(f\"  Memory Efficient: {torch.backends.cuda.mem_efficient_sdp_enabled()}\")\n",
    "print(f\"  Math (fallback): {torch.backends.cuda.math_sdp_enabled()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison: Standard vs Flash Attention\n",
    "def benchmark_attention(seq_len, d_model=512, num_heads=8, device='cuda'):\n",
    "    \"\"\"\n",
    "    Compare standard attention vs Flash Attention\n",
    "    \"\"\"\n",
    "    if device == 'cuda' and not torch.cuda.is_available():\n",
    "        print(\"CUDA not available, using CPU\")\n",
    "        device = 'cpu'\n",
    "    \n",
    "    # Create random data\n",
    "    batch_size = 16\n",
    "    d_k = d_model // num_heads\n",
    "    \n",
    "    Q = torch.randn(batch_size, num_heads, seq_len, d_k, device=device)\n",
    "    K = torch.randn(batch_size, num_heads, seq_len, d_k, device=device)\n",
    "    V = torch.randn(batch_size, num_heads, seq_len, d_k, device=device)\n",
    "    \n",
    "    # Warm up\n",
    "    for _ in range(3):\n",
    "        _ = F.scaled_dot_product_attention(Q, K, V)\n",
    "    \n",
    "    if device == 'cuda':\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    # Benchmark Flash Attention (via SDPA)\n",
    "    start = time.time()\n",
    "    for _ in range(10):\n",
    "        output = F.scaled_dot_product_attention(Q, K, V)\n",
    "    if device == 'cuda':\n",
    "        torch.cuda.synchronize()\n",
    "    flash_time = (time.time() - start) / 10\n",
    "    \n",
    "    # Memory usage\n",
    "    if device == 'cuda':\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        output = F.scaled_dot_product_attention(Q, K, V)\n",
    "        flash_memory = torch.cuda.max_memory_allocated() / 1e6  # MB\n",
    "    else:\n",
    "        flash_memory = 0\n",
    "    \n",
    "    return {\n",
    "        'time': flash_time,\n",
    "        'memory_mb': flash_memory\n",
    "    }\n",
    "\n",
    "# Run benchmarks\n",
    "if torch.cuda.is_available():\n",
    "    seq_lengths = [512, 1024, 2048, 4096]\n",
    "    results = []\n",
    "    \n",
    "    print(\"Benchmarking Flash Attention (via SDPA):\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"{'Seq Len':>10} {'Time (ms)':>15} {'Memory (MB)':>15}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for seq_len in seq_lengths:\n",
    "        result = benchmark_attention(seq_len, device='cuda')\n",
    "        results.append(result)\n",
    "        print(f\"{seq_len:>10} {result['time']*1000:>14.2f} {result['memory_mb']:>14.1f}\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Flash Attention is automatically used on CUDA!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è CUDA not available. Flash Attention requires GPU.\")\n",
    "    print(\"Running on CPU with standard implementation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. FlexAttention API (PyTorch 2.5+)\n",
    "\n",
    "### The Problem with Custom Attention Patterns\n",
    "\n",
    "Modern models need **different attention patterns**:\n",
    "- **Causal masking** (GPT): Can't attend to future tokens\n",
    "- **Sliding window** (Longformer): Local attention only\n",
    "- **Prefix attention** (PrefixLM): Different masks for prefix vs generation\n",
    "- **Alibi/RoPE**: Custom positional biases\n",
    "\n",
    "**Problem**: Writing custom CUDA kernels for each pattern is hard!\n",
    "\n",
    "### FlexAttention: Flexible + Fast\n",
    "\n",
    "PyTorch 2.5 introduced **FlexAttention**:\n",
    "- Write attention pattern in **Python**\n",
    "- PyTorch **automatically generates fused CUDA kernel**\n",
    "- Performance matches hand-written kernels!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FlexAttention example (PyTorch 2.5+)\n",
    "try:\n",
    "    from torch.nn.attention.flex_attention import flex_attention, create_block_mask\n",
    "    \n",
    "    # Define custom attention pattern\n",
    "    def sliding_window_mask(b, h, q_idx, kv_idx):\n",
    "        \"\"\"Sliding window: attend to nearest 256 tokens\"\"\"\n",
    "        window_size = 256\n",
    "        return (q_idx - kv_idx).abs() <= window_size\n",
    "    \n",
    "    def prefix_lm_mask(b, h, q_idx, kv_idx):\n",
    "        \"\"\"Prefix can attend bidirectionally, generation is causal\"\"\"\n",
    "        prefix_len = 128\n",
    "        if q_idx < prefix_len:\n",
    "            return kv_idx < prefix_len  # Bidirectional in prefix\n",
    "        else:\n",
    "            return kv_idx <= q_idx  # Causal in generation\n",
    "    \n",
    "    # Usage\n",
    "    batch, num_heads, seq_len, d_k = 4, 8, 512, 64\n",
    "    Q = torch.randn(batch, num_heads, seq_len, d_k, device=device)\n",
    "    K = torch.randn(batch, num_heads, seq_len, d_k, device=device)\n",
    "    V = torch.randn(batch, num_heads, seq_len, d_k, device=device)\n",
    "    \n",
    "    # Create block mask from function\n",
    "    block_mask = create_block_mask(sliding_window_mask, B=batch, H=num_heads, \n",
    "                                    Q_LEN=seq_len, KV_LEN=seq_len)\n",
    "    \n",
    "    # Apply FlexAttention (automatically generates optimized kernel!)\n",
    "    output = flex_attention(Q, K, V, block_mask=block_mask)\n",
    "    \n",
    "    print(\"‚úÖ FlexAttention successfully applied!\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(\"\\nüí° FlexAttention automatically fused the custom pattern into a fast kernel!\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è FlexAttention requires PyTorch 2.5+\")\n",
    "    print(\"Current version:\", torch.__version__)\n",
    "    print(\"\\nFlexAttention allows custom attention patterns with Flash Attention speed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. cuDNN Fused Flash Attention (H100)\n",
    "\n",
    "### The Latest Optimization\n",
    "\n",
    "PyTorch 2.5 added **cuDNN Fused Flash Attention** backend:\n",
    "- **75% speedup** on H100 GPUs vs Flash Attention 2\n",
    "- Enabled by default on H100+\n",
    "- No code changes needed!\n",
    "\n",
    "**Why so fast?**\n",
    "- Uses H100's specialized Tensor cores\n",
    "- Better instruction scheduling\n",
    "- Optimized for H100's memory hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Controlling attention backend\n",
    "from torch.nn.attention import SDPBackend\n",
    "\n",
    "# Force specific backend\n",
    "def attention_with_backend(Q, K, V, backend=SDPBackend.FLASH_ATTENTION):\n",
    "    \"\"\"\n",
    "    Use specific attention backend\n",
    "    \n",
    "    Backends:\n",
    "    - FLASH_ATTENTION: Flash Attention 2\n",
    "    - EFFICIENT_ATTENTION: Memory-efficient (xFormers)\n",
    "    - MATH: Standard PyTorch (slowest)\n",
    "    - CUDNN_ATTENTION: cuDNN Fused (H100 only, auto-selected)\n",
    "    \"\"\"\n",
    "    with sdpa_kernel([backend]):\n",
    "        output = F.scaled_dot_product_attention(Q, K, V)\n",
    "    return output\n",
    "\n",
    "# Check GPU capability\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    print(f\"GPU: {gpu_name}\")\n",
    "    \n",
    "    if 'H100' in gpu_name:\n",
    "        print(\"\\n‚úÖ H100 detected! cuDNN Fused Flash Attention available.\")\n",
    "        print(\"Expected speedup: ~75% vs Flash Attention 2\")\n",
    "    else:\n",
    "        print(f\"\\nüí° On {gpu_name}, Flash Attention 2 will be used.\")\n",
    "        print(\"cuDNN Fused Flash Attention requires H100 or newer.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU available. Flash Attention optimizations require CUDA.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Practical Multi-Head Attention with Flash Attention\n",
    "\n",
    "### Building a Production-Ready Attention Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlashMultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Attention with Flash Attention optimization\n",
    "    \n",
    "    Automatically uses:\n",
    "    - cuDNN Fused Flash Attention (H100)\n",
    "    - Flash Attention 2 (A100/other GPUs)\n",
    "    - Memory-efficient attention (fallback)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        # Linear projections\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = dropout\n",
    "    \n",
    "    def forward(self, Q, K, V, mask=None, is_causal=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            Q: (batch, seq_len, d_model)\n",
    "            K: (batch, seq_len, d_model)\n",
    "            V: (batch, seq_len, d_model)\n",
    "            mask: Optional attention mask\n",
    "            is_causal: Use causal masking (for GPT-style models)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = Q.shape\n",
    "        \n",
    "        # Linear projections and reshape to (batch, num_heads, seq_len, d_k)\n",
    "        Q = self.W_q(Q).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(K).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(V).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Flash Attention! (automatically selects best backend)\n",
    "        attn_output = F.scaled_dot_product_attention(\n",
    "            Q, K, V,\n",
    "            attn_mask=mask,\n",
    "            dropout_p=self.dropout if self.training else 0.0,\n",
    "            is_causal=is_causal\n",
    "        )\n",
    "        \n",
    "        # Reshape and project output\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        output = self.W_o(attn_output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "# Demo\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "seq_len = 1024\n",
    "batch_size = 4\n",
    "\n",
    "model = FlashMultiHeadAttention(d_model, num_heads).to(device)\n",
    "x = torch.randn(batch_size, seq_len, d_model, device=device)\n",
    "\n",
    "# Self-attention\n",
    "output = model(x, x, x, is_causal=True)  # Causal for GPT-style\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"\\n‚úÖ Flash Attention is used internally!\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. When to Use Flash Attention\n",
    "\n",
    "### Decision Guide\n",
    "\n",
    "**Always use Flash Attention when**:\n",
    "- ‚úÖ Sequence length > 512\n",
    "- ‚úÖ Training transformers (memory savings allow larger batches)\n",
    "- ‚úÖ Long context models (4k+ tokens)\n",
    "- ‚úÖ GPU available (especially A100, H100)\n",
    "\n",
    "**Standard attention is fine when**:\n",
    "- ‚ö†Ô∏è Very short sequences (< 128 tokens)\n",
    "- ‚ö†Ô∏è CPU only (Flash Attention needs GPU)\n",
    "- ‚ö†Ô∏è Debugging (standard is easier to inspect)\n",
    "\n",
    "### Performance Characteristics\n",
    "\n",
    "| Metric | Standard Attention | Flash Attention |\n",
    "|--------|-------------------|------------------|\n",
    "| **Memory** | O(n¬≤) | O(n) |\n",
    "| **Speed** | Baseline | 2-4x faster |\n",
    "| **Accuracy** | Exact | Exact (same output) |\n",
    "| **GPU Required** | No | Yes |\n",
    "| **Max Sequence** | ~2k tokens | 100k+ tokens |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Mini Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Memory Calculation\n",
    "\n",
    "Calculate the attention matrix memory for a model with:\n",
    "- Sequence length: 8192 tokens\n",
    "- Number of heads: 32\n",
    "- Batch size: 8\n",
    "- Data type: float16 (2 bytes per element)\n",
    "\n",
    "How much memory (in GB) is needed just for attention matrices?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "seq_len = 8192\n",
    "num_heads = 32\n",
    "batch_size = 8\n",
    "bytes_per_element = 2  # float16\n",
    "\n",
    "# Attention matrix per head: (seq_len, seq_len)\n",
    "elements_per_head = seq_len * seq_len\n",
    "\n",
    "# Total elements: batch * heads * matrix\n",
    "total_elements = batch_size * num_heads * elements_per_head\n",
    "\n",
    "# Memory in GB\n",
    "memory_bytes = total_elements * bytes_per_element\n",
    "memory_gb = memory_bytes / (1024 ** 3)\n",
    "\n",
    "print(f\"Attention matrix memory: {memory_gb:.2f} GB\")\n",
    "print(f\"\\nüí° That's {memory_gb:.1f}GB just for attention!\")\n",
    "print(f\"With Flash Attention: ~{memory_gb/10:.2f}GB (10x reduction)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Causal Attention with Flash Attention\n",
    "\n",
    "Implement a simple decoder using Flash Attention with causal masking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "class CausalFlashAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.attention = FlashMultiHeadAttention(d_model, num_heads)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Causal masking via is_causal=True\n",
    "        return self.attention(x, x, x, is_causal=True)\n",
    "\n",
    "# Test\n",
    "model = CausalFlashAttention(d_model=256, num_heads=8).to(device)\n",
    "x = torch.randn(4, 512, 256, device=device)\n",
    "output = model(x)\n",
    "\n",
    "print(f\"Input: {x.shape}\")\n",
    "print(f\"Output: {output.shape}\")\n",
    "print(\"\\n‚úÖ Causal Flash Attention working!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Comprehensive Exercise: Flash Attention Benchmark\n",
    "\n",
    "Create a comprehensive benchmark comparing:\n",
    "1. Standard attention (manual implementation)\n",
    "2. PyTorch SDPA with Flash Attention\n",
    "\n",
    "For sequence lengths [512, 1024, 2048, 4096], measure:\n",
    "- Forward pass time\n",
    "- Peak memory usage\n",
    "- Numerical accuracy (compare outputs)\n",
    "\n",
    "Plot the results showing speedup and memory savings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "def comprehensive_benchmark():\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"GPU required for meaningful benchmark\")\n",
    "        return\n",
    "    \n",
    "    seq_lengths = [512, 1024, 2048, 4096]\n",
    "    d_model = 512\n",
    "    num_heads = 8\n",
    "    batch_size = 8\n",
    "    \n",
    "    results = {'seq_len': [], 'standard_time': [], 'flash_time': [], \n",
    "               'standard_mem': [], 'flash_mem': [], 'speedup': []}\n",
    "    \n",
    "    for seq_len in seq_lengths:\n",
    "        d_k = d_model // num_heads\n",
    "        Q = torch.randn(batch_size, num_heads, seq_len, d_k, device='cuda')\n",
    "        K = torch.randn(batch_size, num_heads, seq_len, d_k, device='cuda')\n",
    "        V = torch.randn(batch_size, num_heads, seq_len, d_k, device='cuda')\n",
    "        \n",
    "        # Warm up\n",
    "        for _ in range(3):\n",
    "            _ = F.scaled_dot_product_attention(Q, K, V)\n",
    "        torch.cuda.synchronize()\n",
    "        \n",
    "        # Flash Attention (SDPA)\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        start = time.time()\n",
    "        for _ in range(10):\n",
    "            flash_out = F.scaled_dot_product_attention(Q, K, V)\n",
    "        torch.cuda.synchronize()\n",
    "        flash_time = (time.time() - start) / 10\n",
    "        flash_mem = torch.cuda.max_memory_allocated() / 1e6\n",
    "        \n",
    "        # Standard attention (for small sequences only)\n",
    "        if seq_len <= 2048:\n",
    "            Q_std = Q[:, 0, :, :]  # Single head for comparison\n",
    "            K_std = K[:, 0, :, :]\n",
    "            V_std = V[:, 0, :, :]\n",
    "            \n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "            start = time.time()\n",
    "            for _ in range(10):\n",
    "                std_out, _ = standard_attention(Q_std, K_std, V_std)\n",
    "            torch.cuda.synchronize()\n",
    "            std_time = (time.time() - start) / 10\n",
    "            std_mem = torch.cuda.max_memory_allocated() / 1e6\n",
    "            \n",
    "            speedup = std_time / flash_time\n",
    "        else:\n",
    "            std_time = float('nan')\n",
    "            std_mem = float('nan')\n",
    "            speedup = float('nan')\n",
    "        \n",
    "        results['seq_len'].append(seq_len)\n",
    "        results['standard_time'].append(std_time * 1000)  # ms\n",
    "        results['flash_time'].append(flash_time * 1000)\n",
    "        results['standard_mem'].append(std_mem)\n",
    "        results['flash_mem'].append(flash_mem)\n",
    "        results['speedup'].append(speedup)\n",
    "    \n",
    "    # Plot results\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # Time comparison\n",
    "    axes[0].plot(results['seq_len'][:3], results['standard_time'][:3], 'r-o', label='Standard', linewidth=2)\n",
    "    axes[0].plot(results['seq_len'], results['flash_time'], 'b-s', label='Flash', linewidth=2)\n",
    "    axes[0].set_xlabel('Sequence Length', fontsize=12)\n",
    "    axes[0].set_ylabel('Time (ms)', fontsize=12)\n",
    "    axes[0].set_title('Forward Pass Time', fontsize=14)\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Memory comparison\n",
    "    axes[1].plot(results['seq_len'][:3], results['standard_mem'][:3], 'r-o', label='Standard', linewidth=2)\n",
    "    axes[1].plot(results['seq_len'], results['flash_mem'], 'b-s', label='Flash', linewidth=2)\n",
    "    axes[1].set_xlabel('Sequence Length', fontsize=12)\n",
    "    axes[1].set_ylabel('Peak Memory (MB)', fontsize=12)\n",
    "    axes[1].set_title('Memory Usage', fontsize=14)\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Speedup\n",
    "    valid_speedups = [s for s in results['speedup'][:3] if not np.isnan(s)]\n",
    "    axes[2].bar(range(len(valid_speedups)), valid_speedups, color='green', alpha=0.7)\n",
    "    axes[2].set_xticks(range(len(valid_speedups)))\n",
    "    axes[2].set_xticklabels(results['seq_len'][:len(valid_speedups)])\n",
    "    axes[2].set_xlabel('Sequence Length', fontsize=12)\n",
    "    axes[2].set_ylabel('Speedup (x)', fontsize=12)\n",
    "    axes[2].set_title('Flash Attention Speedup', fontsize=14)\n",
    "    axes[2].axhline(y=1, color='r', linestyle='--', label='Baseline')\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüìä Benchmark Results:\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"{'Seq Len':>10} {'Standard (ms)':>15} {'Flash (ms)':>15} {'Speedup':>10} {'Mem Saved':>10}\")\n",
    "    print(\"=\"*70)\n",
    "    for i in range(len(results['seq_len'])):\n",
    "        if not np.isnan(results['speedup'][i]):\n",
    "            mem_saved = (1 - results['flash_mem'][i] / results['standard_mem'][i]) * 100\n",
    "            print(f\"{results['seq_len'][i]:>10} {results['standard_time'][i]:>14.2f} {results['flash_time'][i]:>14.2f} \"\n",
    "                  f\"{results['speedup'][i]:>9.2f}x {mem_saved:>8.1f}%\")\n",
    "        else:\n",
    "            print(f\"{results['seq_len'][i]:>10} {'N/A':>14} {results['flash_time'][i]:>14.2f} {'N/A':>10} {'N/A':>10}\")\n",
    "\n",
    "comprehensive_benchmark()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Standard attention has O(n¬≤) memory** - the bottleneck for long sequences\n",
    "2. **Flash Attention uses tiling and recomputation** to achieve O(n) memory\n",
    "3. **PyTorch SDPA automatically uses Flash Attention** - just one function call!\n",
    "4. **FlexAttention (2.5+) enables custom patterns** with Flash Attention speed\n",
    "5. **cuDNN Fused Flash Attention (H100)** provides 75% additional speedup\n",
    "6. **Always use Flash Attention for seq_len > 512** and GPU training\n",
    "7. **2-4x faster, 10-20x less memory** - enables longer contexts\n",
    "\n",
    "## Modern LLM Usage\n",
    "\n",
    "- **GPT-4**: Flash Attention for 32k+ context\n",
    "- **Claude 3**: 200k context with optimized attention\n",
    "- **LLaMA 3**: Flash Attention 2 standard\n",
    "- **Mistral**: Flash Attention + GQA combination\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Continue to: [Topic 12: Grouped Query Attention (GQA)](12_grouped_query_attention.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "- [Flash Attention Paper](https://arxiv.org/abs/2205.14135)\n",
    "- [Flash Attention 2 Paper](https://arxiv.org/abs/2307.08691)\n",
    "- [PyTorch SDPA Documentation](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html)\n",
    "- [PyTorch 2.5 FlexAttention](https://pytorch.org/blog/pytorch2-5/)\n",
    "- [cuDNN Flash Attention](https://docs.nvidia.com/deeplearning/cudnn/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
