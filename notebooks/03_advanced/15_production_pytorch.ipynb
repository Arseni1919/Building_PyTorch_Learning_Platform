{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Topic 15: Production PyTorch Best Practices\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this notebook, you will:\n",
        "- Master production-ready project structure and organization\n",
        "- Learn model checkpointing strategies for fault tolerance\n",
        "- Understand quantization techniques (int8, int4, GPTQ, AWQ)\n",
        "- Export models to ONNX for deployment\n",
        "- Compare TorchScript vs ONNX vs torch.compile\n",
        "- Master debugging techniques and common pitfalls\n",
        "- Implement monitoring and logging for production systems\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. The Big Picture: Production vs Research Code\n",
        "\n",
        "### Research Code vs Production Code\n",
        "\n",
        "**Research/Prototype**:\n",
        "- Single notebook or script\n",
        "- Hardcoded paths and hyperparameters\n",
        "- No error handling\n",
        "- Ad-hoc logging\n",
        "- Manual checkpointing\n",
        "- Works on your machine\n",
        "\n",
        "**Production**:\n",
        "- Modular, testable codebase\n",
        "- Configuration files for all settings\n",
        "- Comprehensive error handling\n",
        "- Structured logging and monitoring\n",
        "- Automatic checkpointing and recovery\n",
        "- Works anywhere, at scale\n",
        "\n",
        "### Production Requirements\n",
        "\n",
        "**Reliability**:\n",
        "- ‚úÖ Fault tolerance (recover from crashes)\n",
        "- ‚úÖ Reproducibility (same inputs ‚Üí same outputs)\n",
        "- ‚úÖ Monitoring (know when things go wrong)\n",
        "- ‚úÖ Testing (catch bugs before deployment)\n",
        "\n",
        "**Performance**:\n",
        "- ‚úÖ Efficient inference (low latency, high throughput)\n",
        "- ‚úÖ Memory optimization (quantization, pruning)\n",
        "- ‚úÖ Batching and caching strategies\n",
        "\n",
        "**Maintainability**:\n",
        "- ‚úÖ Clean code structure\n",
        "- ‚úÖ Documentation\n",
        "- ‚úÖ Version control\n",
        "- ‚úÖ Easy updates and rollbacks\n",
        "\n",
        "### What We'll Cover\n",
        "\n",
        "1. **Project Structure**: How to organize production PyTorch projects\n",
        "2. **Checkpointing**: Save/load strategies for training and inference\n",
        "3. **Quantization**: Compress models for efficient deployment\n",
        "4. **Export & Deployment**: ONNX, TorchScript, serving strategies\n",
        "5. **Debugging**: Common issues and how to fix them\n",
        "6. **Monitoring**: Track model performance in production\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import json\n",
        "import time\n",
        "from pathlib import Path\n",
        "from typing import Dict, Any, Optional\n",
        "import warnings\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 2. Production Project Structure\n",
        "\n",
        "### Recommended Directory Layout\n",
        "\n",
        "```\n",
        "my_project/\n",
        "‚îú‚îÄ‚îÄ configs/\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ base_config.yaml\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ train_config.yaml\n",
        "‚îÇ   ‚îî‚îÄ‚îÄ inference_config.yaml\n",
        "‚îú‚îÄ‚îÄ src/\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ models/\n",
        "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n",
        "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ transformer.py\n",
        "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ attention.py\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ data/\n",
        "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n",
        "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dataset.py\n",
        "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ preprocessing.py\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ training/\n",
        "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py\n",
        "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ trainer.py\n",
        "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ callbacks.py\n",
        "‚îÇ   ‚îî‚îÄ‚îÄ utils/\n",
        "‚îÇ       ‚îú‚îÄ‚îÄ __init__.py\n",
        "‚îÇ       ‚îú‚îÄ‚îÄ logging.py\n",
        "‚îÇ       ‚îî‚îÄ‚îÄ metrics.py\n",
        "‚îú‚îÄ‚îÄ checkpoints/\n",
        "‚îú‚îÄ‚îÄ logs/\n",
        "‚îú‚îÄ‚îÄ tests/\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ test_model.py\n",
        "‚îÇ   ‚îî‚îÄ‚îÄ test_data.py\n",
        "‚îú‚îÄ‚îÄ scripts/\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ train.py\n",
        "‚îÇ   ‚îî‚îÄ‚îÄ inference.py\n",
        "‚îú‚îÄ‚îÄ requirements.txt\n",
        "‚îú‚îÄ‚îÄ setup.py\n",
        "‚îî‚îÄ‚îÄ README.md\n",
        "```\n",
        "\n",
        "### Why This Structure?\n",
        "\n",
        "- **Modularity**: Easy to find and modify components\n",
        "- **Testability**: Clear separation allows unit testing\n",
        "- **Reusability**: Import modules across scripts\n",
        "- **Scalability**: Add new features without breaking existing code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Configuration Management\n",
        "\n",
        "class Config:\n",
        "    \"\"\"Central configuration class for training and inference\"\"\"\n",
        "    \n",
        "    def __init__(self, config_dict: Optional[Dict[str, Any]] = None):\n",
        "        # Model architecture\n",
        "        self.d_model = 512\n",
        "        self.num_heads = 8\n",
        "        self.num_layers = 6\n",
        "        self.d_ff = 2048\n",
        "        self.dropout = 0.1\n",
        "        \n",
        "        # Training\n",
        "        self.batch_size = 32\n",
        "        self.learning_rate = 1e-4\n",
        "        self.num_epochs = 10\n",
        "        self.gradient_clip = 1.0\n",
        "        self.warmup_steps = 1000\n",
        "        \n",
        "        # Optimization\n",
        "        self.use_amp = True\n",
        "        self.use_compile = True\n",
        "        self.gradient_accumulation_steps = 1\n",
        "        \n",
        "        # Checkpointing\n",
        "        self.checkpoint_dir = \"./checkpoints\"\n",
        "        self.save_every_n_steps = 1000\n",
        "        self.keep_last_n_checkpoints = 3\n",
        "        \n",
        "        # Logging\n",
        "        self.log_dir = \"./logs\"\n",
        "        self.log_every_n_steps = 100\n",
        "        \n",
        "        # Random seed for reproducibility\n",
        "        self.seed = 42\n",
        "        \n",
        "        # Override with provided config\n",
        "        if config_dict:\n",
        "            self.__dict__.update(config_dict)\n",
        "    \n",
        "    def save(self, path: str):\n",
        "        \"\"\"Save configuration to JSON\"\"\"\n",
        "        with open(path, 'w') as f:\n",
        "            json.dump(self.__dict__, f, indent=2)\n",
        "    \n",
        "    @classmethod\n",
        "    def load(cls, path: str):\n",
        "        \"\"\"Load configuration from JSON\"\"\"\n",
        "        with open(path, 'r') as f:\n",
        "            config_dict = json.load(f)\n",
        "        return cls(config_dict)\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return f\"Config({json.dumps(self.__dict__, indent=2)})\"\n",
        "\n",
        "\n",
        "# Demo configuration\n",
        "config = Config()\n",
        "print(\"Default Configuration:\")\n",
        "print(\"=\"*60)\n",
        "for key, value in config.__dict__.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "\n",
        "print(\"\\nüí° Configuration management benefits:\")\n",
        "print(\"   - All settings in one place\")\n",
        "print(\"   - Easy to save/load experiments\")\n",
        "print(\"   - Version control friendly\")\n",
        "print(\"   - Override specific values easily\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 3. Model Checkpointing\n",
        "\n",
        "### Why Checkpoint?\n",
        "\n",
        "**Problems without checkpointing**:\n",
        "- Training crashes ‚Üí lose all progress\n",
        "- Can't resume training\n",
        "- Can't compare models across epochs\n",
        "- No rollback if model degrades\n",
        "\n",
        "### What to Save\n",
        "\n",
        "A complete checkpoint should include:\n",
        "1. **Model state**: `model.state_dict()`\n",
        "2. **Optimizer state**: `optimizer.state_dict()`\n",
        "3. **Scheduler state**: `scheduler.state_dict()` (if using)\n",
        "4. **Training step/epoch**: For resuming\n",
        "5. **Config**: To reproduce exact setup\n",
        "6. **Random states**: For full reproducibility\n",
        "7. **Metrics**: Best loss, accuracy, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CheckpointManager:\n",
        "    \"\"\"Manages model checkpoints with best model tracking\"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        checkpoint_dir: str,\n",
        "        keep_last_n: int = 3,\n",
        "        keep_best: bool = True\n",
        "    ):\n",
        "        self.checkpoint_dir = Path(checkpoint_dir)\n",
        "        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
        "        self.keep_last_n = keep_last_n\n",
        "        self.keep_best = keep_best\n",
        "        self.best_metric = float('inf')\n",
        "    \n",
        "    def save_checkpoint(\n",
        "        self,\n",
        "        model: nn.Module,\n",
        "        optimizer: torch.optim.Optimizer,\n",
        "        step: int,\n",
        "        epoch: int,\n",
        "        config: Config,\n",
        "        metrics: Dict[str, float],\n",
        "        scheduler: Optional[Any] = None\n",
        "    ) -> str:\n",
        "        \"\"\"Save a complete checkpoint\"\"\"\n",
        "        \n",
        "        checkpoint = {\n",
        "            'step': step,\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'metrics': metrics,\n",
        "            'config': config.__dict__,\n",
        "            'pytorch_version': torch.__version__,\n",
        "        }\n",
        "        \n",
        "        # Add scheduler if provided\n",
        "        if scheduler is not None:\n",
        "            checkpoint['scheduler_state_dict'] = scheduler.state_dict()\n",
        "        \n",
        "        # Add random states for reproducibility\n",
        "        checkpoint['random_state'] = {\n",
        "            'torch': torch.get_rng_state(),\n",
        "            'numpy': np.random.get_state(),\n",
        "        }\n",
        "        if torch.cuda.is_available():\n",
        "            checkpoint['random_state']['cuda'] = torch.cuda.get_rng_state()\n",
        "        \n",
        "        # Save checkpoint\n",
        "        checkpoint_path = self.checkpoint_dir / f\"checkpoint_step_{step}.pt\"\n",
        "        torch.save(checkpoint, checkpoint_path)\n",
        "        \n",
        "        # Save as best if metric improved\n",
        "        current_metric = metrics.get('loss', float('inf'))\n",
        "        if self.keep_best and current_metric < self.best_metric:\n",
        "            self.best_metric = current_metric\n",
        "            best_path = self.checkpoint_dir / \"best_checkpoint.pt\"\n",
        "            torch.save(checkpoint, best_path)\n",
        "            print(f\"üíö New best checkpoint! Loss: {current_metric:.4f}\")\n",
        "        \n",
        "        # Clean up old checkpoints\n",
        "        self._cleanup_old_checkpoints()\n",
        "        \n",
        "        return str(checkpoint_path)\n",
        "    \n",
        "    def load_checkpoint(\n",
        "        self,\n",
        "        checkpoint_path: str,\n",
        "        model: nn.Module,\n",
        "        optimizer: Optional[torch.optim.Optimizer] = None,\n",
        "        scheduler: Optional[Any] = None,\n",
        "        restore_random_state: bool = True\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"Load checkpoint and restore training state\"\"\"\n",
        "        \n",
        "        checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
        "        \n",
        "        # Load model\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        \n",
        "        # Load optimizer if provided\n",
        "        if optimizer is not None:\n",
        "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        \n",
        "        # Load scheduler if provided\n",
        "        if scheduler is not None and 'scheduler_state_dict' in checkpoint:\n",
        "            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "        \n",
        "        # Restore random states for reproducibility\n",
        "        if restore_random_state and 'random_state' in checkpoint:\n",
        "            torch.set_rng_state(checkpoint['random_state']['torch'])\n",
        "            np.random.set_state(checkpoint['random_state']['numpy'])\n",
        "            if torch.cuda.is_available() and 'cuda' in checkpoint['random_state']:\n",
        "                torch.cuda.set_rng_state(checkpoint['random_state']['cuda'])\n",
        "        \n",
        "        print(f\"‚úÖ Loaded checkpoint from step {checkpoint['step']}, epoch {checkpoint['epoch']}\")\n",
        "        print(f\"   Metrics: {checkpoint['metrics']}\")\n",
        "        \n",
        "        return checkpoint\n",
        "    \n",
        "    def _cleanup_old_checkpoints(self):\n",
        "        \"\"\"Keep only last N checkpoints\"\"\"\n",
        "        checkpoints = sorted(\n",
        "            self.checkpoint_dir.glob(\"checkpoint_step_*.pt\"),\n",
        "            key=lambda p: int(p.stem.split('_')[-1])\n",
        "        )\n",
        "        \n",
        "        # Remove old checkpoints\n",
        "        for checkpoint in checkpoints[:-self.keep_last_n]:\n",
        "            checkpoint.unlink()\n",
        "    \n",
        "    def list_checkpoints(self) -> list:\n",
        "        \"\"\"List all available checkpoints\"\"\"\n",
        "        checkpoints = list(self.checkpoint_dir.glob(\"*.pt\"))\n",
        "        return sorted(checkpoints)\n",
        "\n",
        "\n",
        "# Demo checkpointing\n",
        "print(\"Checkpoint Management Demo\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Create dummy model and optimizer\n",
        "model = nn.Linear(10, 10)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "config = Config()\n",
        "\n",
        "# Create checkpoint manager\n",
        "ckpt_manager = CheckpointManager(\"/tmp/pytorch_checkpoints\", keep_last_n=3)\n",
        "\n",
        "# Simulate training and saving\n",
        "print(\"\\nSimulating training...\")\n",
        "for step in range(0, 5000, 1000):\n",
        "    metrics = {'loss': 1.0 / (step + 1), 'accuracy': step / 5000}\n",
        "    \n",
        "    path = ckpt_manager.save_checkpoint(\n",
        "        model, optimizer, step, step // 1000, config, metrics\n",
        "    )\n",
        "    print(f\"Saved: {path}\")\n",
        "\n",
        "# List all checkpoints\n",
        "print(\"\\nAvailable checkpoints:\")\n",
        "for ckpt in ckpt_manager.list_checkpoints():\n",
        "    print(f\"  {ckpt.name}\")\n",
        "\n",
        "print(\"\\nüí° Checkpointing best practices:\")\n",
        "print(\"   - Save frequently (every N steps)\")\n",
        "print(\"   - Keep best checkpoint separate\")\n",
        "print(\"   - Clean up old checkpoints to save disk\")\n",
        "print(\"   - Include all state for perfect resumption\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 4. Model Quantization\n",
        "\n",
        "### Why Quantize?\n",
        "\n",
        "**float32 model**:\n",
        "- 7B parameters √ó 4 bytes = 28 GB\n",
        "- Slow inference\n",
        "- Expensive to serve\n",
        "\n",
        "**int8 quantized**:\n",
        "- 7B parameters √ó 1 byte = 7 GB (4x smaller!)\n",
        "- 2-4x faster inference\n",
        "- Much cheaper deployment\n",
        "\n",
        "### Quantization Types\n",
        "\n",
        "1. **Post-Training Quantization (PTQ)**:\n",
        "   - Quantize after training\n",
        "   - No retraining needed\n",
        "   - Slight quality loss\n",
        "\n",
        "2. **Quantization-Aware Training (QAT)**:\n",
        "   - Train with quantization in mind\n",
        "   - Better quality\n",
        "   - More compute intensive\n",
        "\n",
        "3. **Weight-Only Quantization**:\n",
        "   - Quantize weights, keep activations in float\n",
        "   - Good for memory-bound models\n",
        "\n",
        "### Popular Quantization Methods (2025)\n",
        "\n",
        "- **int8**: Standard, widely supported\n",
        "- **int4**: Extreme compression (GPTQ, AWQ)\n",
        "- **bfloat16**: Not quantization, but effective compression\n",
        "- **GGUF**: Optimized for CPU inference (llama.cpp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dynamic Quantization (simplest approach)\n",
        "\n",
        "class SimpleModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(512, 512)\n",
        "        self.linear2 = nn.Linear(512, 512)\n",
        "        self.linear3 = nn.Linear(512, 10)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.linear1(x))\n",
        "        x = F.relu(self.linear2(x))\n",
        "        return self.linear3(x)\n",
        "\n",
        "\n",
        "print(\"Dynamic Quantization Demo\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Create model\n",
        "model_fp32 = SimpleModel()\n",
        "\n",
        "# Get model size\n",
        "def get_model_size(model):\n",
        "    \"\"\"Get model size in MB\"\"\"\n",
        "    param_size = sum(p.nelement() * p.element_size() for p in model.parameters())\n",
        "    buffer_size = sum(b.nelement() * b.element_size() for b in model.buffers())\n",
        "    return (param_size + buffer_size) / 1024**2\n",
        "\n",
        "fp32_size = get_model_size(model_fp32)\n",
        "\n",
        "# Dynamic quantization\n",
        "model_int8 = torch.quantization.quantize_dynamic(\n",
        "    model_fp32,\n",
        "    {nn.Linear},  # Quantize Linear layers\n",
        "    dtype=torch.qint8\n",
        ")\n",
        "\n",
        "int8_size = get_model_size(model_int8)\n",
        "\n",
        "print(f\"Model sizes:\")\n",
        "print(f\"  float32: {fp32_size:.2f} MB\")\n",
        "print(f\"  int8: {int8_size:.2f} MB\")\n",
        "print(f\"  Compression ratio: {fp32_size / int8_size:.2f}x\")\n",
        "\n",
        "# Test inference\n",
        "x = torch.randn(32, 512)\n",
        "\n",
        "# float32 inference\n",
        "with torch.no_grad():\n",
        "    start = time.time()\n",
        "    for _ in range(100):\n",
        "        _ = model_fp32(x)\n",
        "    fp32_time = (time.time() - start) / 100\n",
        "\n",
        "# int8 inference\n",
        "with torch.no_grad():\n",
        "    start = time.time()\n",
        "    for _ in range(100):\n",
        "        _ = model_int8(x)\n",
        "    int8_time = (time.time() - start) / 100\n",
        "\n",
        "print(f\"\\nInference time:\")\n",
        "print(f\"  float32: {fp32_time*1000:.2f} ms\")\n",
        "print(f\"  int8: {int8_time*1000:.2f} ms\")\n",
        "print(f\"  Speedup: {fp32_time / int8_time:.2f}x\")\n",
        "\n",
        "# Check accuracy\n",
        "with torch.no_grad():\n",
        "    out_fp32 = model_fp32(x)\n",
        "    out_int8 = model_int8(x)\n",
        "    diff = (out_fp32 - out_int8).abs().mean().item()\n",
        "\n",
        "print(f\"\\nAccuracy:\")\n",
        "print(f\"  Mean absolute difference: {diff:.6f}\")\n",
        "print(f\"  Relative error: {diff / out_fp32.abs().mean().item() * 100:.2f}%\")\n",
        "\n",
        "print(\"\\nüí° Dynamic quantization benefits:\")\n",
        "print(\"   - 3-4x smaller model size\")\n",
        "print(\"   - 2-3x faster inference (CPU)\")\n",
        "print(\"   - Minimal accuracy loss\")\n",
        "print(\"   - No retraining needed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 5. Model Export: ONNX\n",
        "\n",
        "### Why Export to ONNX?\n",
        "\n",
        "**ONNX (Open Neural Network Exchange)**:\n",
        "- ‚úÖ Platform-independent format\n",
        "- ‚úÖ Optimized inference engines (ONNX Runtime)\n",
        "- ‚úÖ Hardware acceleration (TensorRT, OpenVINO)\n",
        "- ‚úÖ Cross-framework compatibility\n",
        "\n",
        "**Use cases**:\n",
        "- Mobile deployment (iOS, Android)\n",
        "- Edge devices (Raspberry Pi, Jetson)\n",
        "- Web browsers (ONNX.js)\n",
        "- Production servers (ONNX Runtime)\n",
        "\n",
        "### Export Process\n",
        "\n",
        "1. Define input shapes (ONNX is static)\n",
        "2. Export using `torch.onnx.export()`\n",
        "3. Verify exported model\n",
        "4. Optimize with ONNX tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import io\n",
        "\n",
        "print(\"ONNX Export Demo\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Create a simple model\n",
        "model = SimpleModel()\n",
        "model.eval()\n",
        "\n",
        "# Define dummy input (must match expected input shape)\n",
        "dummy_input = torch.randn(1, 512)\n",
        "\n",
        "# Export to ONNX\n",
        "onnx_path = \"/tmp/model.onnx\"\n",
        "\n",
        "print(\"Exporting to ONNX...\")\n",
        "torch.onnx.export(\n",
        "    model,\n",
        "    dummy_input,\n",
        "    onnx_path,\n",
        "    export_params=True,\n",
        "    opset_version=14,  # ONNX opset version\n",
        "    do_constant_folding=True,  # Optimization\n",
        "    input_names=['input'],\n",
        "    output_names=['output'],\n",
        "    dynamic_axes={\n",
        "        'input': {0: 'batch_size'},  # Variable batch size\n",
        "        'output': {0: 'batch_size'}\n",
        "    }\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Model exported to {onnx_path}\")\n",
        "\n",
        "# Verify ONNX model\n",
        "try:\n",
        "    import onnx\n",
        "    import onnxruntime as ort\n",
        "    \n",
        "    # Load and check\n",
        "    onnx_model = onnx.load(onnx_path)\n",
        "    onnx.checker.check_model(onnx_model)\n",
        "    print(\"\\n‚úÖ ONNX model is valid\")\n",
        "    \n",
        "    # Run inference with ONNX Runtime\n",
        "    ort_session = ort.InferenceSession(onnx_path)\n",
        "    \n",
        "    # Prepare input\n",
        "    ort_inputs = {ort_session.get_inputs()[0].name: dummy_input.numpy()}\n",
        "    \n",
        "    # Run inference\n",
        "    ort_outputs = ort_session.run(None, ort_inputs)\n",
        "    \n",
        "    # Compare with PyTorch\n",
        "    with torch.no_grad():\n",
        "        torch_output = model(dummy_input).numpy()\n",
        "    \n",
        "    diff = np.abs(torch_output - ort_outputs[0]).max()\n",
        "    print(f\"\\nPyTorch vs ONNX difference: {diff:.6f}\")\n",
        "    \n",
        "    # Benchmark\n",
        "    print(\"\\nBenchmarking...\")\n",
        "    \n",
        "    # PyTorch\n",
        "    with torch.no_grad():\n",
        "        start = time.time()\n",
        "        for _ in range(1000):\n",
        "            _ = model(dummy_input)\n",
        "        torch_time = (time.time() - start) / 1000\n",
        "    \n",
        "    # ONNX Runtime\n",
        "    start = time.time()\n",
        "    for _ in range(1000):\n",
        "        _ = ort_session.run(None, ort_inputs)\n",
        "    onnx_time = (time.time() - start) / 1000\n",
        "    \n",
        "    print(f\"  PyTorch: {torch_time*1000:.2f} ms\")\n",
        "    print(f\"  ONNX Runtime: {onnx_time*1000:.2f} ms\")\n",
        "    print(f\"  Speedup: {torch_time / onnx_time:.2f}x\")\n",
        "    \n",
        "except ImportError:\n",
        "    print(\"\\n‚ö†Ô∏è onnx and onnxruntime not installed\")\n",
        "    print(\"   Install with: pip install onnx onnxruntime\")\n",
        "\n",
        "print(\"\\nüí° ONNX benefits:\")\n",
        "print(\"   - Cross-platform deployment\")\n",
        "print(\"   - Optimized inference engines\")\n",
        "print(\"   - Hardware acceleration support\")\n",
        "print(\"   - Production-ready tooling\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 6. Debugging PyTorch Models\n",
        "\n",
        "### Common Issues and Solutions\n",
        "\n",
        "#### 1. NaN/Inf in Training\n",
        "\n",
        "**Causes**:\n",
        "- Learning rate too high\n",
        "- Gradient explosion\n",
        "- Numerical instability (log(0), division by zero)\n",
        "- Mixed precision underflow\n",
        "\n",
        "**Solutions**:\n",
        "- Use gradient clipping\n",
        "- Lower learning rate\n",
        "- Check for inf/nan after each operation\n",
        "- Use `torch.autograd.detect_anomaly()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def debug_nan_inf(model, x, target, optimizer):\n",
        "    \"\"\"Helper to debug NaN/Inf issues\"\"\"\n",
        "    \n",
        "    # Enable anomaly detection\n",
        "    with torch.autograd.detect_anomaly():\n",
        "        # Forward pass\n",
        "        output = model(x)\n",
        "        loss = F.mse_loss(output, target)\n",
        "        \n",
        "        # Check for NaN/Inf\n",
        "        if torch.isnan(loss) or torch.isinf(loss):\n",
        "            print(\"‚ùå NaN/Inf detected in loss!\")\n",
        "            print(f\"   Output stats: min={output.min():.4f}, max={output.max():.4f}\")\n",
        "            print(f\"   Target stats: min={target.min():.4f}, max={target.max():.4f}\")\n",
        "            return False\n",
        "        \n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        \n",
        "        # Check gradients\n",
        "        for name, param in model.named_parameters():\n",
        "            if param.grad is not None:\n",
        "                if torch.isnan(param.grad).any():\n",
        "                    print(f\"‚ùå NaN in gradients of {name}\")\n",
        "                    return False\n",
        "                if torch.isinf(param.grad).any():\n",
        "                    print(f\"‚ùå Inf in gradients of {name}\")\n",
        "                    return False\n",
        "                \n",
        "                grad_norm = param.grad.norm()\n",
        "                if grad_norm > 100:\n",
        "                    print(f\"‚ö†Ô∏è Large gradient in {name}: {grad_norm:.2f}\")\n",
        "        \n",
        "        # Apply gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        \n",
        "        optimizer.step()\n",
        "    \n",
        "    return True\n",
        "\n",
        "\n",
        "# Demo debugging\n",
        "print(\"Debugging NaN/Inf Issues\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "model = SimpleModel()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# Normal case\n",
        "x = torch.randn(32, 512)\n",
        "target = torch.randn(32, 10)\n",
        "\n",
        "print(\"Testing normal data...\")\n",
        "success = debug_nan_inf(model, x, target, optimizer)\n",
        "print(f\"Result: {'‚úÖ OK' if success else '‚ùå Failed'}\")\n",
        "\n",
        "# Pathological case (very large values)\n",
        "x_bad = torch.randn(32, 512) * 1e10  # Extremely large values\n",
        "target_bad = torch.randn(32, 10)\n",
        "\n",
        "print(\"\\nTesting pathological data (very large values)...\")\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "    success = debug_nan_inf(model, x_bad, target_bad, optimizer)\n",
        "    print(f\"Result: {'‚úÖ OK' if success else '‚ùå Failed'}\")\n",
        "\n",
        "print(\"\\nüí° Debugging tips:\")\n",
        "print(\"   - Always use gradient clipping\")\n",
        "print(\"   - Check data ranges before training\")\n",
        "print(\"   - Use torch.autograd.detect_anomaly() to find source\")\n",
        "print(\"   - Log gradient norms to monitor stability\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Memory Issues\n",
        "\n",
        "**Out of Memory (OOM) errors**:\n",
        "\n",
        "**Causes**:\n",
        "- Batch size too large\n",
        "- Model too large\n",
        "- Gradient accumulation\n",
        "- Memory leaks (detach issues)\n",
        "\n",
        "**Solutions**:\n",
        "- Reduce batch size\n",
        "- Use gradient checkpointing\n",
        "- Use mixed precision (AMP)\n",
        "- Clear cache: `torch.cuda.empty_cache()`\n",
        "- Profile memory usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def profile_memory_usage(model, batch_sizes=[8, 16, 32, 64]):\n",
        "    \"\"\"Profile memory usage for different batch sizes\"\"\"\n",
        "    \n",
        "    if not torch.cuda.is_available():\n",
        "        print(\"‚ö†Ô∏è CUDA not available, skipping memory profiling\")\n",
        "        return\n",
        "    \n",
        "    print(\"Memory Usage Profile\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"{'Batch Size':>12} {'Peak Memory (MB)':>20} {'Success':>10}\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    model = model.to('cuda')\n",
        "    \n",
        "    for batch_size in batch_sizes:\n",
        "        torch.cuda.reset_peak_memory_stats()\n",
        "        torch.cuda.empty_cache()\n",
        "        \n",
        "        try:\n",
        "            x = torch.randn(batch_size, 512, device='cuda')\n",
        "            target = torch.randn(batch_size, 10, device='cuda')\n",
        "            \n",
        "            output = model(x)\n",
        "            loss = F.mse_loss(output, target)\n",
        "            loss.backward()\n",
        "            \n",
        "            peak_mem = torch.cuda.max_memory_allocated() / 1024**2\n",
        "            print(f\"{batch_size:>12} {peak_mem:>19.2f} {'‚úÖ':>10}\")\n",
        "            \n",
        "        except RuntimeError as e:\n",
        "            if \"out of memory\" in str(e):\n",
        "                print(f\"{batch_size:>12} {'OOM':>19} {'‚ùå':>10}\")\n",
        "            else:\n",
        "                raise\n",
        "    \n",
        "    print(\"\\nüí° Memory optimization tips:\")\n",
        "    print(\"   - Use mixed precision (AMP) for 2x memory savings\")\n",
        "    print(\"   - Enable gradient checkpointing for large models\")\n",
        "    print(\"   - Use gradient accumulation instead of large batches\")\n",
        "    print(\"   - Clear cache between runs: torch.cuda.empty_cache()\")\n",
        "\n",
        "\n",
        "# Demo memory profiling\n",
        "profile_memory_usage(SimpleModel())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 7. Production Monitoring\n",
        "\n",
        "### What to Monitor\n",
        "\n",
        "**During Training**:\n",
        "- Loss curves (train and validation)\n",
        "- Learning rate\n",
        "- Gradient norms\n",
        "- Weight norms\n",
        "- GPU utilization\n",
        "- Memory usage\n",
        "\n",
        "**During Inference**:\n",
        "- Latency (p50, p95, p99)\n",
        "- Throughput (requests/sec)\n",
        "- Error rates\n",
        "- Model confidence scores\n",
        "- Input/output distributions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TrainingMonitor:\n",
        "    \"\"\"Monitor training metrics and detect issues\"\"\"\n",
        "    \n",
        "    def __init__(self, window_size: int = 100):\n",
        "        self.window_size = window_size\n",
        "        self.metrics = {\n",
        "            'loss': [],\n",
        "            'learning_rate': [],\n",
        "            'grad_norm': [],\n",
        "            'weight_norm': []\n",
        "        }\n",
        "    \n",
        "    def log_step(\n",
        "        self,\n",
        "        loss: float,\n",
        "        learning_rate: float,\n",
        "        model: nn.Module,\n",
        "        step: int\n",
        "    ):\n",
        "        \"\"\"Log metrics for current step\"\"\"\n",
        "        \n",
        "        # Compute gradient norm\n",
        "        grad_norm = 0.0\n",
        "        for param in model.parameters():\n",
        "            if param.grad is not None:\n",
        "                grad_norm += param.grad.norm().item() ** 2\n",
        "        grad_norm = grad_norm ** 0.5\n",
        "        \n",
        "        # Compute weight norm\n",
        "        weight_norm = sum(p.norm().item() for p in model.parameters())\n",
        "        \n",
        "        # Store metrics\n",
        "        self.metrics['loss'].append(loss)\n",
        "        self.metrics['learning_rate'].append(learning_rate)\n",
        "        self.metrics['grad_norm'].append(grad_norm)\n",
        "        self.metrics['weight_norm'].append(weight_norm)\n",
        "        \n",
        "        # Keep only recent window\n",
        "        for key in self.metrics:\n",
        "            self.metrics[key] = self.metrics[key][-self.window_size:]\n",
        "        \n",
        "        # Detect anomalies\n",
        "        self._check_anomalies(step)\n",
        "    \n",
        "    def _check_anomalies(self, step: int):\n",
        "        \"\"\"Check for training anomalies\"\"\"\n",
        "        \n",
        "        if len(self.metrics['loss']) < 10:\n",
        "            return  # Not enough data\n",
        "        \n",
        "        recent_loss = self.metrics['loss'][-10:]\n",
        "        \n",
        "        # Check for NaN\n",
        "        if any(np.isnan(recent_loss)):\n",
        "            print(f\"\\n‚ùå Step {step}: NaN detected in loss!\")\n",
        "        \n",
        "        # Check for loss explosion\n",
        "        if recent_loss[-1] > 10 * np.median(recent_loss[:-1]):\n",
        "            print(f\"\\n‚ö†Ô∏è Step {step}: Loss exploded!\")\n",
        "            print(f\"   Current: {recent_loss[-1]:.4f}\")\n",
        "            print(f\"   Median: {np.median(recent_loss[:-1]):.4f}\")\n",
        "        \n",
        "        # Check for vanishing gradients\n",
        "        recent_grad = self.metrics['grad_norm'][-10:]\n",
        "        if np.mean(recent_grad) < 1e-6:\n",
        "            print(f\"\\n‚ö†Ô∏è Step {step}: Vanishing gradients detected!\")\n",
        "            print(f\"   Mean gradient norm: {np.mean(recent_grad):.2e}\")\n",
        "        \n",
        "        # Check for exploding gradients\n",
        "        if recent_grad[-1] > 100:\n",
        "            print(f\"\\n‚ö†Ô∏è Step {step}: Exploding gradients!\")\n",
        "            print(f\"   Gradient norm: {recent_grad[-1]:.2f}\")\n",
        "    \n",
        "    def plot_metrics(self):\n",
        "        \"\"\"Plot training metrics\"\"\"\n",
        "        \n",
        "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "        \n",
        "        # Loss\n",
        "        axes[0, 0].plot(self.metrics['loss'], linewidth=2)\n",
        "        axes[0, 0].set_xlabel('Step', fontsize=12)\n",
        "        axes[0, 0].set_ylabel('Loss', fontsize=12)\n",
        "        axes[0, 0].set_title('Training Loss', fontsize=14)\n",
        "        axes[0, 0].grid(True, alpha=0.3)\n",
        "        \n",
        "        # Learning rate\n",
        "        axes[0, 1].plot(self.metrics['learning_rate'], linewidth=2, color='orange')\n",
        "        axes[0, 1].set_xlabel('Step', fontsize=12)\n",
        "        axes[0, 1].set_ylabel('Learning Rate', fontsize=12)\n",
        "        axes[0, 1].set_title('Learning Rate Schedule', fontsize=14)\n",
        "        axes[0, 1].grid(True, alpha=0.3)\n",
        "        \n",
        "        # Gradient norm\n",
        "        axes[1, 0].plot(self.metrics['grad_norm'], linewidth=2, color='green')\n",
        "        axes[1, 0].set_xlabel('Step', fontsize=12)\n",
        "        axes[1, 0].set_ylabel('Gradient Norm', fontsize=12)\n",
        "        axes[1, 0].set_title('Gradient Norm', fontsize=14)\n",
        "        axes[1, 0].grid(True, alpha=0.3)\n",
        "        \n",
        "        # Weight norm\n",
        "        axes[1, 1].plot(self.metrics['weight_norm'], linewidth=2, color='purple')\n",
        "        axes[1, 1].set_xlabel('Step', fontsize=12)\n",
        "        axes[1, 1].set_ylabel('Weight Norm', fontsize=12)\n",
        "        axes[1, 1].set_title('Weight Norm', fontsize=14)\n",
        "        axes[1, 1].grid(True, alpha=0.3)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "# Demo monitoring\n",
        "print(\"Training Monitoring Demo\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "model = SimpleModel()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "monitor = TrainingMonitor(window_size=100)\n",
        "\n",
        "print(\"\\nSimulating training...\")\n",
        "for step in range(200):\n",
        "    # Dummy training step\n",
        "    x = torch.randn(32, 512)\n",
        "    target = torch.randn(32, 10)\n",
        "    \n",
        "    output = model(x)\n",
        "    loss = F.mse_loss(output, target)\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    # Log metrics\n",
        "    monitor.log_step(\n",
        "        loss.item(),\n",
        "        optimizer.param_groups[0]['lr'],\n",
        "        model,\n",
        "        step\n",
        "    )\n",
        "    \n",
        "    if (step + 1) % 50 == 0:\n",
        "        print(f\"Step {step+1}: Loss={loss.item():.4f}\")\n",
        "\n",
        "print(\"\\n‚úÖ Training complete!\")\n",
        "monitor.plot_metrics()\n",
        "\n",
        "print(\"\\nüí° Monitoring best practices:\")\n",
        "print(\"   - Track multiple metrics, not just loss\")\n",
        "print(\"   - Set up anomaly detection for early warning\")\n",
        "print(\"   - Visualize trends to spot issues\")\n",
        "print(\"   - Log to persistent storage (TensorBoard, W&B)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Mini Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 1: Complete Checkpoint System\n",
        "\n",
        "Implement a training loop that:\n",
        "1. Saves checkpoints every 100 steps\n",
        "2. Keeps only last 3 checkpoints\n",
        "3. Can resume from checkpoint\n",
        "4. Saves best model based on validation loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 2: Quantization Comparison\n",
        "\n",
        "Compare dynamic quantization vs static quantization for a model.\n",
        "Measure: model size, inference speed, and accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 3: Production Inference API\n",
        "\n",
        "Create a simple inference API that:\n",
        "1. Loads a checkpoint\n",
        "2. Handles batching\n",
        "3. Logs latency\n",
        "4. Handles errors gracefully"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Key Takeaways\n",
        "\n",
        "1. **Structure matters**: Organize code for maintainability and scalability\n",
        "2. **Configuration over hardcoding**: Use config files for all hyperparameters\n",
        "3. **Checkpoint everything**: Model, optimizer, scheduler, random states\n",
        "4. **Quantize for deployment**: 4x smaller, 2-3x faster with minimal quality loss\n",
        "5. **Export to ONNX**: Cross-platform deployment and optimized inference\n",
        "6. **Monitor extensively**: Track metrics to catch issues early\n",
        "7. **Debug systematically**: Use profiling and anomaly detection tools\n",
        "\n",
        "## Production Checklist\n",
        "\n",
        "**Before deploying**:\n",
        "- ‚úÖ All hyperparameters in config files\n",
        "- ‚úÖ Comprehensive checkpointing\n",
        "- ‚úÖ Quantized model for efficiency\n",
        "- ‚úÖ ONNX export tested\n",
        "- ‚úÖ Error handling for all edge cases\n",
        "- ‚úÖ Monitoring and logging in place\n",
        "- ‚úÖ Unit tests for critical components\n",
        "- ‚úÖ Documentation complete\n",
        "\n",
        "## Modern LLM Production Practices (2025)\n",
        "\n",
        "**Model Serving**:\n",
        "- vLLM: High-throughput LLM serving\n",
        "- TensorRT-LLM: NVIDIA optimized inference\n",
        "- Hugging Face TGI: Production-ready serving\n",
        "\n",
        "**Quantization**:\n",
        "- GPTQ: 4-bit quantization for LLMs\n",
        "- AWQ: Activation-aware weight quantization\n",
        "- GGUF: CPU-optimized format (llama.cpp)\n",
        "\n",
        "**Monitoring**:\n",
        "- Weights & Biases: Experiment tracking\n",
        "- TensorBoard: Visualization\n",
        "- Prometheus + Grafana: Production metrics\n",
        "\n",
        "---\n",
        "\n",
        "## Further Reading\n",
        "\n",
        "- [PyTorch Production Best Practices](https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_models_for_inference.html)\n",
        "- [Quantization Documentation](https://pytorch.org/docs/stable/quantization.html)\n",
        "- [ONNX Documentation](https://onnx.ai/)\n",
        "- [TorchServe](https://pytorch.org/serve/)\n",
        "- [Model Optimization Guide](https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html)\n",
        "- [Production ML Best Practices](https://developers.google.com/machine-learning/guides/rules-of-ml)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
