{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Topic 12: Grouped Query Attention (GQA) - The Optimal Attention Architecture\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this notebook, you will:\n",
        "- Understand the evolution: Multi-Head ‚Üí Multi-Query ‚Üí Grouped Query Attention\n",
        "- Learn why GQA is the optimal trade-off for modern LLMs\n",
        "- Implement GQA from scratch with detailed step-by-step explanation\n",
        "- Calculate memory and compute savings for inference\n",
        "- Know how GQA is used in LLaMA 2/3, Mistral, and GPT-4\n",
        "- Understand KV cache optimization and its critical role\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. The Big Picture: Why GQA Was Invented\n",
        "\n",
        "### The Inference Bottleneck Problem\n",
        "\n",
        "When deploying LLMs in production, **autoregressive generation is slow**:\n",
        "- Generate one token at a time\n",
        "- For each token, must attend to **all previous tokens**\n",
        "- As sequence grows, attention becomes a bottleneck\n",
        "\n",
        "**Example**: Generating a 1000-token response\n",
        "- Token 1: Attend to 0 tokens\n",
        "- Token 500: Attend to 499 tokens\n",
        "- Token 1000: Attend to 999 tokens\n",
        "- **Average**: ~500 tokens of context per generation step!\n",
        "\n",
        "### The KV Cache Solution\n",
        "\n",
        "**Key insight**: We don't need to recompute Keys and Values for previous tokens!\n",
        "\n",
        "```\n",
        "Without KV cache:\n",
        "  Generate token 1: Compute K‚ÇÅ, V‚ÇÅ\n",
        "  Generate token 2: Compute K‚ÇÅ, V‚ÇÅ, K‚ÇÇ, V‚ÇÇ  ‚Üê Recomputing K‚ÇÅ, V‚ÇÅ!\n",
        "  Generate token 3: Compute K‚ÇÅ, V‚ÇÅ, K‚ÇÇ, V‚ÇÇ, K‚ÇÉ, V‚ÇÉ  ‚Üê Recomputing again!\n",
        "\n",
        "With KV cache:\n",
        "  Generate token 1: Compute K‚ÇÅ, V‚ÇÅ ‚Üí Store in cache\n",
        "  Generate token 2: Load K‚ÇÅ, V‚ÇÅ from cache, compute K‚ÇÇ, V‚ÇÇ ‚Üí Add to cache\n",
        "  Generate token 3: Load K‚ÇÅ, V‚ÇÅ, K‚ÇÇ, V‚ÇÇ from cache, compute K‚ÇÉ, V‚ÇÉ ‚Üí Add to cache\n",
        "```\n",
        "\n",
        "**Result**: Massive speedup! But...\n",
        "\n",
        "### The KV Cache Memory Problem\n",
        "\n",
        "**For Multi-Head Attention (MHA)**:\n",
        "- Model: 32 heads, d_model=4096, d_k=128 per head\n",
        "- Batch size: 16\n",
        "- Sequence length: 2048 tokens\n",
        "- Data type: float16 (2 bytes)\n",
        "\n",
        "**KV cache size**:\n",
        "```\n",
        "Size = batch √ó heads √ó seq_len √ó d_k √ó 2 (K and V) √ó 2 bytes\n",
        "     = 16 √ó 32 √ó 2048 √ó 128 √ó 2 √ó 2\n",
        "     = 536,870,912 bytes\n",
        "     = 512 MB per layer!\n",
        "```\n",
        "\n",
        "For a 40-layer model: **20 GB just for KV cache!**\n",
        "\n",
        "### The Evolution of Attention Architectures\n",
        "\n",
        "This led to three approaches:\n",
        "\n",
        "1. **Multi-Head Attention (MHA)** - Original Transformer (2017)\n",
        "   - Each head has its own Q, K, V\n",
        "   - Best quality, most memory\n",
        "\n",
        "2. **Multi-Query Attention (MQA)** - Fast Transformer Decoding (2019)\n",
        "   - All heads share single K, V\n",
        "   - Minimal memory, quality drop\n",
        "\n",
        "3. **Grouped Query Attention (GQA)** - LLaMA 2 (2023)\n",
        "   - Heads grouped to share K, V\n",
        "   - **Best of both worlds!**\n",
        "\n",
        "### Why GQA is Optimal\n",
        "\n",
        "GQA provides the **sweet spot**:\n",
        "- üíö **Quality**: Near-identical to MHA (< 0.1% perplexity difference)\n",
        "- üíö **Speed**: Close to MQA (30-40% faster than MHA)\n",
        "- üíö **Memory**: 4-8x less KV cache than MHA\n",
        "- üíö **Scalability**: Enables larger batches and longer contexts\n",
        "\n",
        "**Industry Adoption** (as of 2025):\n",
        "- ‚úÖ **LLaMA 2/3**: GQA with 8 groups\n",
        "- ‚úÖ **Mistral/Mixtral**: GQA standard\n",
        "- ‚úÖ **GPT-4**: Rumored to use GQA variant\n",
        "- ‚úÖ **Claude 3**: Likely uses GQA or similar\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "from typing import Optional, Tuple\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 2. Multi-Head Attention (MHA) - The Baseline\n",
        "\n",
        "### Architecture\n",
        "\n",
        "In standard MHA, each head has its own Q, K, V projections:\n",
        "\n",
        "```\n",
        "Input: (batch, seq_len, d_model)\n",
        "\n",
        "Head 1: Q‚ÇÅ, K‚ÇÅ, V‚ÇÅ  ‚Üí  Attention‚ÇÅ\n",
        "Head 2: Q‚ÇÇ, K‚ÇÇ, V‚ÇÇ  ‚Üí  Attention‚ÇÇ\n",
        "...\n",
        "Head n: Q‚Çô, K‚Çô, V‚Çô  ‚Üí  Attention‚Çô\n",
        "\n",
        "Concat all heads ‚Üí Linear projection ‚Üí Output\n",
        "```\n",
        "\n",
        "### Mathematical Formulation\n",
        "\n",
        "For each head $h \\in [1, n_{heads}]$:\n",
        "\n",
        "$$\n",
        "Q_h = X W_h^Q \\quad K_h = X W_h^K \\quad V_h = X W_h^V\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\text{head}_h = \\text{Attention}(Q_h, K_h, V_h) = \\text{softmax}\\left(\\frac{Q_h K_h^T}{\\sqrt{d_k}}\\right) V_h\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\text{MHA}(X) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_n) W^O\n",
        "$$\n",
        "\n",
        "### Parameter Count\n",
        "\n",
        "- $W^Q$: $(d_{model}, n_{heads} \\times d_k)$\n",
        "- $W^K$: $(d_{model}, n_{heads} \\times d_k)$\n",
        "- $W^V$: $(d_{model}, n_{heads} \\times d_k)$\n",
        "- $W^O$: $(n_{heads} \\times d_k, d_{model})$\n",
        "\n",
        "**Total**: $4 \\times d_{model} \\times (n_{heads} \\times d_k)$ parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"Standard Multi-Head Attention (MHA)\"\"\"\n",
        "    \n",
        "    def __init__(self, d_model: int, num_heads: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "        \n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "        \n",
        "        # Each head gets its own Q, K, V projections\n",
        "        self.W_q = nn.Linear(d_model, d_model)  # Projects to all Q heads\n",
        "        self.W_k = nn.Linear(d_model, d_model)  # Projects to all K heads\n",
        "        self.W_v = nn.Linear(d_model, d_model)  # Projects to all V heads\n",
        "        self.W_o = nn.Linear(d_model, d_model)  # Output projection\n",
        "        \n",
        "        self.dropout = dropout\n",
        "    \n",
        "    def forward(\n",
        "        self, \n",
        "        x: torch.Tensor,\n",
        "        mask: Optional[torch.Tensor] = None,\n",
        "        is_causal: bool = False,\n",
        "        use_cache: bool = False,\n",
        "        past_kv: Optional[Tuple[torch.Tensor, torch.Tensor]] = None\n",
        "    ) -> Tuple[torch.Tensor, Optional[Tuple[torch.Tensor, torch.Tensor]]]:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (batch, seq_len, d_model)\n",
        "            mask: Optional attention mask\n",
        "            is_causal: Use causal masking\n",
        "            use_cache: Return KV cache for next iteration\n",
        "            past_kv: Previous KV cache (K, V) each (batch, num_heads, past_len, d_k)\n",
        "        \n",
        "        Returns:\n",
        "            output: (batch, seq_len, d_model)\n",
        "            new_kv: Optional (K, V) cache\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "        \n",
        "        # Linear projections and reshape to (batch, num_heads, seq_len, d_k)\n",
        "        Q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        K = self.W_k(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        V = self.W_v(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        \n",
        "        # Handle KV cache\n",
        "        if past_kv is not None:\n",
        "            past_K, past_V = past_kv\n",
        "            K = torch.cat([past_K, K], dim=2)  # Concat along sequence dimension\n",
        "            V = torch.cat([past_V, V], dim=2)\n",
        "        \n",
        "        # Compute attention using Flash Attention\n",
        "        attn_output = F.scaled_dot_product_attention(\n",
        "            Q, K, V,\n",
        "            attn_mask=mask,\n",
        "            dropout_p=self.dropout if self.training else 0.0,\n",
        "            is_causal=is_causal\n",
        "        )\n",
        "        \n",
        "        # Reshape and project output\n",
        "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
        "        output = self.W_o(attn_output)\n",
        "        \n",
        "        # Return KV cache if requested\n",
        "        new_kv = (K, V) if use_cache else None\n",
        "        \n",
        "        return output, new_kv\n",
        "\n",
        "\n",
        "# Demo MHA\n",
        "d_model = 512\n",
        "num_heads = 8\n",
        "batch_size = 4\n",
        "seq_len = 128\n",
        "\n",
        "mha = MultiHeadAttention(d_model, num_heads).to(device)\n",
        "x = torch.randn(batch_size, seq_len, d_model, device=device)\n",
        "\n",
        "output, kv_cache = mha(x, use_cache=True)\n",
        "\n",
        "print(\"Multi-Head Attention (MHA)\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Input shape: {x.shape}\")\n",
        "print(f\"Output shape: {output.shape}\")\n",
        "print(f\"KV cache shapes: K={kv_cache[0].shape}, V={kv_cache[1].shape}\")\n",
        "print(f\"Parameters: {sum(p.numel() for p in mha.parameters()):,}\")\n",
        "\n",
        "# Calculate KV cache memory\n",
        "kv_elements = 2 * batch_size * num_heads * seq_len * (d_model // num_heads)\n",
        "kv_memory_mb = (kv_elements * 4) / (1024**2)  # float32\n",
        "print(f\"KV cache memory: {kv_memory_mb:.2f} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 3. Multi-Query Attention (MQA) - Maximum Speed\n",
        "\n",
        "### The Radical Simplification\n",
        "\n",
        "MQA (Shazeer, 2019) asked: **What if all heads shared the same K and V?**\n",
        "\n",
        "```\n",
        "Input: (batch, seq_len, d_model)\n",
        "\n",
        "           Single K, V (shared)\n",
        "                  ‚Üì\n",
        "Head 1: Q‚ÇÅ  ‚Üí  Attention‚ÇÅ (using shared K, V)\n",
        "Head 2: Q‚ÇÇ  ‚Üí  Attention‚ÇÇ (using shared K, V)\n",
        "...\n",
        "Head n: Q‚Çô  ‚Üí  Attention‚Çô (using shared K, V)\n",
        "\n",
        "Concat all heads ‚Üí Linear projection ‚Üí Output\n",
        "```\n",
        "\n",
        "### Mathematical Formulation\n",
        "\n",
        "**Shared projections**:\n",
        "$$\n",
        "K = X W^K \\quad V = X W^V\n",
        "$$\n",
        "where $W^K, W^V \\in \\mathbb{R}^{d_{model} \\times d_k}$ (single head dimension!)\n",
        "\n",
        "**Per-head queries**:\n",
        "$$\n",
        "Q_h = X W_h^Q \\quad \\text{for } h \\in [1, n_{heads}]\n",
        "$$\n",
        "\n",
        "**Attention**:\n",
        "$$\n",
        "\\text{head}_h = \\text{Attention}(Q_h, K, V)\n",
        "$$\n",
        "\n",
        "### Memory Savings\n",
        "\n",
        "**KV cache reduction**:\n",
        "- MHA: Store $n_{heads}$ copies of K and V\n",
        "- MQA: Store **1 copy** of K and V\n",
        "- **Reduction factor**: $n_{heads}$ (e.g., 32x for 32 heads!)\n",
        "\n",
        "### The Quality Trade-off\n",
        "\n",
        "**Problem**: Sharing K, V across all heads reduces expressiveness\n",
        "- Heads can't learn independent key/value representations\n",
        "- Typically **1-3% perplexity degradation** vs MHA\n",
        "- Used in PaLM, some versions of LLaMA 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiQueryAttention(nn.Module):\n",
        "    \"\"\"Multi-Query Attention (MQA) - Single shared K, V\"\"\"\n",
        "    \n",
        "    def __init__(self, d_model: int, num_heads: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0\n",
        "        \n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "        \n",
        "        # Each head gets its own Q, but K, V are SHARED!\n",
        "        self.W_q = nn.Linear(d_model, d_model)  # All Q heads\n",
        "        self.W_k = nn.Linear(d_model, self.d_k)  # Single K (shared)\n",
        "        self.W_v = nn.Linear(d_model, self.d_k)  # Single V (shared)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "        \n",
        "        self.dropout = dropout\n",
        "    \n",
        "    def forward(\n",
        "        self,\n",
        "        x: torch.Tensor,\n",
        "        mask: Optional[torch.Tensor] = None,\n",
        "        is_causal: bool = False,\n",
        "        use_cache: bool = False,\n",
        "        past_kv: Optional[Tuple[torch.Tensor, torch.Tensor]] = None\n",
        "    ) -> Tuple[torch.Tensor, Optional[Tuple[torch.Tensor, torch.Tensor]]]:\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "        \n",
        "        # Q: (batch, num_heads, seq_len, d_k)\n",
        "        Q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        \n",
        "        # K, V: (batch, 1, seq_len, d_k) - note the '1' for single head!\n",
        "        K = self.W_k(x).unsqueeze(1)  # Add head dimension\n",
        "        V = self.W_v(x).unsqueeze(1)\n",
        "        \n",
        "        # Handle KV cache\n",
        "        if past_kv is not None:\n",
        "            past_K, past_V = past_kv\n",
        "            K = torch.cat([past_K, K], dim=2)\n",
        "            V = torch.cat([past_V, V], dim=2)\n",
        "        \n",
        "        # Expand K, V to all heads (broadcasting)\n",
        "        K = K.expand(-1, self.num_heads, -1, -1)\n",
        "        V = V.expand(-1, self.num_heads, -1, -1)\n",
        "        \n",
        "        # Compute attention\n",
        "        attn_output = F.scaled_dot_product_attention(\n",
        "            Q, K, V,\n",
        "            attn_mask=mask,\n",
        "            dropout_p=self.dropout if self.training else 0.0,\n",
        "            is_causal=is_causal\n",
        "        )\n",
        "        \n",
        "        # Reshape and project\n",
        "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
        "        output = self.W_o(attn_output)\n",
        "        \n",
        "        # Store only single K, V in cache (not expanded)\n",
        "        new_kv = (K[:, :1, :, :], V[:, :1, :, :]) if use_cache else None\n",
        "        \n",
        "        return output, new_kv\n",
        "\n",
        "\n",
        "# Demo MQA\n",
        "mqa = MultiQueryAttention(d_model, num_heads).to(device)\n",
        "x = torch.randn(batch_size, seq_len, d_model, device=device)\n",
        "\n",
        "output, kv_cache = mqa(x, use_cache=True)\n",
        "\n",
        "print(\"\\nMulti-Query Attention (MQA)\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Input shape: {x.shape}\")\n",
        "print(f\"Output shape: {output.shape}\")\n",
        "print(f\"KV cache shapes: K={kv_cache[0].shape}, V={kv_cache[1].shape}\")\n",
        "print(f\"Parameters: {sum(p.numel() for p in mqa.parameters()):,}\")\n",
        "\n",
        "# Calculate KV cache memory (only 1 head worth!)\n",
        "kv_elements = 2 * batch_size * 1 * seq_len * (d_model // num_heads)  # Note: 1 head\n",
        "kv_memory_mb = (kv_elements * 4) / (1024**2)\n",
        "print(f\"KV cache memory: {kv_memory_mb:.2f} MB\")\n",
        "print(f\"\\nüí° KV cache is {num_heads}x smaller than MHA!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 4. Grouped Query Attention (GQA) - The Sweet Spot\n",
        "\n",
        "### The Key Insight\n",
        "\n",
        "GQA (Ainslie et al., 2023) realized:\n",
        "- MHA: Too much memory (num_heads copies of K, V)\n",
        "- MQA: Too little capacity (1 copy of K, V)\n",
        "- **Solution**: Group heads, share K, V within groups!\n",
        "\n",
        "### Architecture\n",
        "\n",
        "```\n",
        "Example: 8 heads, 2 groups (4 heads per group)\n",
        "\n",
        "Group 1 (shares K‚ÇÅ, V‚ÇÅ):\n",
        "  Head 1: Q‚ÇÅ  ‚Üí  Attention‚ÇÅ(Q‚ÇÅ, K‚ÇÅ, V‚ÇÅ)\n",
        "  Head 2: Q‚ÇÇ  ‚Üí  Attention‚ÇÇ(Q‚ÇÇ, K‚ÇÅ, V‚ÇÅ)\n",
        "  Head 3: Q‚ÇÉ  ‚Üí  Attention‚ÇÉ(Q‚ÇÉ, K‚ÇÅ, V‚ÇÅ)\n",
        "  Head 4: Q‚ÇÑ  ‚Üí  Attention‚ÇÑ(Q‚ÇÑ, K‚ÇÅ, V‚ÇÅ)\n",
        "\n",
        "Group 2 (shares K‚ÇÇ, V‚ÇÇ):\n",
        "  Head 5: Q‚ÇÖ  ‚Üí  Attention‚ÇÖ(Q‚ÇÖ, K‚ÇÇ, V‚ÇÇ)\n",
        "  Head 6: Q‚ÇÜ  ‚Üí  Attention‚ÇÜ(Q‚ÇÜ, K‚ÇÇ, V‚ÇÇ)\n",
        "  Head 7: Q‚Çá  ‚Üí  Attention‚Çá(Q‚Çá, K‚ÇÇ, V‚ÇÇ)\n",
        "  Head 8: Q‚Çà  ‚Üí  Attention‚Çà(Q‚Çà, K‚ÇÇ, V‚ÇÇ)\n",
        "```\n",
        "\n",
        "### Mathematical Formulation\n",
        "\n",
        "Given $n_{heads}$ query heads and $n_{kv\\_groups}$ KV groups:\n",
        "\n",
        "$$\n",
        "\\text{heads\\_per\\_group} = \\frac{n_{heads}}{n_{kv\\_groups}}\n",
        "$$\n",
        "\n",
        "For group $g \\in [1, n_{kv\\_groups}]$:\n",
        "$$\n",
        "K_g = X W_g^K \\quad V_g = X W_g^V\n",
        "$$\n",
        "\n",
        "For each head $h$ in group $g$:\n",
        "$$\n",
        "Q_h = X W_h^Q\n",
        "$$\n",
        "$$\n",
        "\\text{head}_h = \\text{Attention}(Q_h, K_g, V_g)\n",
        "$$\n",
        "\n",
        "### Design Choices\n",
        "\n",
        "**Number of groups**:\n",
        "- More groups = better quality, more memory\n",
        "- Fewer groups = less memory, slight quality loss\n",
        "\n",
        "**Common configurations**:\n",
        "- **LLaMA 2/3**: 32 heads, 8 KV groups (4 heads per group)\n",
        "- **Mistral**: 32 heads, 8 KV groups\n",
        "- **GPT-4** (rumored): 128 heads, 16 KV groups\n",
        "\n",
        "### Special Cases\n",
        "\n",
        "GQA generalizes both MHA and MQA:\n",
        "- $n_{kv\\_groups} = n_{heads}$ ‚Üí **MHA** (each head has own K, V)\n",
        "- $n_{kv\\_groups} = 1$ ‚Üí **MQA** (all heads share K, V)\n",
        "- $1 < n_{kv\\_groups} < n_{heads}$ ‚Üí **GQA**\n",
        "\n",
        "### Performance Characteristics\n",
        "\n",
        "**Memory savings** (vs MHA):\n",
        "$$\n",
        "\\text{Reduction} = \\frac{n_{heads}}{n_{kv\\_groups}}\n",
        "$$\n",
        "\n",
        "Example (32 heads, 8 groups): **4x less KV cache memory**\n",
        "\n",
        "**Quality** (perplexity vs MHA):\n",
        "- 8 groups: < 0.1% degradation\n",
        "- 4 groups: ~0.2% degradation\n",
        "- 2 groups: ~0.5% degradation\n",
        "- 1 group (MQA): 1-3% degradation\n",
        "\n",
        "**Speed** (inference):\n",
        "- 20-30% faster than MHA\n",
        "- Within 5-10% of MQA speed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GroupedQueryAttention(nn.Module):\n",
        "    \"\"\"Grouped Query Attention (GQA) - The optimal attention architecture\"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self, \n",
        "        d_model: int, \n",
        "        num_heads: int,\n",
        "        num_kv_groups: int,\n",
        "        dropout: float = 0.1\n",
        "    ):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0\n",
        "        assert num_heads % num_kv_groups == 0, \"num_heads must be divisible by num_kv_groups\"\n",
        "        \n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.num_kv_groups = num_kv_groups\n",
        "        self.heads_per_group = num_heads // num_kv_groups\n",
        "        self.d_k = d_model // num_heads\n",
        "        \n",
        "        # Q: All heads get their own projections\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        \n",
        "        # K, V: Only num_kv_groups projections (NOT num_heads!)\n",
        "        self.W_k = nn.Linear(d_model, num_kv_groups * self.d_k)\n",
        "        self.W_v = nn.Linear(d_model, num_kv_groups * self.d_k)\n",
        "        \n",
        "        # Output projection\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "        \n",
        "        self.dropout = dropout\n",
        "        \n",
        "        print(f\"\\nGQA Configuration:\")\n",
        "        print(f\"  Query heads: {num_heads}\")\n",
        "        print(f\"  KV groups: {num_kv_groups}\")\n",
        "        print(f\"  Heads per group: {self.heads_per_group}\")\n",
        "        print(f\"  Memory reduction vs MHA: {num_heads / num_kv_groups:.1f}x\")\n",
        "    \n",
        "    def forward(\n",
        "        self,\n",
        "        x: torch.Tensor,\n",
        "        mask: Optional[torch.Tensor] = None,\n",
        "        is_causal: bool = False,\n",
        "        use_cache: bool = False,\n",
        "        past_kv: Optional[Tuple[torch.Tensor, torch.Tensor]] = None\n",
        "    ) -> Tuple[torch.Tensor, Optional[Tuple[torch.Tensor, torch.Tensor]]]:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (batch, seq_len, d_model)\n",
        "            mask: Optional attention mask\n",
        "            is_causal: Use causal masking\n",
        "            use_cache: Return KV cache\n",
        "            past_kv: Previous KV cache\n",
        "        \n",
        "        Returns:\n",
        "            output: (batch, seq_len, d_model)\n",
        "            new_kv: Optional (K, V) cache with num_kv_groups heads\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "        \n",
        "        # Q projection: (batch, num_heads, seq_len, d_k)\n",
        "        Q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        \n",
        "        # K, V projections: (batch, num_kv_groups, seq_len, d_k)\n",
        "        K = self.W_k(x).view(batch_size, seq_len, self.num_kv_groups, self.d_k).transpose(1, 2)\n",
        "        V = self.W_v(x).view(batch_size, seq_len, self.num_kv_groups, self.d_k).transpose(1, 2)\n",
        "        \n",
        "        # Handle KV cache\n",
        "        if past_kv is not None:\n",
        "            past_K, past_V = past_kv\n",
        "            K = torch.cat([past_K, K], dim=2)\n",
        "            V = torch.cat([past_V, V], dim=2)\n",
        "        \n",
        "        # Expand K, V to match number of query heads\n",
        "        # Each KV group is repeated heads_per_group times\n",
        "        # (batch, num_kv_groups, seq_len, d_k) -> (batch, num_heads, seq_len, d_k)\n",
        "        K = K.repeat_interleave(self.heads_per_group, dim=1)\n",
        "        V = V.repeat_interleave(self.heads_per_group, dim=1)\n",
        "        \n",
        "        # Compute attention (now Q, K, V all have num_heads dimension)\n",
        "        attn_output = F.scaled_dot_product_attention(\n",
        "            Q, K, V,\n",
        "            attn_mask=mask,\n",
        "            dropout_p=self.dropout if self.training else 0.0,\n",
        "            is_causal=is_causal\n",
        "        )\n",
        "        \n",
        "        # Reshape and project output\n",
        "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
        "        output = self.W_o(attn_output)\n",
        "        \n",
        "        # Store only num_kv_groups worth of K, V (before expansion)\n",
        "        if use_cache:\n",
        "            # Get back the grouped K, V (undo the repeat_interleave)\n",
        "            cached_K = K[:, ::self.heads_per_group, :, :]\n",
        "            cached_V = V[:, ::self.heads_per_group, :, :]\n",
        "            new_kv = (cached_K, cached_V)\n",
        "        else:\n",
        "            new_kv = None\n",
        "        \n",
        "        return output, new_kv\n",
        "\n",
        "\n",
        "# Demo GQA with different group configurations\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Comparing Different GQA Configurations\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "configs = [\n",
        "    (8, 8, \"MHA (8 groups)\"),\n",
        "    (8, 4, \"GQA-4 (4 groups)\"),\n",
        "    (8, 2, \"GQA-2 (2 groups)\"),\n",
        "    (8, 1, \"MQA (1 group)\"),\n",
        "]\n",
        "\n",
        "for num_heads, num_kv_groups, name in configs:\n",
        "    print(f\"\\n{name}:\")\n",
        "    gqa = GroupedQueryAttention(d_model, num_heads, num_kv_groups).to(device)\n",
        "    x = torch.randn(batch_size, seq_len, d_model, device=device)\n",
        "    output, kv_cache = gqa(x, use_cache=True)\n",
        "    \n",
        "    # Calculate KV cache memory\n",
        "    kv_elements = 2 * batch_size * num_kv_groups * seq_len * (d_model // num_heads)\n",
        "    kv_memory_mb = (kv_elements * 4) / (1024**2)\n",
        "    \n",
        "    params = sum(p.numel() for p in gqa.parameters())\n",
        "    print(f\"  Parameters: {params:,}\")\n",
        "    print(f\"  KV cache: {kv_cache[0].shape}\")\n",
        "    print(f\"  KV memory: {kv_memory_mb:.2f} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 5. Comprehensive Performance Comparison\n",
        "\n",
        "### Memory Analysis\n",
        "\n",
        "Let's calculate exact memory savings for a realistic LLM configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_kv_cache_memory(\n",
        "    batch_size: int,\n",
        "    num_heads: int,\n",
        "    num_kv_groups: int,\n",
        "    seq_len: int,\n",
        "    d_model: int,\n",
        "    num_layers: int,\n",
        "    dtype: torch.dtype = torch.float16\n",
        ") -> dict:\n",
        "    \"\"\"Calculate KV cache memory requirements\"\"\"\n",
        "    \n",
        "    d_k = d_model // num_heads\n",
        "    bytes_per_element = 2 if dtype == torch.float16 else 4\n",
        "    \n",
        "    # KV cache per layer: batch √ó num_kv_groups √ó seq_len √ó d_k √ó 2 (K and V)\n",
        "    elements_per_layer = batch_size * num_kv_groups * seq_len * d_k * 2\n",
        "    bytes_per_layer = elements_per_layer * bytes_per_element\n",
        "    \n",
        "    # Total for all layers\n",
        "    total_bytes = bytes_per_layer * num_layers\n",
        "    total_gb = total_bytes / (1024**3)\n",
        "    \n",
        "    return {\n",
        "        'elements_per_layer': elements_per_layer,\n",
        "        'mb_per_layer': bytes_per_layer / (1024**2),\n",
        "        'total_gb': total_gb,\n",
        "        'num_kv_groups': num_kv_groups\n",
        "    }\n",
        "\n",
        "\n",
        "# LLaMA-style configuration\n",
        "config = {\n",
        "    'batch_size': 16,\n",
        "    'num_heads': 32,\n",
        "    'd_model': 4096,\n",
        "    'seq_len': 4096,  # 4k context\n",
        "    'num_layers': 32,\n",
        "    'dtype': torch.float16\n",
        "}\n",
        "\n",
        "print(\"LLaMA-Style Model KV Cache Analysis\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Configuration:\")\n",
        "print(f\"  Batch size: {config['batch_size']}\")\n",
        "print(f\"  Heads: {config['num_heads']}\")\n",
        "print(f\"  d_model: {config['d_model']}\")\n",
        "print(f\"  Sequence length: {config['seq_len']:,} tokens\")\n",
        "print(f\"  Layers: {config['num_layers']}\")\n",
        "print(f\"  Dtype: {config['dtype']}\")\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "\n",
        "# Compare different attention types\n",
        "attention_types = [\n",
        "    (32, \"MHA (32 groups)\"),\n",
        "    (8, \"GQA-8 (LLaMA 2/3)\"),\n",
        "    (4, \"GQA-4\"),\n",
        "    (1, \"MQA (1 group)\"),\n",
        "]\n",
        "\n",
        "results = []\n",
        "for num_kv_groups, name in attention_types:\n",
        "    result = analyze_kv_cache_memory(\n",
        "        num_kv_groups=num_kv_groups,\n",
        "        **config\n",
        "    )\n",
        "    result['name'] = name\n",
        "    results.append(result)\n",
        "    \n",
        "    print(f\"\\n{name}:\")\n",
        "    print(f\"  KV cache per layer: {result['mb_per_layer']:.1f} MB\")\n",
        "    print(f\"  Total KV cache: {result['total_gb']:.2f} GB\")\n",
        "    \n",
        "    # Compare to MHA\n",
        "    if num_kv_groups < 32:\n",
        "        savings = (1 - result['total_gb'] / results[0]['total_gb']) * 100\n",
        "        print(f\"  Memory saved vs MHA: {savings:.1f}%\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"üí° Key Insights:\")\n",
        "print(\"  - GQA-8 (LLaMA 2/3) saves 75% memory vs MHA\")\n",
        "print(\"  - Quality loss is < 0.1% perplexity\")\n",
        "print(\"  - This enables 4x larger batch size or longer context!\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize memory savings\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Plot 1: Total memory\n",
        "names = [r['name'] for r in results]\n",
        "memory_gb = [r['total_gb'] for r in results]\n",
        "\n",
        "colors = ['#e74c3c', '#3498db', '#2ecc71', '#9b59b6']\n",
        "bars = axes[0].bar(range(len(names)), memory_gb, color=colors, alpha=0.7)\n",
        "axes[0].set_xticks(range(len(names)))\n",
        "axes[0].set_xticklabels(names, rotation=15, ha='right')\n",
        "axes[0].set_ylabel('KV Cache Memory (GB)', fontsize=12)\n",
        "axes[0].set_title('KV Cache Memory by Attention Type', fontsize=14)\n",
        "axes[0].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Add value labels\n",
        "for i, (bar, val) in enumerate(zip(bars, memory_gb)):\n",
        "    axes[0].text(bar.get_x() + bar.get_width()/2, val + 0.5, \n",
        "                f'{val:.1f} GB', ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "# Plot 2: Memory savings percentage\n",
        "baseline = results[0]['total_gb']\n",
        "savings_pct = [(1 - r['total_gb'] / baseline) * 100 for r in results]\n",
        "\n",
        "bars2 = axes[1].bar(range(len(names)), savings_pct, color=colors, alpha=0.7)\n",
        "axes[1].set_xticks(range(len(names)))\n",
        "axes[1].set_xticklabels(names, rotation=15, ha='right')\n",
        "axes[1].set_ylabel('Memory Saved vs MHA (%)', fontsize=12)\n",
        "axes[1].set_title('Memory Savings Comparison', fontsize=14)\n",
        "axes[1].axhline(y=0, color='black', linestyle='-', linewidth=0.8)\n",
        "axes[1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Add value labels\n",
        "for bar, val in zip(bars2, savings_pct):\n",
        "    if val > 0:\n",
        "        axes[1].text(bar.get_x() + bar.get_width()/2, val + 1, \n",
        "                    f'{val:.1f}%', ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüìä Visualization shows:\")\n",
        "print(\"  - Linear reduction in memory with fewer KV groups\")\n",
        "print(\"  - GQA-8 provides sweet spot: 75% savings with minimal quality loss\")\n",
        "print(\"  - MQA saves 96.9% but has noticeable quality degradation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 6. Autoregressive Generation with KV Cache\n",
        "\n",
        "### How KV Cache Accelerates Generation\n",
        "\n",
        "Let's see the KV cache in action during text generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def autoregressive_generation_demo(\n",
        "    model: nn.Module,\n",
        "    initial_tokens: torch.Tensor,\n",
        "    num_new_tokens: int,\n",
        "    use_cache: bool = True\n",
        ") -> Tuple[torch.Tensor, list]:\n",
        "    \"\"\"\n",
        "    Demonstrate autoregressive generation with/without KV cache\n",
        "    \n",
        "    Args:\n",
        "        model: Attention module (GQA, MHA, or MQA)\n",
        "        initial_tokens: (batch, initial_len, d_model)\n",
        "        num_new_tokens: How many tokens to generate\n",
        "        use_cache: Whether to use KV cache\n",
        "    \n",
        "    Returns:\n",
        "        generated: All tokens including initial\n",
        "        timings: Time per generation step\n",
        "    \"\"\"\n",
        "    batch_size, seq_len, d_model = initial_tokens.shape\n",
        "    \n",
        "    # Start with initial tokens\n",
        "    generated = initial_tokens\n",
        "    past_kv = None\n",
        "    timings = []\n",
        "    \n",
        "    if device.type == 'cuda':\n",
        "        torch.cuda.synchronize()\n",
        "    \n",
        "    for i in range(num_new_tokens):\n",
        "        start = time.time()\n",
        "        \n",
        "        if use_cache and past_kv is not None:\n",
        "            # With cache: only process the last token\n",
        "            input_tokens = generated[:, -1:, :]\n",
        "        else:\n",
        "            # Without cache: process entire sequence\n",
        "            input_tokens = generated\n",
        "        \n",
        "        # Forward pass\n",
        "        output, new_kv = model(\n",
        "            input_tokens,\n",
        "            is_causal=True,\n",
        "            use_cache=use_cache,\n",
        "            past_kv=past_kv\n",
        "        )\n",
        "        \n",
        "        if use_cache:\n",
        "            past_kv = new_kv\n",
        "        \n",
        "        # \"Generate\" next token (just random for demo)\n",
        "        next_token = torch.randn(batch_size, 1, d_model, device=device)\n",
        "        generated = torch.cat([generated, next_token], dim=1)\n",
        "        \n",
        "        if device.type == 'cuda':\n",
        "            torch.cuda.synchronize()\n",
        "        \n",
        "        timings.append(time.time() - start)\n",
        "    \n",
        "    return generated, timings\n",
        "\n",
        "\n",
        "# Compare generation with/without cache\n",
        "print(\"\\nAutoregressive Generation Benchmark\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "d_model = 512\n",
        "num_heads = 8\n",
        "num_kv_groups = 2\n",
        "batch_size = 4\n",
        "initial_len = 64\n",
        "num_new_tokens = 50\n",
        "\n",
        "gqa = GroupedQueryAttention(d_model, num_heads, num_kv_groups).to(device)\n",
        "initial_tokens = torch.randn(batch_size, initial_len, d_model, device=device)\n",
        "\n",
        "# With cache\n",
        "print(\"\\nGenerating with KV cache...\")\n",
        "_, timings_with_cache = autoregressive_generation_demo(\n",
        "    gqa, initial_tokens, num_new_tokens, use_cache=True\n",
        ")\n",
        "\n",
        "# Without cache\n",
        "print(\"Generating without KV cache...\")\n",
        "_, timings_without_cache = autoregressive_generation_demo(\n",
        "    gqa, initial_tokens, num_new_tokens, use_cache=False\n",
        ")\n",
        "\n",
        "avg_with = np.mean(timings_with_cache) * 1000\n",
        "avg_without = np.mean(timings_without_cache) * 1000\n",
        "speedup = avg_without / avg_with\n",
        "\n",
        "print(f\"\\nResults:\")\n",
        "print(f\"  With KV cache: {avg_with:.2f} ms/token\")\n",
        "print(f\"  Without cache: {avg_without:.2f} ms/token\")\n",
        "print(f\"  Speedup: {speedup:.2f}x\")\n",
        "\n",
        "# Plot timing comparison\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "token_indices = range(1, num_new_tokens + 1)\n",
        "plt.plot(token_indices, np.array(timings_without_cache) * 1000, \n",
        "         'r-o', label='Without Cache', alpha=0.7, markersize=3)\n",
        "plt.plot(token_indices, np.array(timings_with_cache) * 1000, \n",
        "         'b-s', label='With Cache', alpha=0.7, markersize=3)\n",
        "plt.xlabel('Token Position', fontsize=12)\n",
        "plt.ylabel('Time per Token (ms)', fontsize=12)\n",
        "plt.title('Generation Time: With vs Without KV Cache', fontsize=14)\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.bar(['With\\nCache', 'Without\\nCache'], [avg_with, avg_without], \n",
        "        color=['#3498db', '#e74c3c'], alpha=0.7)\n",
        "plt.ylabel('Average Time (ms/token)', fontsize=12)\n",
        "plt.title(f'Average Generation Time ({speedup:.1f}x speedup)', fontsize=14)\n",
        "plt.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüí° Without cache, time increases linearly with sequence length!\")\n",
        "print(\"   With cache, time is constant per token.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## 7. Real-World Usage: LLaMA-Style Transformer Block\n",
        "\n",
        "### Complete Implementation\n",
        "\n",
        "Let's build a complete transformer block using GQA, as used in LLaMA 2/3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RMSNorm(nn.Module):\n",
        "    \"\"\"Root Mean Square Layer Normalization (used in LLaMA)\"\"\"\n",
        "    \n",
        "    def __init__(self, d_model: int, eps: float = 1e-6):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.ones(d_model))\n",
        "    \n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # RMS normalization\n",
        "        rms = torch.sqrt(torch.mean(x ** 2, dim=-1, keepdim=True) + self.eps)\n",
        "        return x / rms * self.weight\n",
        "\n",
        "\n",
        "class SwiGLU(nn.Module):\n",
        "    \"\"\"SwiGLU activation (used in LLaMA)\"\"\"\n",
        "    \n",
        "    def __init__(self, d_model: int, d_ff: int):\n",
        "        super().__init__()\n",
        "        self.w1 = nn.Linear(d_model, d_ff, bias=False)\n",
        "        self.w2 = nn.Linear(d_ff, d_model, bias=False)\n",
        "        self.w3 = nn.Linear(d_model, d_ff, bias=False)\n",
        "    \n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # SwiGLU(x) = (Swish(W1¬∑x) ‚äô W3¬∑x) W2\n",
        "        return self.w2(F.silu(self.w1(x)) * self.w3(x))\n",
        "\n",
        "\n",
        "class LLaMATransformerBlock(nn.Module):\n",
        "    \"\"\"Complete transformer block with GQA (LLaMA 2/3 style)\"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model: int,\n",
        "        num_heads: int,\n",
        "        num_kv_groups: int,\n",
        "        d_ff: int,\n",
        "        dropout: float = 0.1\n",
        "    ):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Pre-attention norm\n",
        "        self.attn_norm = RMSNorm(d_model)\n",
        "        \n",
        "        # Grouped Query Attention\n",
        "        self.attention = GroupedQueryAttention(\n",
        "            d_model, num_heads, num_kv_groups, dropout\n",
        "        )\n",
        "        \n",
        "        # Pre-FFN norm\n",
        "        self.ffn_norm = RMSNorm(d_model)\n",
        "        \n",
        "        # SwiGLU feedforward\n",
        "        self.ffn = SwiGLU(d_model, d_ff)\n",
        "    \n",
        "    def forward(\n",
        "        self,\n",
        "        x: torch.Tensor,\n",
        "        mask: Optional[torch.Tensor] = None,\n",
        "        is_causal: bool = False,\n",
        "        use_cache: bool = False,\n",
        "        past_kv: Optional[Tuple[torch.Tensor, torch.Tensor]] = None\n",
        "    ) -> Tuple[torch.Tensor, Optional[Tuple[torch.Tensor, torch.Tensor]]]:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (batch, seq_len, d_model)\n",
        "        \n",
        "        Returns:\n",
        "            output: (batch, seq_len, d_model)\n",
        "            kv_cache: Optional KV cache\n",
        "        \"\"\"\n",
        "        # Attention block with pre-norm (LLaMA style)\n",
        "        attn_out, kv_cache = self.attention(\n",
        "            self.attn_norm(x),\n",
        "            mask=mask,\n",
        "            is_causal=is_causal,\n",
        "            use_cache=use_cache,\n",
        "            past_kv=past_kv\n",
        "        )\n",
        "        x = x + attn_out  # Residual connection\n",
        "        \n",
        "        # FFN block with pre-norm\n",
        "        x = x + self.ffn(self.ffn_norm(x))  # Residual connection\n",
        "        \n",
        "        return x, kv_cache\n",
        "\n",
        "\n",
        "# Demo: LLaMA-style block\n",
        "print(\"LLaMA-Style Transformer Block with GQA\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "d_model = 4096\n",
        "num_heads = 32\n",
        "num_kv_groups = 8  # LLaMA 2/3 uses 8 groups\n",
        "d_ff = 4 * d_model  # Standard FFN hidden size\n",
        "\n",
        "block = LLaMATransformerBlock(d_model, num_heads, num_kv_groups, d_ff).to(device)\n",
        "\n",
        "batch_size = 2\n",
        "seq_len = 512\n",
        "x = torch.randn(batch_size, seq_len, d_model, device=device)\n",
        "\n",
        "output, kv_cache = block(x, is_causal=True, use_cache=True)\n",
        "\n",
        "print(f\"\\nConfiguration:\")\n",
        "print(f\"  d_model: {d_model}\")\n",
        "print(f\"  Heads: {num_heads}\")\n",
        "print(f\"  KV groups: {num_kv_groups}\")\n",
        "print(f\"  d_ff: {d_ff}\")\n",
        "\n",
        "print(f\"\\nShapes:\")\n",
        "print(f\"  Input: {x.shape}\")\n",
        "print(f\"  Output: {output.shape}\")\n",
        "print(f\"  KV cache: K={kv_cache[0].shape}, V={kv_cache[1].shape}\")\n",
        "\n",
        "total_params = sum(p.numel() for p in block.parameters())\n",
        "print(f\"\\nParameters: {total_params:,}\")\n",
        "\n",
        "# Memory footprint\n",
        "kv_elements = 2 * kv_cache[0].numel()  # K and V\n",
        "kv_memory_mb = (kv_elements * 2) / (1024**2)  # float16\n",
        "print(f\"KV cache memory (float16): {kv_memory_mb:.2f} MB\")\n",
        "\n",
        "print(\"\\n‚úÖ This is how modern LLMs like LLaMA 2/3 are structured!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Mini Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 1: Calculate Optimal GQA Configuration\n",
        "\n",
        "Given a model with 64 query heads, find the optimal number of KV groups that:\n",
        "- Reduces memory by at least 4x\n",
        "- Minimizes quality loss (more groups = better)\n",
        "\n",
        "List all valid configurations and their memory reduction factors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your code here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Solution\n",
        "num_heads = 64\n",
        "min_reduction = 4  # At least 4x reduction\n",
        "\n",
        "print(\"Valid GQA Configurations for 64 Query Heads:\")\n",
        "print(\"=\"*60)\n",
        "print(f\"{'KV Groups':>12} {'Heads/Group':>15} {'Memory Reduction':>20}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "valid_configs = []\n",
        "for num_kv_groups in range(1, num_heads + 1):\n",
        "    if num_heads % num_kv_groups == 0:  # Must divide evenly\n",
        "        reduction = num_heads / num_kv_groups\n",
        "        if reduction >= min_reduction:\n",
        "            heads_per_group = num_heads // num_kv_groups\n",
        "            valid_configs.append((num_kv_groups, heads_per_group, reduction))\n",
        "            print(f\"{num_kv_groups:>12} {heads_per_group:>15} {reduction:>19.1f}x\")\n",
        "\n",
        "print(\"\\nüí° Recommended: 16 groups (4 heads/group) for 4x reduction\")\n",
        "print(\"   This balances memory savings with quality preservation.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 2: Implement MQA from Scratch\n",
        "\n",
        "Implement Multi-Query Attention without using the provided class. Ensure:\n",
        "- Single K, V shared across all heads\n",
        "- Each head has its own Q\n",
        "- Outputs match standard attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your code here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Solution\n",
        "class MyMultiQueryAttention(nn.Module):\n",
        "    def __init__(self, d_model: int, num_heads: int):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0\n",
        "        \n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "        \n",
        "        # Q for all heads, but single K, V\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, self.d_k)  # Single head dimension\n",
        "        self.W_v = nn.Linear(d_model, self.d_k)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "    \n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        batch, seq_len, _ = x.shape\n",
        "        \n",
        "        # Multi-head Q\n",
        "        Q = self.W_q(x).view(batch, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        \n",
        "        # Single K, V (broadcast to all heads)\n",
        "        K = self.W_k(x).view(batch, seq_len, 1, self.d_k).transpose(1, 2)\n",
        "        V = self.W_v(x).view(batch, seq_len, 1, self.d_k).transpose(1, 2)\n",
        "        \n",
        "        # Expand to all heads\n",
        "        K = K.expand(-1, self.num_heads, -1, -1)\n",
        "        V = V.expand(-1, self.num_heads, -1, -1)\n",
        "        \n",
        "        # Attention\n",
        "        out = F.scaled_dot_product_attention(Q, K, V)\n",
        "        out = out.transpose(1, 2).contiguous().view(batch, seq_len, self.d_model)\n",
        "        \n",
        "        return self.W_o(out)\n",
        "\n",
        "# Test\n",
        "mqa = MyMultiQueryAttention(256, 8).to(device)\n",
        "x = torch.randn(2, 64, 256, device=device)\n",
        "out = mqa(x)\n",
        "print(f\"Input: {x.shape}\")\n",
        "print(f\"Output: {out.shape}\")\n",
        "print(\"‚úÖ MQA implementation working!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 3: KV Cache Memory Calculator\n",
        "\n",
        "Build a tool that calculates KV cache memory for different model configurations.\n",
        "Include: batch size, context length, number of layers, and data type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your code here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Solution\n",
        "def kv_cache_calculator(\n",
        "    model_name: str,\n",
        "    num_heads: int,\n",
        "    num_kv_groups: int,\n",
        "    d_model: int,\n",
        "    num_layers: int,\n",
        "    batch_size: int,\n",
        "    context_len: int,\n",
        "    dtype: str = \"float16\"\n",
        "):\n",
        "    \"\"\"Calculate KV cache memory requirements\"\"\"\n",
        "    \n",
        "    bytes_per_element = 2 if dtype == \"float16\" else 4\n",
        "    d_k = d_model // num_heads\n",
        "    \n",
        "    # KV cache: batch √ó num_kv_groups √ó context √ó d_k √ó 2 (K+V) √ó bytes\n",
        "    total_elements = batch_size * num_kv_groups * context_len * d_k * 2 * num_layers\n",
        "    total_bytes = total_elements * bytes_per_element\n",
        "    total_gb = total_bytes / (1024**3)\n",
        "    \n",
        "    print(f\"\\n{model_name} KV Cache Analysis\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Model Config:\")\n",
        "    print(f\"  Heads: {num_heads}, KV Groups: {num_kv_groups}, d_model: {d_model}\")\n",
        "    print(f\"  Layers: {num_layers}\")\n",
        "    print(f\"\\nInference Config:\")\n",
        "    print(f\"  Batch size: {batch_size}\")\n",
        "    print(f\"  Context length: {context_len:,} tokens\")\n",
        "    print(f\"  Data type: {dtype}\")\n",
        "    print(f\"\\nMemory Requirements:\")\n",
        "    print(f\"  Total KV cache: {total_gb:.2f} GB\")\n",
        "    print(f\"  Per sample: {total_gb/batch_size:.2f} GB\")\n",
        "    print(f\"  Memory reduction vs MHA: {num_heads/num_kv_groups:.1f}x\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "# Test with various models\n",
        "kv_cache_calculator(\n",
        "    \"LLaMA 2 7B\",\n",
        "    num_heads=32,\n",
        "    num_kv_groups=8,\n",
        "    d_model=4096,\n",
        "    num_layers=32,\n",
        "    batch_size=8,\n",
        "    context_len=4096,\n",
        "    dtype=\"float16\"\n",
        ")\n",
        "\n",
        "kv_cache_calculator(\n",
        "    \"Mistral 7B\",\n",
        "    num_heads=32,\n",
        "    num_kv_groups=8,\n",
        "    d_model=4096,\n",
        "    num_layers=32,\n",
        "    batch_size=16,\n",
        "    context_len=8192,  # Longer context\n",
        "    dtype=\"float16\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Comprehensive Exercise: Compare MHA, MQA, and GQA\n",
        "\n",
        "Build a comprehensive comparison of all three attention mechanisms:\n",
        "\n",
        "1. Implement all three (MHA, MQA, GQA-4)\n",
        "2. Measure:\n",
        "   - Parameter count\n",
        "   - KV cache size\n",
        "   - Forward pass time\n",
        "   - Generation speed (with cache)\n",
        "3. Visualize the results with plots\n",
        "4. Analyze the quality-speed-memory trade-offs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your code here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Solution\n",
        "def comprehensive_attention_comparison():\n",
        "    \"\"\"Compare MHA, MQA, and GQA across all metrics\"\"\"\n",
        "    \n",
        "    # Configuration\n",
        "    d_model = 512\n",
        "    num_heads = 8\n",
        "    batch_size = 4\n",
        "    seq_len = 256\n",
        "    num_gen_tokens = 30\n",
        "    \n",
        "    # Create models\n",
        "    models = {\n",
        "        'MHA': GroupedQueryAttention(d_model, num_heads, num_kv_groups=8),  # 8 groups = MHA\n",
        "        'GQA-4': GroupedQueryAttention(d_model, num_heads, num_kv_groups=4),\n",
        "        'GQA-2': GroupedQueryAttention(d_model, num_heads, num_kv_groups=2),\n",
        "        'MQA': GroupedQueryAttention(d_model, num_heads, num_kv_groups=1),\n",
        "    }\n",
        "    \n",
        "    results = {}\n",
        "    \n",
        "    for name, model in models.items():\n",
        "        model = model.to(device)\n",
        "        x = torch.randn(batch_size, seq_len, d_model, device=device)\n",
        "        \n",
        "        # 1. Parameter count\n",
        "        params = sum(p.numel() for p in model.parameters())\n",
        "        \n",
        "        # 2. Forward pass benchmark\n",
        "        if device.type == 'cuda':\n",
        "            torch.cuda.synchronize()\n",
        "        \n",
        "        start = time.time()\n",
        "        for _ in range(20):\n",
        "            _, _ = model(x, is_causal=True)\n",
        "        \n",
        "        if device.type == 'cuda':\n",
        "            torch.cuda.synchronize()\n",
        "        \n",
        "        forward_time = (time.time() - start) / 20 * 1000  # ms\n",
        "        \n",
        "        # 3. Generation benchmark with cache\n",
        "        initial_tokens = torch.randn(batch_size, 32, d_model, device=device)\n",
        "        _, gen_timings = autoregressive_generation_demo(\n",
        "            model, initial_tokens, num_gen_tokens, use_cache=True\n",
        "        )\n",
        "        gen_time = np.mean(gen_timings) * 1000  # ms/token\n",
        "        \n",
        "        # 4. KV cache size\n",
        "        _, kv_cache = model(x, use_cache=True)\n",
        "        kv_elements = 2 * kv_cache[0].numel()\n",
        "        kv_memory_mb = (kv_elements * 4) / (1024**2)\n",
        "        \n",
        "        results[name] = {\n",
        "            'params': params,\n",
        "            'forward_time': forward_time,\n",
        "            'gen_time': gen_time,\n",
        "            'kv_memory': kv_memory_mb\n",
        "        }\n",
        "    \n",
        "    # Print results table\n",
        "    print(\"\\nComprehensive Attention Mechanism Comparison\")\n",
        "    print(\"=\"*85)\n",
        "    print(f\"{'Type':>8} {'Params':>12} {'Forward (ms)':>15} {'Gen (ms/tok)':>15} {'KV Cache (MB)':>15}\")\n",
        "    print(\"=\"*85)\n",
        "    \n",
        "    for name, res in results.items():\n",
        "        print(f\"{name:>8} {res['params']:>12,} {res['forward_time']:>14.2f} {res['gen_time']:>14.2f} {res['kv_memory']:>14.2f}\")\n",
        "    \n",
        "    # Visualize\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    \n",
        "    names = list(results.keys())\n",
        "    colors = ['#e74c3c', '#3498db', '#2ecc71', '#9b59b6']\n",
        "    \n",
        "    # Plot 1: Parameters\n",
        "    params = [results[n]['params'] for n in names]\n",
        "    axes[0, 0].bar(names, params, color=colors, alpha=0.7)\n",
        "    axes[0, 0].set_ylabel('Parameters', fontsize=12)\n",
        "    axes[0, 0].set_title('Parameter Count', fontsize=14)\n",
        "    axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
        "    \n",
        "    # Plot 2: Forward time\n",
        "    forward_times = [results[n]['forward_time'] for n in names]\n",
        "    axes[0, 1].bar(names, forward_times, color=colors, alpha=0.7)\n",
        "    axes[0, 1].set_ylabel('Time (ms)', fontsize=12)\n",
        "    axes[0, 1].set_title('Forward Pass Time', fontsize=14)\n",
        "    axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
        "    \n",
        "    # Plot 3: Generation speed\n",
        "    gen_times = [results[n]['gen_time'] for n in names]\n",
        "    axes[1, 0].bar(names, gen_times, color=colors, alpha=0.7)\n",
        "    axes[1, 0].set_ylabel('Time (ms/token)', fontsize=12)\n",
        "    axes[1, 0].set_title('Generation Speed (with cache)', fontsize=14)\n",
        "    axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
        "    \n",
        "    # Plot 4: KV cache memory\n",
        "    kv_memories = [results[n]['kv_memory'] for n in names]\n",
        "    axes[1, 1].bar(names, kv_memories, color=colors, alpha=0.7)\n",
        "    axes[1, 1].set_ylabel('Memory (MB)', fontsize=12)\n",
        "    axes[1, 1].set_title('KV Cache Memory', fontsize=14)\n",
        "    axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Analysis\n",
        "    print(\"\\nüìä Key Insights:\")\n",
        "    print(\"=\"*85)\n",
        "    print(\"1. Parameters decrease slightly with fewer KV groups (less K, V projections)\")\n",
        "    print(\"2. Forward pass time is similar across all variants\")\n",
        "    print(\"3. Generation speed improves with fewer KV groups (less cache to load)\")\n",
        "    print(\"4. KV cache memory scales linearly with number of groups\")\n",
        "    print(\"\\nüí° GQA-4 provides the best balance: 2x memory savings, minimal quality loss\")\n",
        "    print(\"=\"*85)\n",
        "\n",
        "comprehensive_attention_comparison()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Key Takeaways\n",
        "\n",
        "1. **Evolution**: MHA ‚Üí MQA ‚Üí GQA represents optimizing the quality-speed-memory trade-off\n",
        "2. **GQA is optimal**: Near-MHA quality with 4-8x less KV cache memory\n",
        "3. **KV cache is critical**: Enables fast autoregressive generation\n",
        "4. **Industry standard**: LLaMA 2/3, Mistral, and likely GPT-4 use GQA\n",
        "5. **Configuration**: 8 KV groups for 32 heads is a proven sweet spot\n",
        "6. **Memory savings scale**: More heads = bigger savings with GQA\n",
        "7. **Quality preservation**: < 0.1% perplexity difference vs MHA with 8 groups\n",
        "\n",
        "## Modern LLM Usage (2025)\n",
        "\n",
        "**LLaMA 2/3**:\n",
        "- 32 query heads, 8 KV groups\n",
        "- 4x memory reduction vs MHA\n",
        "- Enables 128k context windows\n",
        "\n",
        "**Mistral/Mixtral**:\n",
        "- 32 query heads, 8 KV groups\n",
        "- Combined with sliding window attention\n",
        "- Enables efficient long-context processing\n",
        "\n",
        "**GPT-4** (rumored):\n",
        "- 128 query heads, 16 KV groups\n",
        "- 8x memory reduction\n",
        "- Enables massive 32k+ context\n",
        "\n",
        "**Implementation Tips**:\n",
        "- Use `repeat_interleave` to expand KV groups to query heads\n",
        "- Cache the unexpanded K, V (num_kv_groups dimension)\n",
        "- Combine with Flash Attention for maximum efficiency\n",
        "- Choose num_kv_groups as divisor of num_heads\n",
        "\n",
        "---\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "Continue to: [Topic 13: Mixture of Experts (MoE)](13_mixture_of_experts.ipynb)\n",
        "\n",
        "---\n",
        "\n",
        "## Further Reading\n",
        "\n",
        "- [GQA: Training Generalized Multi-Query Transformer Models](https://arxiv.org/abs/2305.13245) (2023)\n",
        "- [Fast Transformer Decoding: One Write-Head is All You Need](https://arxiv.org/abs/1911.02150) (MQA, 2019)\n",
        "- [LLaMA 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/abs/2307.09288)\n",
        "- [Mistral 7B Technical Report](https://arxiv.org/abs/2310.06825)\n",
        "- [PyTorch Attention Mechanisms](https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.scaled_dot_product_attention)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
